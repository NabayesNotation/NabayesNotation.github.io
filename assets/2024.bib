@inbook{Howard-Matheson1984,
    author          = {Ronald A. Howard and James E. Matheson},
    chapter        = {Influence Diagrams},
    editor         = {},
    pages          = {721-762},
    publisher      = {SDG Decision Systems},
    title          = {},
    year           = {1984}
}

@book{Diestel2017,
    author         = {Reinhard Diestel},
    year           = {2017},
    title          = {Graph Theory},
    series         = {Graduate Texts in Mathematics},
    volume         = {173},
    edition        = {5},
    url            = {https://link.springer.com/book/10.1007/978-3-662-53622-3},
    publisher      = {Springer Berlin Heidelberg}
}

@book{Preston1974,
    author         = {Christopher J. Preston},
    year           = {1974},
    title          = {Gibbs States on Countable Sets},
    series         = {Cambdirge Tracts in Mathematics},
    volume         = {68},
    edition        = {},
    url            = {https://www.cambridge.org/core/books/gibbs-states-on-countable-sets/970A88C66DA6E06BA1828142B3399272},
    publisher      = {Cambridge University Press}
}

@book{Kindermann-Snell1980,
    author         = {Ross Kindermann and J. Laurie Snell},
    year           = {1980},
    title          = {Markov Random Fields and Their Applications},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.ams.org/books/conm/001/},
    publisher      = {American Mathematical Society}
}

@unpublished{Hammersley-Clifford1971,
    author = {J. Hammersley and P. Clifford},
    year   = {1971},
    title  = {Markov Fields on Finite Graphs and Lattices},
    url    = {https://ora.ox.ac.uk/objects/uuid:4ea849da-1511-4578-bb88-6a8d02f457a6}
}

@book{Baxter1982,
    author         = {Rodney J. Baxter},
    year           = {1982},
    title          = {Exactly Solved Models in Statistical Mechanics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://physics.anu.edu.au/research/ftp/mpg/baxter_book.php},
    publisher      = {Academic Press}
}
@book{Kittel2018,
    author         = {Charles Kittel},
    year           = {2018},
    title          = {Introduction to Solid State Physics},
    series         = {},
    volume         = {},
    edition        = {8},
    url            = {https://www.wiley.com/en-ie/Kittel%27s+Introduction+to+Solid+State+Physics%2C+Global+Edition%2C+8th+Edition-p-9781119454168},
    publisher      = {John Wiley}
}

@book{Huebener2019,
    author         = {Rudolf P. Huebener},
    year           = {2019},
    title          = {Conductors, Semiconductors, Superconductors: An Introduction to Solid-State Physics},
    series         = {Undergraduate Lecture Notes in Physics},
    volume         = {},
    edition        = {3},
    url            = {https://link-springer-com.utokyo.idm.oclc.org/book/10.1007/978-3-030-31420-0},
    publisher      = {Springer Cham}
}

@book{Boer-Pohl2018,
    author         = {Karl W Böer and Udo W. Pohl},
    year           = {2018},
    title          = {Semiconductor Physics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/referencework/10.1007/978-3-319-69150-3},
    publisher      = {Springer Cham}
}

@book{Richard2023,
    author         = {Corey Richard},
    year           = {2023},
    title          = {Understanding Semiconductors: A Technical Guide for Non-Technical People},
    series         = {Maker Innovations Series},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4842-8847-4},
    publisher      = {Apress Berkeley}
}

@book{Miller2022,
    author         = {Chris Miller},
    year           = {2022},
    title          = {Chip War: The Fight for the World's Most Critical Technology},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.simonandschuster.com/books/Chip-War/Chris-Miller/9781982172008},
    publisher      = {Scribner}
}

@book{Evstigneev2022,
    author         = {Mykhaylo Evstigneev},
    year           = {2022},
    title          = {Introduction to Semiconductor Physics and Devices},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-031-08458-4},
    publisher      = {Springer Cham}
}

@book{Lau2021,
    author         = {John H. Lau},
    year           = {2021},
    title          = {Semiconductor Advanced Packaging},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-981-16-1376-0},
    publisher      = {Springer Singapore}
}

@book{SemiconductorHandbook2023,
    author         = {},
    editor         = {Massimo Rudan and Rossella Brunetti and Susanna Reggiani},
    year           = {2023},
    title          = {Springer Handbook of Semiconductor Devices},
    series         = {Springer Handbooks},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-030-79827-7},
    publisher      = {Springer Cham}
}

@book{菊池正典2023,
    author         = {菊池正典},
    year           = {2023},
    month          = {3},
    title          = {半導体産業のすべて：世界の先端企業から日本メーカーの展望まで},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.diamond.co.jp/book/9784478117118.html},
    publisher      = {ダイヤモンド社}
}

@book{ずーぼ2024,
    author         = {ずーぼ},
    year           = {2024},
    month          = {1},
    title          = {今と未来がわかる半導体},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.natsume.co.jp/books/19325},
    publisher      = {ナツメ社}
}

@book{May-Spanos2006,
    author         = {Gary S. May and Costas J. Spanos},
    year           = {2006},
    title          = {Fundamentals of Semiconductor Manufacturing and Process Control},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://onlinelibrary.wiley.com/doi/book/10.1002/0471790281},
    publisher      = {John Wiley \& Sons}
}

@book{Sze-Lee2012,
    author         = {Simon M. Sze and Ming-Kwei Lee},
    year           = {2012},
    title          = {Semiconductor Devices: Physics and Technology},
    series         = {},
    volume         = {},
    edition        = {3},
    url            = {https://www.wiley.com/en-us/Semiconductor+Devices:+Physics+and+Technology,+3rd+Edition-p-9780470537947},
    publisher      = {John Wiley \& Sons}
}

@book{May-Sze2003,
    author         = {Gary S. May and Simon M. Sze},
    year           = {2003},
    title          = {Fundamentals of Semiconductor Fabrication},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.wiley.com/en-us/Fundamentals+of+Semiconductor+Fabrication-p-9780471232797},
    publisher      = {John Wiley \& Sons}
}


@incollection{VanRossum2005,
	address = {Oxford},
	author = {M. {Van Rossum}},
	booktitle = {Encyclopedia of Condensed Matter Physics (Second Edition)},
	doi = {10.1016/B978-0-323-90800-9.00292-4},
	edition = {Second Edition},
	editor = {Tapash Chakraborty},
	isbn = {978-0-323-91408-6},
	keywords = {85.30.--z, 85.40.--e, 85.40.Hp, 85.40.Ls, 85.40.Qx, 85.40.Ry, 85.40.Sz},
	pages = {748-756},
	publisher = {Academic Press},
	title = {Integrated Circuits},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323908009002924},
	year = {2005},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/B9780323908009002924},
	bdsk-url-2 = {https://doi.org/10.1016/B978-0-323-90800-9.00292-4}}

@book{Patterson-Hennessy2014,
    author         = {David A. Patterson and John L. Hennessy},
    year           = {2014},
    title          = {Computer Organization and Design: The Hardware/Software Interface},
    series         = {},
    volume         = {},
    edition        = {5},
    url            = {},
    publisher      = {Elsevier}
}

@book{Pierret2003,
    author         = {Robert F. Pierret},
    year           = {2003},
    title          = {Advanced Semiconductor Fundamentals},
    series         = {Modular Series on Solid State Devices},
    volume         = {VI},
    edition        = {2},
    url            = {https://www.pearson.com/en-us/subject-catalog/p/advanced-semiconductor-fundamentals/P200000003285/9780130617927},
    publisher      = {Pearson}
}

@book{Ng2002,
    author         = {Kwok K. Ng},
    year           = {2002},
    title          = {Complete Guide to Semiconductor Devices},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118014769},
    publisher      = {John Wiley \& Sons}
}

@misc{Dennard1967,
    author       = {Robert H. Dennard},
    howpublished = {U.S. Patent 3,387,286A},
    title        = {Field Effect Transistor Memory},
    year         = {1967},
    url          = {https://patents.google.com/patent/US3387286A/en},
}

@misc{Andrus1957,
    author       = {Jules Andrus},
    howpublished = {U.S. Patent 3,122,817A},
    title        = {Fabrication of Semiconductor Devices},
    year         = {1957},
    url          = {https://patents.google.com/patent/US3122817A/en},
}

@article{Czochralski1918,
    author          = {J. Czochralski},
    year            = {1918},
    title           = {Ein neues Verfahren zur Messung der Kristallisationsgeschwindigkeit der Metalle},
    journal         = {Zeitschrift für Physikalische Chemie},
    volume          = {92U},
    number          = {1},
    pages           = {219-221},
    url             = {https://doi.org/10.1515/zpch-1918-9212}
}

@article{Bridgman1925,
    author          = {P. W. Bridgman},
    year            = {1925},
    title           = {Certain Physical Properties of Single Crystals of Tungsten, Antimony, Bismuth, Tellurium, Cadmium, Zinc, and Tin},
    journal         = {Proceedings of the American Academy of Arts and Sciences},
    volume          = {60},
    number          = {6},
    pages           = {305-383},
    url             = {https://www.jstor.org/stable/25130058}
}

@article{Welker1952,
    author          = {H. Welker},
    year            = {1952},
    title           = {Über neue halbleitende Verbindungen},
    journal         = {Zeitschrift für Naturforschung A},
    volume          = {7},
    number          = {11},
    pages           = {744-749},
    url             = {https://doi.org/10.1515/zna-1952-1110}
}

@article{Frosch-Derick1957,
    author          = {C. J. Frosch and L. Derick},
    year            = {1957},
    title           = {Surface Protection and Selective Masking during Diffusion in Silicon},
    journal         = {Journal of the Electrochemical Society},
    volume          = {104},
    number          = {9},
    pages           = {547},
    url             = {https://iopscience.iop.org/article/10.1149/1.2428650}
}

@misc{Kilby1959,
    author       = {Jack S. Kilby},
    howpublished = {U.S. Patent 3,138,743A},
    title        = {Miniaturized Electronic Circuits},
    year         = {1959},
    url          = {https://patents.google.com/patent/US3138743A/en},
}

@misc{Noyce1959,
    author       = {Robert N. Noyce},
    howpublished = {U.S. Patent 2,981,877A},
    title        = {Semiconductor Device-and-Lead Structure},
    year         = {1959},
    url          = {https://patents.google.com/patent/US2981877A/en},
}

@article{Hoff+1996,
    author          = {M. E. Hoff and F. Faggin and S. Mazor and M. Shima},
    year            = {1996},
    title           = {The history of the 4004},
    journal         = {IEEE Micro},
    volume          = {16},
    number          = {6},
    pages           = {10-20},
    url             = {https://ieeexplore.ieee.org/document/546561}
}

@inproceedings{Hoerni1960,
    author          = {J. A. Hoerni},
    year            = {1960},
    title           = {Planar silicon diodes and transistors},
    booktitle       = {1960 International Electron Devices Meeting},
    volume          = {},
    pages           = {},
    url             = {https://ieeexplore.ieee.org/document/1472833}
}


@article{Round1907,
    author          = {H. J. Round},
    year            = {1907},
    title           = {A Note On Carborundum},
    journal         = {Electrical World},
    volume          = {19},
    number          = {},
    pages           = {309-310},
    url             = {https://www.worldscientific.com/doi/abs/10.1142/9789814503464_0116}
}

@article{Shockley1949,
    author          = {W. Shockley},
    year            = {1949},
    title           = {The Theory of $p$-$n$ Junction in Semiconductors and $p$-$n$ Hunction Transistors},
    journal         = {The Bell System Technical Journal},
    volume          = {28},
    number          = {3},
    pages           = {435-489},
    url             = {https://ieeexplore.ieee.org/document/6773080}
}

@article{Kahng-Atalla1960,
    author          = {D. Kahng and M. M. Atalla},
    year            = {1960},
    title           = {Silicon-Silicon Dioxide Surface Device},
    journal         = {IRE Device Reserach Conference, Pittsburgh},
    volume          = {},
    number          = {},
    pages           = {},
    url             = {https://www.worldscientific.com/doi/10.1142/9789814503464_0076}
}

@inproceedings{Abts+2022,
author = {Abts, Dennis and Kimmell, Garrin and Ling, Andrew and Kim, John and Boyd, Matt and Bitar, Andrew and Parmar, Sahil and Ahmed, Ibrahim and DiCecco, Roberto and Han, David and Thompson, John and Bye, Michael and Hwang, Jennifer and Fowers, Jeremy and Lillian, Peter and Murthy, Ashwin and Mehtabuddin, Elyas and Tekur, Chetan and Sohmers, Thomas and Kang, Kris and Maresh, Stephen and Ross, Jonathan},
title = {A software-defined tensor streaming multiprocessor for large-scale machine learning},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527405},
doi = {10.1145/3470496.3527405},
abstract = {We describe our novel commercial software-defined approach for large-scale interconnection networks of tensor streaming processing (TSP) elements. The system architecture includes packaging, routing, and flow control of the interconnection network of TSPs. We describe the communication and synchronization primitives of a bandwidth-rich substrate for global communication. This scalable communication fabric provides the backbone for large-scale systems based on a software-defined Dragonfly topology, ultimately yielding a parallel machine learning system with elasticity to support a variety of workloads, both training and inference. We extend the TSP's producer-consumer stream programming model to include global memory which is implemented as logically shared, but physically distributed SRAM on-chip memory. Each TSP contributes 220 MiBytes to the global memory capacity, with the maximum capacity limited only by the network's scale --- the maximum number of endpoints in the system. The TSP acts as both a processing element (endpoint) and network switch for moving tensors across the communication links. We describe a novel software-controlled networking approach that avoids the latency variation introduced by dynamic contention for network links. We describe the topology, routing and flow control to characterize the performance of the network that serves as the fabric for a large-scale parallel machine learning system with up to 10,440 TSPs and more than 2 TeraBytes of global memory accessible in less than 3 microseconds of end-to-end system latency.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {567–580},
numpages = {14},
keywords = {dragonfly, machine learning, software scheduling, tensor streaming processor},
location = {New York, New York},
series = {ISCA '22}
}

@book{戸田+2011,
    author         = {戸田盛和 and 斎藤信彦 and 久保亮五 and 橋爪夏樹},
    year           = {2011},
    title          = {統計物理学},
    series         = {現代物理学の基礎},
    volume         = {5},
    edition        = {新装版},
    url            = {https://www.iwanami.co.jp/book/b259545.html},
    publisher      = {岩波書店}
}

@book{Gibbs1902,
    author         = {Josiah Willard Gibbs},
    year           = {1902},
    title          = {Elementary Principles in Statistical Mechanics: Developed with Especial Reference to the Rational Foundation of Thermodynamics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Charles Scribner's Sons}
}

@article{Weiss1907,
    author          = {Pierre Weiss},
    year            = {1907},
    title           = {L'hypothèse du champ moléculaire et la propriété ferromagnétique},
    journal         = {Journal of Physics: Theories and Applications},
    volume          = {6},
    number          = {1},
    pages           = {661-690},
    url             = {https://hal.science/jpa-00241247/en}
}

@inbook{Villani-Mouhot2015,
    author         = {Cédric Villani and Clément Mouhot},
    chapter        = {Kinetic Theory},
    editor         = {Nicholas J. Higham},
    pages          = {},
    publisher      = {Princeton University Press},
    title          = {The Princeton Companion to Applied Mathematics},
    year           = {2015},
    url            = {https://press.princeton.edu/books/hardcover/9780691150390/the-princeton-companion-to-applied-mathematics},
}

@inbook{Villani2002,
    author         = {Cédric Villani},
    chapter        = {A Review of Mathemaical Topics in Collisional Kinetic Theory},
    editor         = {S. Friedlander and D. Serre},
    pages          = {71-305},
    publisher      = {North-Holland, Amsterdam},
    title          = {Handbook of Mathematical Fluid Dynamics},
    volume         = {I},
    year           = {2002},
    url            = {https://www.sciencedirect.com/science/article/abs/pii/S1874579202800040?via%3Dihub},
}

@article{Zadeh1989,
    author          = {Lotfi A. Zadeh},
    year            = {1989},
    title           = {Knowledge representation in fuzzy logic},
    journal         = {IEEE Transactions on Knowledge and Data Engineering},
    volume          = {1},
    number          = {1},
    pages           = {89-100},
    url             = {https://ieeexplore.ieee.org/document/43406}
}

@book{Li2009,
    author         = {Stan Z. Li},
    year           = {2009},
    title          = {Markov Random Field Modeling in Image Analysis},
    series         = {Advances in Computer Vision and Pattern Recognition},
    volume         = {},
    edition        = {3},
    url            = {https://link.springer.com/book/10.1007/978-1-84800-279-1},
    publisher      = {Springer London}
}

@article{Besag1974,
    author          = {Julian Besag},
    year            = {1974},
    title           = {Spatial Interaction and the Statistical Analysis of Lattice Systems},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {36},
    number          = {2},
    pages           = {192-236},
    url             = {https://www.jstor.org/stable/2984812}
}

@unpublished{Grenander1983,
    author = {U. Grenander},
    year   = {1983},
    title  = {Tutorial in Pattern Theory},
    url    = {},
    publisher = {Brown University}
}
@article{Grenander-Miller1994,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346184},
 abstract = {Modern sensor technologies, especially in biomedicine, produce increasingly detailed and informative image ensembles, many extremely complex. It will be argued that pattern theory can supply mathematical representations of subject-matter knowledge that can be used as a basis for algorithmic `understanding' of such pictures. After a brief survey of the basic principles of pattern theory we shall illustrate them by an application to a concrete situation: high magnification (greater than 15 000 ×) electron micrographs of cardiac muscle cells. The aim is to build algorithms for automatic hypothesis formation concerning the number, location, orientation and shape of mitochondria and membranes. For this we construct a pattern theoretic model in the form of a prior probability measure on the space of configurations describing these hypotheses. This measure is synthesized by solving sequentially a jump-diffusion equation of generalized Langevin form. The jumps occur for the creation-annihilation of hypotheses, corresponding to a jump from one continuum to another in configuration (hypothesis) space. These continua (subhypotheses) are expressed in terms of products of low dimensional Lie groups acting on the generators of a template. We use a modified Bayes approach to obtain the hypothesis formation, also organized by solving a generalized Langevin equation. To justify this it is shown that the resulting jump-diffusion process is ergodic so that the solution converges to the desired probability measure. To speed up the convergence we reduce the computation of the drift term in the stochastic differential equation analytically to a curvilinear integral, with the random term computed almost instantaneously. The algorithms thus obtained are implemented, both for mitochondria and membranes, on a 4000 processor parallel machine. Photographs of the graphics illustrate how automatic hypothesis formation is achieved. This approach is applied to deformable neuroanatomical atlases and tracking recognition from narrow band and high resolution sensor arrays.},
 author = {Ulf Grenander and Michael I. Miller},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {4},
 pages = {549--603},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Representations of Knowledge in Complex Systems},
 urldate = {2024-06-04},
 volume = {56},
 year = {1994}
}


@article{Besag-Green1993,
	abstract = {SUMMARY Markov chain Monte Carlo (MCMC) algorithms, such as the Gibbs sampler, have provided a Bayesian inference machine in image analysis and in other areas of spatial statistics for several years, founded on the pioneering ideas of Ulf Grenander. More recently, the observation that hyperparameters can be included as part of the updating schedule and the fact that almost any multivariate distribution is equivalently a Markov random field has opened the way to the use of MCMC in general Bayesian computation. In this paper, we trace the early development of MCMC in Bayesian inference, review some recent computational progress in statistical physics, based on the introduction of auxiliary variables, and discuss its current and future relevance in Bayesian applications. We briefly describe a simple MCMC implementation for the Bayesian analysis of agricultural field experiments, with which we have some practical experience.},
	author = {Besag, Julian and Green, Peter J.},
	doi = {https://doi.org/10.1111/j.2517-6161.1993.tb01467.x},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1993.tb01467.x},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	keywords = {agricultural field experiments, antithetic variables, auxiliary variables, gibbs sampler, markov chain monte carlo, markov random fields, metropolis method, multigrid, multimodality, swendsen-wang method},
	number = {1},
	pages = {25-37},
	title = {Spatial Statistics and Bayesian Computation},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1993.tb01467.x},
	volume = {55},
	year = {1993},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1993.tb01467.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2517-6161.1993.tb01467.x}}


@article{Boltzmann1872,
    author          = {L. Boltzmann},
    year            = {1872},
    title           = {Weitere Studien über das Wärme gleichgenicht unfer Gasmoläkuler},
    journal         = {Sitzungsberichte der Akademie der Wissenschaften},
    volume          = {66},
    number          = {},
    pages           = {275-370},
    url             = {}
}

@article{Maxwell1867,
    author          = {J. Clerk Maxwell},
    year            = {1867},
    title           = {On the Dynamical Theory of Gases},
    journal         = {Philosophical Transactions of the Royal Society of London},
    volume          = {157},
    number          = {},
    pages           = {49-88},
    url             = {https://www.jstor.org/stable/108968}
}

@article{Maxwell1878,
    author          = {J. Clerk Maxwell},
    year            = {1878},
    title           = {On Stresses in Rarefied Gases Arising from Inequalities of Temperature},
    journal         = {Proceedings of the Royal Society of London},
    volume          = {27},
    number          = {},
    pages           = {304-308},
    url             = {https://www.jstor.org/stable/113680}
}

@article{Gidas1989,
    author          = {B. Gidas},
    year            = {1989},
    title           = {A Renormalization Group Approach to Image Processing Problems},
    journal         = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    volume          = {11},
    number          = {2},
    pages           = {164-180},
    url             = {https://ieeexplore.ieee.org/document/16712}
}

@phdthesis{Li1991,
    author      = {S. Ziqing Li},
    school      = {University of Surrey, Guildford Surrey, UK},
    title       = {Towards 3D Vision from Range Images: An Optimisation Framework and Parallel Distributed Networks},
    year        = {1991},
    url         = {https://www.sciencedirect.com/science/article/pii/104996609290023V},
}

@article{Faraday1833,
    author          = {Michael Faraday},
    year            = {1833},
    title           = {Experimental Researches in Electricity. Third Series},
    journal         = {Philosophical Transactions of the Royal Society of London},
    volume          = {123},
    number          = {},
    pages           = {23-54},
    url             = {https://www.jstor.org/stable/107985}
}

@article{Braun1874,
    author          = {Ferdinand Braun},
    year            = {1874},
    title           = {Über die Stromleitung durch Schwefelmetalic},
    journal         = {Annalen der Physik and Chemie},
    volume          = {153},
    number          = {4},
    pages           = {556-563},
    url             = {}
}

@article{Griffiths1967,
    author          = {Robert B. Griffiths},
    year            = {1967},
    title           = {Thermodynamic Functions for Fluids and Ferromagnets near the Critical Point},
    journal         = {Physical Review},
    volume          = {158},
    number          = {1},
    pages           = {176-187},
    url             = {https://journals.aps.org/pr/abstract/10.1103/PhysRev.158.176}
}

@article{Lenz1920,
    author          = {W. Lenz},
    year            = {1920},
    title           = {Beiträg zum Verständnis der magnetischen Eigenschaften in festen Körpern},
    journal         = {Physikalische Zeitschrift},
    volume          = {21},
    number          = {},
    pages           = {613-615},
    url             = {}
}

@article{Peieris1936,
    author          = {R. E. Peieris},
    year            = {1936},
    title           = {On Ising's Model of Ferromagnetism},
    journal         = {Mathematical Proceedings of the Cambridge Philosophical Society},
    volume          = {32},
    number          = {3},
    pages           = {477-481},
    url             = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/abs/on-isings-model-of-ferromagnetism/C0584C5711BC3D25830B63A4C2F09609}
}

@article{Ising1925,
    author          = {Ernst Ising},
    year            = {1925},
    title           = {Beitrag zur Theorie des Ferromagnetismus},
    journal         = {Zeitschrift für Physik},
    volume          = {31},
    number          = {},
    pages           = {253-258},
    url             = {https://link.springer.com/article/10.1007/BF02980577}
}

@article{Fisher1966,
    author          = {Michael E. Fisher},
    year            = {1966},
    title           = {Quantum Corrections to Critical-Point Behavior},
    journal         = {Physical Review Letters},
    volume          = {16},
    number          = {1},
    pages           = {11-14},
    url             = {https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.16.11}
}

@book{Madelung1978,
    author         = {Otfried Madelung},
    year           = {1978},
    title          = {Introduction to Solid-State Theory},
    series         = {Springer Series in Solid-State Sciences},
    volume         = {2},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-642-61885-7},
    publisher      = {Springer Berlin, Heidelberg}
}

@article{Heisenberg1931,
    author          = {W. Heisenberg},
    year            = {1931},
    title           = {Zum Paulischen Ausschließungsprinzip},
    journal         = {Annalen der Physik},
    volume          = {402},
    number          = {7},
    pages           = {888-904},
    url             = {https://onlinelibrary.wiley.com/doi/10.1002/andp.19314020710}
}

@article{Criens+2023,
    author          = {David Criens and Peter Pfaffelhuber and Thorsten Schmidt},
    year            = {2023},
    title           = {The Martingale Problem Method Revisited},
    journal         = {Electronic Journal of Probability},
    volume          = {28},
    number          = {},
    pages           = {1-46},
    url             = {https://projecteuclid.org/journals/electronic-journal-of-probability/volume-28/issue-none/The-martingale-problem-method-revisited/10.1214/23-EJP902.full}
}

@unpublished{Hoh1998,
    author         = {Walter Hoh},
    year           = {1998},
    title          = {Pseudo-Differential operators generating Markov processes},
    note           = {Habilitationsschrift Universität Bielefeld},
    url    = {http://www.mathematik.uni-bielefeld.de/%7Ehoh/pdo_mp.ps}
}

@article{Valiant1984,
    author          = {L. G. Valiant},
    year            = {1984},
    title           = {A Theory of the Learnable},
    journal         = {Communications of the ACM},
    volume          = {27},
    number          = {11},
    pages           = {1134-1142},
    url             = {https://dl.acm.org/doi/10.1145/1968.1972}
}

@article{Seeger2002,
    author          = {Matthias Seeger},
    year            = {2002},
    title           = {PAC-Bayesian Generalisation Error Bounds for Gaussian Process Classification},
    journal         = {Journal of Machine Learning Research},
    volume          = {3},
    number          = {},
    pages           = {233-269},
    url             = {https://www.jmlr.org/papers/v3/seeger02a.html}
}

@inbook{Haussler-Warmuth1993,
    author         = {David Haussler and Manfred Warmuth},
    chapter        = {The Probably Approximately Correct (PAC) and Other Learning Models},
    editor         = {Alan L. Meyrowitz and Susan Chipman},
    pages          = {291-312},
    publisher      = {Springer New York},
    title          = {Foundations of Knowledge Acquisition: Machine Learning},
    year           = {1993},
    url            = {https://link.springer.com/book/10.1007/b102257},
}

@inproceedings{Shawe-Taylor-Williamson1997,
    author          = {John Shawe-Taylor and Robert C. Williamson},
    booktitle       = {Proceedings of the Tenth Annual Conference on Computational Learning Theory},
    editor          = {},
    title           = {A PAC Analysis of a Bayesian Estimator},
    year            = {1997},
    pages           = {2-9},
    url             = {https://dl.acm.org/doi/10.1145/267460.267466},
}

@article{McAllester1999,
    author          = {David A. McAllester},
    year            = {1999},
    title           = {Some PAC-Bayesian Theorems},
    journal         = {Machine Learning},
    volume          = {37},
    number          = {},
    pages           = {355-363},
    url             = {https://link.springer.com/article/10.1023/A:1007618624809}
}

@book{Devroye+1996,
    author         = {Luc Devroye and László Györfi and Gábor Lugosi},
    year           = {1996},
    title          = {A Probabilistic Theory of Pattern Recognition},
    series         = {Stochastic Modelling and Applied Probability},
    volume         = {31},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4612-0711-5},
    publisher      = {Springer New York}
}

@book{Vapnik1998,
    author         = {Vladimir N. Vapnik},
    year           = {1998},
    title          = {Statistical Learning Theory},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.wiley.com/en-us/Statistical+Learning+Theory-p-9780471030034},
    publisher      = {Wiley-Blackwell}
}

@book{Catoni2007,
    author         = {Olivier Catoni},
    year           = {2007},
    title          = {PAC-Bayesian Supervised Classification: The Thermodynamics of Statistical Learning},
    series         = {IMS Lecture Notes - Monograph Series},
    volume         = {56},
    edition        = {},
    url            = {https://projecteuclid.org/ebooks/institute-of-mathematical-statistics-lecture-notes-monograph-series/Pac-Bayesian-Supervised-Classification/toc/10.1214/074921707000000391},
    publisher      = {Institute of Mathematical Statistics}
}

@article{Blei+2017,
    author          = {David M. Blei and Alp Kucukelbir and Jon D. McAuliffe},
    year            = {2017},
    title           = {Variational Inference: A Review for Statisticians},
    journal         = {Journal of the American Statistical Association},
    volume          = {112},
    number          = {518},
    pages           = {859-877},
    url             = {https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773}
}

@article{Scheffe1947,
    author          = {H Scheffé},
    year            = {1947},
    title           = {A Useful Convergence Theorem for Probability Discributions},
    journal         = {The Annals of Mathematical Statistics},
    volume          = {18},
    number          = {3},
    pages           = {434-438},
    url             = {https://www.jstor.org/stable/2235739}
}

@book{Bishop2006,
    author         = {Christopher M. Bishop},
    year           = {2006},
    title          = {Pattern Recognition and Machine Learning},
    series         = {Information Science and Statistics},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/9780387310732},
    publisher      = {Springer New York}
}

@book{須山敦志2019,
    author         = {須山敦志},
    year           = {2019},
    month          = {8},
    title          = {ベイズ深層学習},
    series         = {機械学習プロフェッショナルシリーズ},
    volume         = {},
    edition        = {},
    url            = {https://www.kspub.co.jp/book/detail/5168707.html},
    publisher      = {講談社サイエンティフィク}
}

@article{Hinton-Salakhutdinov2006,
    author          = {Geoffrey E. Hinton and R. R. Salakhutdinov},
    journal         = {Science},
    number          = {5786},
    title           = {Reducing the Dimensionality of Data with Neural Networks},
    volume          = {313},
    year            = {2006},
    pages           = {504-507},
    url             = {https://www.science.org/doi/10.1126/science.1127647},
}

@article{大王製紙CB発行事件,
    author          = {東京地判},
    year            = {2018},
    note            = {平成30年9月20日},
    title           = {判決〔控訴〕},
    journal         = {資料版商事法務},
    volume          = {415},
    number          = {},
    pages           = {83},
    url             = {https://www.shojihomu.co.jp/publishing/subscription_detail?id=509&category=2&sub_category=8&publish_id=509&cd=850415}
}

@article{川島いづみ2021,
    author          = {川島いづみ},
    year            = {2021},
    title           = {新株予約権付社債の有利発行・不公正発行該当性},
    journal         = {TKC ローライブラリー 新・判例解説 Watch 商法},
    volume          = {146},
    number          = {},
    pages           = {},
    url             = {http://lex.lawlibrary.jp/commentary/pdf/z18817009-00-051462025_tkc.pdf}
}

@article{潘阿憲2020,
    author          = {潘阿憲},
    year            = {2020},
    title           = {新株予約権付社債の不公正発行と取締役の責任},
    journal         = {法学教室},
    volume          = {},
    number          = {473},
    pages           = {129},
    url             = {https://www.yuhikaku.co.jp/hougaku/detail/020394}
}

@book{Pazy1983,
    author         = {A. Pazy},
    year           = {1983},
    title          = {Semigroups of Linear Operators and Applications to Partial Differential Equations},
    series         = {Applied Mathematical Sciences},
    volume         = {44},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4612-5561-1},
    publisher      = {Springer New York}
}

@article{Davis1984,
    author          = {M. H. A. Davis},
    year            = {1984},
    title           = {Piecewise-Deterministic Markov Processes: A General Class of Non-Diffusion Stochastic Models},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {46},
    number          = {3},
    pages           = {353-388},
    url             = {https://www.jstor.org/stable/2345677}
}

@article{Goldman-Singh2021,
    author          = {Jacob Vorstrup Goldman and Sumeetpal S. Singh},
    year            = {2021},
    title           = {Spatiotemporal Blocking of the Bouncy Particle Sampler for Efficient Inference in State-Space Models},
    journal         = {Statistics and Computing},
    volume          = {31},
    number          = {68},
    pages           = {67-81},
    url             = {https://link.springer.com/article/10.1007/s11222-021-10034-6}
}

@article{Bierkens+2020,
    author          = {Joris Bierkens and Sebastiano Grazzi and Kengo Kamatani and Gareth O. Roberts},
    year            = {2020},
    title           = {The Boomerang Sampler},
    journal         = {Proceedings of the 37th International Conference on Machine Learning},
    volume          = {119},
    number          = {},
    pages           = {908-918},
    url             = {https://proceedings.mlr.press/v119/bierkens20a.html}
}

@book{Sutton-Barto2018,
    author         = {Richard S. Sutton and Andrew G. Barto},
    year           = {2018},
    title          = {Reinforcement Learning: An Introduction},
    series         = {Adaptive Computation and Machine Learning Series},
    volume         = {},
    edition        = {2},
    url            = {https://mitpress.mit.edu/9780262352703/reinforcement-learning/},
    publisher      = {MIT Press}
}

@book{Powell2011,
    author         = {Warren B. Powell},
    year           = {2011},
    title          = {Approximate Dynamic Programming: Solving the Curses of Dimensionality},
    series         = {Wiley Series in Probability and Statistics},
    volume         = {},
    edition        = {2},
    url            = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118029176},
    publisher      = {John Wiley \& Sons}
}

@inproceedings{Singh-Bertsekas1996,
    author          = {Satinder Singh and Dimitri Bertsekas},
    year            = {1996},
    title           = {Reinforcement Learning for Dynamic Channel Allocation in Cellular Telephone Systems},
    booktitle       = {Advances in Neural Information Processing Systems 9},
    pages           = {},
    url             = {https://proceedings.neurips.cc/paper/1996/hash/3948ead63a9f2944218de038d8934305-Abstract.html}
}

@article{Silver+2018,
    author          = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
    year            = {2018},
    title           = {A General Reinforcement Learning Algorithm that Masters Chess, Shogi, and Go through Self-Play},
    journal         = {Science},
    volume          = {362},
    number          = {6419},
    pages           = {1140-1144},
    url             = {https://www.science.org/doi/10.1126/science.aar6404}
}

@inproceedings{VanRoy+1997,
    author          = {B. Van{\ }Roy and D. P. Bertsekas and Y. Lee and J. N. Tsitsiklis},
    year            = {1997},
    title           = {A Neuro-dynamic Programming Approach to Retailer Inventory Management},
    booktitle       = {Proceedings of the 36th IEEE Conference on Decision and Control},
    pages           = {},
    url             = {https://ieeexplore.ieee.org/abstract/document/652501}
}

@article{Crites-Barto1998,
    author          = {Robert H. Crites and Andrew G. Barto},
    year            = {1998},
    title           = {Elevator Group Control Using Multiple Reinforcement Learning Agents},
    journal         = {Machine Learning},
    volume          = {33},
    number          = {},
    pages           = {235-262},
    url             = {https://link.springer.com/article/10.1023/A:1007518724497}
}

@article{Rolnick+2022,
    author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra Sasha and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla P. and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
    title = {Tackling Climate Change with Machine Learning},
    year = {2022},
    issue_date = {February 2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {55},
    number = {2},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3485128},
    doi = {10.1145/3485128},
    abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.},
    journal = {ACM Computing Survey},
    month = {feb},
    articleno = {42},
    numpages = {96},
    keywords = {artificial intelligence, machine learning, adaptation, mitigation, Climate change}
}



@phdthesis{Watkins1989,
    author      = {Christopher J. C. H. Watkins},
    school      = {University of London},
    title       = {Learning from Delayed Rewards},
    year        = {1989},
    url         = {https://www.cs.rhul.ac.uk/~chrisw/thesis.html},
}

@article{Watkins-Dayan1992,
    author          = {Christopher J. C. H. Watkins and Peter Dayan},
    year            = {1992},
    title           = {Q-Leraning},
    journal         = {Machine Learning},
    volume          = {8},
    number          = {},
    pages           = {279-292},
    url             = {https://link.springer.com/article/10.1007/BF00992698}
}

@article{Sutton1988,
    author          = {Richard S. Sutton},
    year            = {1988},
    title           = {Learning to Predict by the Methods of Temporal Differences},
    journal         = {Machine Learning},
    volume          = {3},
    number          = {},
    pages           = {9-44},
    url             = {https://link.springer.com/article/10.1007/BF00115009}
}

@article{Sun+2016,
    author          = {Ying Sun and Prabhu Babu and Daniel P. Palomar},
    year            = {2016},
    title           = {Majorization-Minimization Algorithms in Signal Processing, Communications, and Machine Learning},
    journal         = {IEEE Transactions on Signal Processing},
    volume          = {65},
    number          = {3},
    pages           = {794-816},
    url             = {https://ieeexplore.ieee.org/document/7547360}
}

@article{Fisher1912,
    author          = {R. A. Fisher},
    year            = {1912},
    title           = {On an Absolute Criterion for Fitting Frequency Curves},
    journal         = {Messenger of Mathematics},
    volume          = {41},
    number          = {},
    pages           = {155-160},
    url             = {https://www.jstor.org/stable/2246266}
}

@article{Wu-Lange2010,
    author          = {Tong Tong Wu and Kenneth Lange},
    year            = {2010},
    title           = {The MM Alternative to EM},
    journal         = {Statistical Science},
    volume          = {25},
    number          = {4},
    pages           = {492-505},
    url             = {https://www.jstor.org/stable/23061097}
}

@book{Neal1996,
    author         = {Radford M. Neal},
    year           = {1996},
    title          = {Bayesian Learning for Neural Networks},
    series         = {Lecture Notes in Statistics},
    volume         = {118},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4612-0745-0},
    publisher      = {Springer New York}
}


@InProceedings{Cobb-Jalaian2021,
  title = 	 {Scaling Hamiltonian Monte Carlo inference for Bayesian neural networks with symmetric splitting},
  author =       {Cobb, Adam D. and Jalaian, Brian},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {675--685},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/cobb21a/cobb21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/cobb21a.html},
  abstract = 	 {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) approach that exhibits favourable exploration properties in high-dimensional models such as neural networks. Unfortunately, HMC has limited use in large-data regimes and little work has explored suitable approaches that aim to preserve the entire Hamiltonian. In our work, we introduce a new symmetric integration scheme for split HMC that does not rely on stochastic gradients. We show that our new formulation is more efficient than previous approaches and is easy to implement with a single GPU. As a result, we are able to perform full HMC over common deep learning architectures using entire data sets. In addition, when we compare with stochastic gradient MCMC, we show that our method achieves better performance in both accuracy and uncertainty quantification. Our approach demonstrates HMC as a feasible option when considering inference schemes for large-scale machine learning problems.}
}

@inproceedings{Smith-Le2018,
title={A Bayesian Perspective on Generalization and Stochastic Gradient Descent},
author={Samuel L. Smith and Quoc V. Le},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJij4yg0Z},
}

@ARTICLE{Imaizumi-SchmidtHieber2023,
  author={Imaizumi, Masaaki and Schmidt-Hieber, Johannes},
  journal={IEEE Transactions on Information Theory}, 
  title={On Generalization Bounds for Deep Networks Based on Loss Surface Implicit Regularization}, 
  year={2023},
  volume={69},
  number={2},
  pages={1203-1223},
  keywords={Neural networks;Deep learning;Statistics;Sociology;Convergence;Complexity theory;Training data;Deep neural network;generalization error;uniform convergence;non-convex optimization},
  doi={10.1109/TIT.2022.3215088}}

@inproceedings{Tolstikhin+2018,
title={Wasserstein Auto-Encoders},
author={Ilya Tolstikhin and Olivier Bousquet and Sylvain Gelly and Bernhard Schoelkopf},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HkL7n1-0b},
}


@inproceedings{Sabour+2017,
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Dynamic Routing Between Capsules},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/2cad8fa47bbef282badbb8de5374b894-Paper.pdf}}

@inbook{Scholkopf2022,
   title={Causality for Machine Learning},
   ISBN={9781450395861},
   url={http://dx.doi.org/10.1145/3501714.3501755},
   DOI={10.1145/3501714.3501755},
   booktitle={Probabilistic and Causal Inference},
   publisher={ACM},
   author={Schölkopf, Bernhard},
   year={2022},
   month=feb, pages={765–804},
   url            = {https://arxiv.org/abs/1911.10500},
}


@book{Theodoridis2020,
    author         = {Sergios Theodoridis},
    year           = {2020},
    title          = {Machine Learning: A Bayesian and Optimization Perspective},
    series         = {},
    volume         = {},
    edition        = {2},
    url            = {https://doi.org/10.1016/C2019-0-03772-7},
    publisher      = {Academic Press}
}

@book{Shalev-Shwartz-Ben-David2014,
    author         = {Shai Shalev-Shwartz and Shai Ben-David},
    year           = {2014},
    title          = {Understanding Machine Learning: From Theory to Algorithms},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1017/CBO9781107298019},
    publisher      = {Cambridge University Press}
}

@techreport{MacKay1994,
    author      = {D. J. C. MacKay},
    institution = {American Society of Heating, Refrigerating, and Air Conditioning Engineers (ASHRAE)},
    title       = {Bayesian nonlinear modeling for the prediction competition},
    volume          = {100},
    number          = {2},
    year        = {1994},
    url         = {https://www.osti.gov/biblio/33309},
}

@book{Rasmussen-Williams2006,
    author         = {Carl Edward Rasmussen and Christopher K. I. Williams},
    year           = {2006},
    title          = {Gaussian Processes for Machine Learning},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://direct.mit.edu/books/book/2320/Gaussian-Processes-for-Machine-Learning},
    publisher      = {The MIT Press}
}

@article{Rumelhart+1986,
    author          = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
    year            = {1986},
    title           = {Learning Representations by Back-Propagating Errors},
    journal         = {Nature},
    volume          = {323},
    number          = {},
    pages           = {533-536},
    url             = {https://www.nature.com/articles/323533a0}
}

@inproceedings{Williams1996,
    author          = {Christopher K. I. Williams},
    year            = {1996},
    title           = {Computing with Infinite Networks},
    booktitle       = {Advances in Neural Information Processing Systems 9},
    pages           = {295-301},
    url             = {https://papers.nips.cc/paper_files/paper/1996/hash/ae5e3ce40e0404a45ecacaaf05e5f735-Abstract.html}
}

@inproceedings{Lee+2018,
title={Deep Neural Networks as Gaussian Processes},
author={Jaehoon Lee and Jascha Sohl-dickstein and Jeffrey Pennington and Roman Novak and Sam Schoenholz and Yasaman Bahri},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=B1EA-M-0Z},
}

@book{Bishop-Bishop2024,
    author         = {Christopher M. Bishop and Hugo Bishop},
    year           = {2024},
    title          = {Deep Learning: Foundations and Concepts},
    url            = {https://link.springer.com/book/10.1007/978-3-031-45468-4},
    publisher      = {Springer Cham}
}

@article{LeCun1998,
    author          = {Yan LeCun and L. Bottou and Y. Bengio and P. Haffner},
    year            = {1998},
    title           = {Gradient-Based Learning Applied to Document Recognition},
    journal         = {Proceedings of the IEEE},
    volume          = {86},
    number          = {11},
    pages           = {2278-2324},
    url             = {https://ieeexplore.ieee.org/document/726791}
}

@article{Schmidhuber2015,
    author          = {Jürgen Schmidhuber},
    year            = {2015},
    title           = {Deep Learning in Neural Networks: An Overview},
    journal         = {Neural Networks},
    volume          = {61},
    number          = {},
    pages           = {85-117},
    url             = {https://www.sciencedirect.com/science/article/pii/S0893608014002135}
}

@inproceedings{Goodfellow+2014,
    author          = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    year            = {2014},
    title           = {Generative Adversarial Nets},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {27},
    pages           = {1-9},
    url             = {https://papers.nips.cc/paper/2014/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html}
}

@inproceedings{Krizhevsky+2012,
    author          = {Alex Krizhevsky and Ilya Sutskever and Geoffrey E. Hinton},
    year            = {2012},
    title           = {ImageNet Classification with Deep Convolutional Neural Networks},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {25},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html}
}

@unpublished{Hinton+2012,
    title={Improving neural networks by preventing co-adaptation of feature detectors}, 
    author={Geoffrey E. Hinton and Nitish Srivastava and Alex Krizhevsky and Ilya Sutskever and Ruslan R. Salakhutdinov},
    year   = {2012},
    url    = {https://arxiv.org/abs/1207.0580}
}

@article{Srivastava+2014,
    author          = {Nitish Srivastava and Geoffrey E. Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
    year            = {2014},
    title           = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
    journal         = {The Journal of Machine Learning Research},
    volume          = {15},
    number          = {56},
    pages           = {1929-1958},
    url             = {https://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{Salakhutdinov-Hinton2009,
    author          = {Ruslan Salakhutdinov and Geoffrey Hinton},
    year            = {2009},
    title           = {Deep Boltzmann Machines},
    booktitle       = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
    volume          = {5},
    pages           = {448-455},
    url             = {https://proceedings.mlr.press/v5/salakhutdinov09a.html}
}

@inproceedings{Bengio+2014,
    author          = {Yoshua Bengio and Eric Laufer and Guillaume Alain and Jason Yosinski},
    year            = {2014},
    title           = {Deep Generative Stochastic Networks Trainable by Backprop},
    booktitle       = {Proceedings of the 31st International Conference on Machine Learning},
    volume          = {32},
    number          = {2},
    pages           = {226-234},
    url             = {https://proceedings.mlr.press/v32/bengio14.html}
}

@article{Endres-Schindelin2003,
    author          = {D. M. Endres and J. E. Schindelin},
    year            = {2003},
    title           = {A New Metric for Probability Distributions},
    journal         = {IEEE Transactions on Information Theory},
    volume          = {49},
    number          = {7},
    pages           = {1858-1860},
    url             = {https://ieeexplore.ieee.org/document/1207388}
}

@article{Osan+2018,
    author          = {Tristán M. Osán and Diego G. Bussandri and Pedro W. Lamberti},
    year            = {2018},
    title           = {Monoparametric Family of Metrics Derived from Classical Jensen-Shannon Divergence},
    journal         = {Physica A: Statistical Mechanics and its Applications},
    volume          = {495},
    number          = {},
    pages           = {336-344},
    url             = {https://www.sciencedirect.com/science/article/pii/S0378437117313225}
}

@article{Rao1987,
    author          = {C. R. Rao},
    year            = {1987},
    title           = {Differential Metrics in Probability Spaces},
    journal         = {IMS Lecture Notes Monograph Series},
    volume          = {10},
    number          = {},
    pages           = {217-240},
    url             = {https://projecteuclid.org/ebooks/institute-of-mathematical-statistics-lecture-notes-monograph-series/Differential-geometry-in-statistical-inference/chapter/Chapter-5-Differential-Metrics-in-Probability-Spaces/10.1214/lnms/1215467062}
}

@article{Rao1982,
    author          = {C. R. Rao},
    year            = {1982},
    title           = {Diversity and Dissimilarity Coefficients: A Unified Approach},
    journal         = {Theoretical Population Biology},
    volume          = {21},
    number          = {1},
    pages           = {24-43},
    url             = {https://www.sciencedirect.com/science/article/abs/pii/0040580982900041}
}

@article{Lin1991,
    author          = {J. Lin},
    year            = {1991},
    title           = {Divergence Measures Based on the Shannon Entropy},
    journal         = {IEEE Transactions on Information Theory},
    volume          = {37},
    number          = {1},
    pages           = {145-151},
    url             = {https://ieeexplore.ieee.org/document/61115}
}


@article{Nielsen2021,
    author          = {Frank Nielsen},
    year            = {2021},
    title           = {On a Variational Definition for the Jensen-Shannon Symmetrization of Distances Based on the Information Radius},
    journal         = {Entropy},
    volume          = {23},
    number          = {4},
    pages           = {464},
    url             = {https://www.mdpi.com/1099-4300/23/4/464}
}

@article{Ali-Silvey1966,
    author          = {S. M. Ali and S. D. Silvey},
    year            = {1966},
    title           = {A General Class of Coefficients of Divergence of One Distribution from Another},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {28},
    number          = {1},
    pages           = {131-142},
    url             = {https://www.jstor.org/stable/2984279}
}

@inproceedings{Renyi1961,
    author          = {Alfréd Rényi},
    year            = {1961},
    title           = {On Measures of Entropy and Information},
    booktitle       = {Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability},
    volume          = {1},
    pages           = {547-561},
    url             = {https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fourth-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/On-Measures-of-Entropy-and-Information/bsmsp/1200512181}
}

@article{Csiszar1963,
    author          = {Imere Csiszár},
    year            = {1963},
    title           = {Eine informationstheoretische Ungleichung und ihre Anwendung auf Beweis der Ergodizitaet von Markoffschen Ketten},
    journal         = {Magyár Tudomá Akadémia Mahematikai Kutató Intézetének Köezleményei},
    volume          = {6},
    number          = {},
    pages           = {85-108},
    url             = {https://www.fuw.edu.pl/~kostecki/scans/csiszar1963.pdf}
}

@article{Morimoto1963,
    author          = {Tetsuzo Morimoto},
    year            = {1963},
    title           = {Markov Processes and the $H$-Theorem},
    journal         = {Journal of the Physical Society of Japan},
    volume          = {18},
    number          = {3},
    pages           = {328-331},
    url             = {https://journals.jps.jp/doi/abs/10.1143/JPSJ.18.328?journalCode=jpsj}
}

@book{Robert2007,
    author         = {Christian P. Robert},
    year           = {2007},
    title          = {The Bayesian Choice: From Decision-Theoretic Foundations to Computational Implementation},
    series         = {Springer Texts in Statistics},
    volume         = {},
    edition        = {2},
    url            = {https://link.springer.com/book/10.1007/0-387-71599-1},
    publisher      = {Springer New York}
}

@article{Wu1983,
    author          = {C. F. Jeff Wu},
    year            = {1983},
    title           = {On the Convergence Properties of the EM Algorithm},
    journal         = {The Annals of Statistics},
    volume          = {11},
    number          = {1},
    pages           = {95-103},
    url             = {https://www.jstor.org/stable/2240463}
}

@article{Finch+1989,
    author          = {Stephen J. Finch and Nancy R. Mendell and Henry C. Thode{\ }Jr.},
    year            = {1989},
    title           = {Probabilistic Measures of Adequacy of a Numerical Search for a Global Maximum},
    journal         = {Journal of the American Statistical Association},
    volume          = {84},
    number          = {408},
    pages           = {1020-1023},
    url             = {https://www.jstor.org/stable/2290078}
}

@article{Boyles1983,
    author          = {Russell A. Boyles},
    year            = {1983},
    title           = {On the Convergence of the EM Algorithm},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {45},
    number          = {1},
    pages           = {47-50},
    url             = {https://www.jstor.org/stable/2345622}
}

@inproceedings{Attias1999,
    author          = {Hagai Attias},
    year            = {1999},
    title           = {Inferring Parameters and Structure of Latent Variable Models by Variational Bayes},
    booktitle       = {Proceedings of the Fifteenth Conference in Artificial Intelligence},
    volume          = {},
    pages           = {21-30},
    url             = {https://dl.acm.org/doi/10.5555/2073796.2073799}
}

@article{Kingma-Welling2019,
    author          = {Diederik P. Kingma and Max Welling},
    year            = {2019},
    title           = {An Introduction to Variational Autoencoders},
    journal         = {Foundations and Treands in Machine Learning},
    volume          = {12},
    number          = {4},
    pages           = {307-392},
    url             = {https://www.nowpublishers.com/article/Details/MAL-056}
}

@inproceedings{Kingma-Welling2014,
    author          = {Diederik P. Kingma and Max Welling},
    year            = {2014},
    title           = {Auto-Encoding Variational Bayes},
    booktitle       = {International Conference on Learning Representations},
    volume          = {2},
    pages           = {},
    url             = {https://openreview.net/forum?id=33X9fd2-9FyZd}
}

@article{Jordan+1999,
    author          = {Michael I. Jordan and Zoubin Ghahramani and Tommi S. Jaakkola and Lawrence K. Saul},
    year            = {1999},
    title           = {An Introduction to Variational Methods for Graphical Models},
    journal         = {Machine Learning},
    volume          = {37},
    number          = {},
    pages           = {183-233},
    url             = {https://link.springer.com/article/10.1023/A:1007665907178}
}

@article{Peterson-Anderson1987,
    author          = {Carsten Peterson and James R. Anderson},
    year            = {1987},
    title           = {A Mean Field Theory Learning Algorithm for Neural Networks},
    journal         = {Complex Systems},
    volume          = {1},
    number          = {5},
    pages           = {1987},
    url             = {https://www.complex-systems.com/abstracts/v01_i05_a06/}
}

@book{Parisi1988,
    author         = {Giorgio Parisi},
    year           = {1988},
    title          = {Statistical Field Theory},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Addison-Wesley}
}

@book{Bathe1996,
    author         = {Klaus-Jürgen Bathe},
    year           = {1996},
    title          = {Finite Element Procedures},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Prentice-Hall}
}

@book{Sakurai1985,
    author         = {J. Sakurai},
    year           = {1985},
    title          = {Modern Quantum Mechanics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Addison-Wesley}
}

@book{Rustagi1976,
    author         = {J. Rustagi},
    year           = {1976},
    title          = {Variational Methods in Statistics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Academic Press}
}

@inbook{Neal-Hinton1998,
    author         = {Radford M. Neal and Geoffrey E. Hinton},
    chapter        = {A View of the EM Algorithm that Justifies Incremental, Sparse and Other Variants},
    editor         = {Michael I. Jordan},
    pages          = {355-368},
    publisher      = {Springer Dordrecht},
    title          = {Learning in Graphical Models},
    year           = {1998},
    url            = {https://link.springer.com/chapter/10.1007/978-94-011-5014-9_12},
}

@inproceedings{Rezende+2014,
    author          = {Danilo Jimenez Rezende and Shakir Mohamed and Daan Wierstra},
    year            = {2014},
    title           = {Approximate Inference in Deep Generative Models},
    booktitle       = {Proceedings of the 31st International Conference on Machine Learning},
    volume          = {32},
    pages           = {1278-1286},
    url             = {https://proceedings.mlr.press/v32/rezende14.html}
}


@InProceedings{Rezende-Mohamed2015,
  title = 	 {Variational Inference with Normalizing Flows},
  author = 	 {Rezende, Danilo and Mohamed, Shakir},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1530--1538},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/rezende15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/rezende15.html},
  abstract = 	 {The choice of the approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.}
}


@article{Rosenblatt1958,
    author          = {F. Rosenblatt},
    year            = {1958},
    title           = {The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain},
    journal         = {Psychological Review},
    volume          = {65},
    number          = {6},
    pages           = {386-408},
    url             = {https://psycnet.apa.org/record/1959-09865-001}
}

@book{Minsky-Papert1969,
    author         = {Marvin Minsky and Seymour A. Papert},
    year           = {1969},
    title          = {Perceptrons: An Introduction to Computational Geometry},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://direct.mit.edu/books/book/3132/PerceptronsAn-Introduction-to-Computational},
    publisher      = {The MIT Press}
}

@book{Hebb1949,
    author         = {D. O. Hebb},
    year           = {1949},
    title          = {The Organization of Behavior: A Neuropsychological Theory},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.taylorfrancis.com/books/mono/10.4324/9781410612403/organization-behavior-hebb},
    publisher      = {John Wiley \& Sons, Chapman and Hall}
}

@article{McCulloch-Pitts1943,
    author          = {W. McCulloch and W. Pitts},
    year            = {1943},
    title           = {A Logical Calculus of the Ideas Immanent in Nervous Activity},
    journal         = {Bulletin of Mathematical Biophysics},
    volume          = {7},
    number          = {},
    pages           = {115-133},
    url             = {https://link.springer.com/article/10.1007/BF02478259}
}

@article{Siegelmann-Sontag1991,
    author          = {Hava T. Siegelmann and Eduardo D. Sontag},
    year            = {1991},
    title           = {Turing Computability with Neural Nets},
    journal         = {Applied Mathematics Letters},
    volume          = {4},
    number          = {6},
    pages           = {77-80},
    url             = {https://www.sciencedirect.com/science/article/pii/089396599190080F}
}

@book{人工知能学会2015,
    author         = {麻生英樹 and 安田宗樹 and 前田新一 and 岡野原大輔 and 岡谷貴之 and 久保陽太郎 and ボレガラダヌシカ},
    year           = {2015},
    title          = {深層学習},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.kindaikagaku.co.jp/book_list/detail/9784764904873/},
    publisher      = {近代科学社}
}

@book{甘利俊一1989,
    author         = {甘利俊一},
    year           = {1989},
    title          = {神経回路網モデルとコネクショニズム},
    series         = {認知科学選書},
    volume         = {22},
    edition        = {},
    url            = {https://www.utp.or.jp/book/b305707.html},
    publisher      = {東京大学出版会}
}

@article{Amari1967,
    author          = {Shunichi Amari},
    year            = {1967},
    title           = {A Theory of Adaptive Pattern Classifiers},
    journal         = {IEEE Transactions on Electronic Computers},
    volume          = {EC-16},
    number          = {3},
    pages           = {299-307},
    url             = {https://ieeexplore.ieee.org/document/4039068}
}

@book{Amari1985,
    author         = {Shun-ichi Amari},
    year           = {1985},
    title          = {Differential-Geometrical Methods in Statistics},
    series         = {Lecture Notes in Statistics},
    volume         = {28},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4612-5056-2},
    publisher      = {Springer New York}
}

@article{Fukushima1980,
    author          = {K. Fukushima},
    year            = {1980},
    title           = {Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position--Neocognitron},
    journal         = {Biological Cybernetics},
    volume          = {36},
    number          = {},
    pages           = {193-202},
    url             = {https://link.springer.com/article/10.1007/BF00344251}
}

@article{Hubel-Wiesel1959,
    author          = {D. H. Wiesel and T. N. Hubel},
    year            = {1959},
    title           = {Receptive Fields of Single Neurones in the Cat's Striate Cortex},
    journal         = {Journal of Physiology},
    volume          = {148},
    number          = {3},
    pages           = {574-591},
    url             = {https://physoc.onlinelibrary.wiley.com/doi/10.1113/jphysiol.1959.sp006308}
}

@article{Fukushima1975,
    author          = {K. Fukushima},
    year            = {1975},
    title           = {Cognitron: a Self-Organizing Multilayered Neural Network},
    journal         = {Biological Cybernetics},
    volume          = {20},
    number          = {},
    pages           = {121-136},
    url             = {}
}

@article{Malsburg1973,
    author          = {C. von{\ }der{\ }Malsburg},
    year            = {1973},
    title           = {Self-Organization of Orientation Sensitive Cells in the Striate Cortex},
    journal         = {Kybernetik},
    volume          = {14},
    number          = {},
    pages           = {85-100},
    url             = {}
}

@article{Hubel1967,
    author          = {David H. Hubel},
    year            = {1967},
    title           = {Effects of Distortion of Sensory Input on the Visual System of Kittens},
    journal         = {The Physiologist},
    volume          = {10},
    number          = {},
    pages           = {17-45},
    url             = {}
}

@article{Sejnowski-Rosenberg1987,
    author          = {Terrence J. Sejnowski and Charles R. Rosenberg},
    year            = {1987},
    title           = {Parallel Networks that Learn to Pronounce English Text},
    journal         = {Complex Systems},
    volume          = {1},
    number          = {1},
    pages           = {145-168},
    url             = {https://www.complex-systems.com/abstracts/v01_i01_a10/}
}

@inproceedings{Bengio+2006,
    author          = {Yoshua Bengio and Pascal Lamblin and Dan Popovici and Hugo Larochelle},
    year            = {2006},
    title           = {Greedy Layer-Wise Training of Deep Networks},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {19},
    pages           = {153-160},
    url             = {https://papers.nips.cc/paper_files/paper/2006/hash/5da713a690c067105aeb2fae32403405-Abstract.html}
}

@inproceedings{Cottrell-Munro1988,
    author          = {Garrison W. Cottrell and Paul Munro},
    year            = {1988},
    title           = {Principal Component Analysis of Images via Back Propagation},
    booktitle       = {Proceedings of SPIE Visual Communications and Image Processings},
    volume          = {1001},
    pages           = {1070-1076},
    url             = {https://www.spiedigitallibrary.org/conference-proceedings-of-spie/1001/1/Principal-Components-Analysis-Of-Images-Via-Back-Propagation/10.1117/12.969060.short}
}

@article{Baldi-Hornik1989,
    author          = {Pierre Baldi and Kurt Hornik},
    year            = {1989},
    title           = {Neural Networks and Principal Component Analysis: Learning from Examples without Local Minima},
    journal         = {Neural Networks},
    volume          = {2},
    number          = {1},
    pages           = {53-58},
    url             = {https://www.sciencedirect.com/science/article/pii/0893608089900142}
}

@inproceedings{DeMers-Cottrell1992,
    author          = {David DeMers and Garrison Cottrell},
    year            = {1992},
    title           = {Non-Linear Dimensionality Reduction},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {5},
    pages           = {580-587},
    url             = {https://proceedings.neurips.cc/paper/1992/hash/cdc0d6e63aa8e41c89689f54970bb35f-Abstract.html}
}

@inproceedings{He+2016,
    author          = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
    year            = {2016},
    title           = {Deep Residual Learning for Image Recognition},
    booktitle       = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    volume          = {},
    pages           = {770-778},
    url             = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html}
}

@inproceedings{Vaswani+2017,
    author          = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Łukasz Kaiser and Illia Polosukhin},
    year            = {2017},
    title           = {Attention is All you Need},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {30},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@inproceedings{Hecht-Nielsen1989,
    author          = {R. Hecht-Nielsen},
    year            = {1989},
    title           = {Theory of the Backpropagation Neural Network},
    booktitle       = {International 1989 Joint Conference on Neural Networks},
    volume          = {},
    pages           = {},
    url             = {https://ieeexplore.ieee.org/document/118638}
}

@article{Ballard1987,
    author          = {Dana H. Ballard},
    year            = {1987},
    title           = {Modular Learning in Neural Networks},
    journal         = {Proceedings of the Sixth National Conference on Artificial Intelligence},
    volume          = {1},
    number          = {},
    pages           = {279-284},
    url             = {https://dl.acm.org/doi/10.5555/1863696.1863746}
}

@article{Hinton+2006,
    author          = {Geoffrey E. Hinton and Simon Esindero and Yee-Whye Teh},
    year            = {2006},
    title           = {A Fast Learning Algorithm for Deep Belief Nets},
    journal         = {Naural Computation},
    volume          = {18},
    number          = {7},
    pages           = {1527-1554},
    url             = {https://direct.mit.edu/neco/article/18/7/1527/7065/A-Fast-Learning-Algorithm-for-Deep-Belief-Nets}
}

@article{Hopfield1982,
    author          = {J. J. Hopfield},
    year            = {1982},
    title           = {Neural Networks and Physical Systems with Emergent Collective Computational Abilities},
    journal         = {Proceedings of the National Academy of Science},
    volume          = {79},
    number          = {8},
    pages           = {2554-2558},
    url             = {https://www.pnas.org/doi/10.1073/pnas.79.8.2554}
}

@article{Hopfield-Tank1985,
    author          = {J. J. Hopfield and D. W. Tank},
    year            = {1985},
    title           = {"Neural" Computation of Decisions in Optimization Problems},
    journal         = {Biological Cybernetics},
    volume          = {52},
    number          = {},
    pages           = {141-152},
    url             = {https://link.springer.com/article/10.1007/BF00339943}
}

@article{Robbins-Monro1951,
    author          = {Herbert Robbins and Sutton Monro},
    year            = {1951},
    title           = {A Stochastic Approximation Method},
    journal         = {The Annals of Mathematical Statistics},
    volume          = {22},
    number          = {3},
    pages           = {400-407},
    url             = {https://www.jstor.org/stable/2236626}
}

@inproceedings{Robbins1956,
    author          = {Herbert Robbins},
    year            = {1956},
    title           = {{An Empirical Bayes Approach to Statistics}},
    booktitle       = {Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability},
    volume          = {1},
    pages           = {157-163},
    url             = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Third-Berkeley-Symposium-on-Mathematical-Statistics-and-Probability/chapter/An-Empirical-Bayes-Approach-to-Statistics/bsmsp/1200512992}
}

@article{Kiefer-Wolfowitz1952,
    author          = {J. Kiefer and J. Wolfowitz},
    year            = {1952},
    title           = {Stochastic Estimation of the Maximum of a Regression Function},
    journal         = {The Annals of Mathematical Statistics},
    volume          = {22},
    number          = {3},
    pages           = {462-466},
    url             = {https://www.jstor.org/stable/2236690}
}

@inproceedings{Duchi+2011,
    author          = {John Duchi and Elad Hazan and Yoram Singer},
    year            = {2011},
    title           = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
    booktitle       = {Journal of Machine Learning Research},
    volume          = {12},
    number          = {61},
    pages           = {2121-2159},
    url             = {https://jmlr.org/papers/v12/duchi11a.html}
}

@unpublished{Tieleman-Hinton2012,
    author = {T. Tieleman and Geoffrey Hinton},
    year   = {2012},
    title  = {Neural Networks for Machine Learning. Lecture 6.5: Divide the Gradient by a Running Average of its Recent Magnitude},
    url    = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}
}

@inproceedings{Nowozin+2016,
    author          = {Sebastian Nowozin and Botond Cseke and Ryota Tomioka},
    year            = {2016},
    title           = {f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {29},
    pages           = {},
    url             = {https://papers.nips.cc/paper/2016/hash/cedebb6e872f539bef8c3f919874e9d7-Abstract.html}
}

@inproceedings{Arjovsky+2017,
    author          = {Martin Arjovsky and Soumith Chintala and Léon Bottou},
    year            = {2017},
    title           = {Wasserstein Generative Adversarial Networks},
    booktitle       = {Proceedings of the 34th International Conference on Machine Learning},
    volume          = {70},
    pages           = {214-223},
    url             = {http://proceedings.mlr.press/v70/arjovsky17a.html}
}

@inbook{Robert1996,
    author         = {C. P. Robert},
    chapter        = {Inference in Mixture Models},
    editor         = {W. R. Gilks and David Spiegelhalter},
    pages          = {441-464},
    publisher      = {Chapman \& Hall, London},
    title          = {Markov Chain Monte Carlo in Practice},
    year           = {1996}
}

@inproceedings{Mengersen-Robert1996,
    author          = {Kerrie L. Mengersen and Christian P. Robert},
    year            = {1996},
    title           = {Testing for Mixtures: A Bayesian Entropic Approach},
    booktitle       = {Bayesian Statistics 5: Proceedings of the Fifth Valencia International Meetings},
    pages           = {255-276},
    url             = {https://academic.oup.com/book/54042/chapter-abstract/422209682}
}

@article{Wei-Tanner1990,
    author          = {Greg C. G. Wei and Martin A. Tanner},
    year            = {1990},
    title           = {A Monte Carlo Implementation of the EM Algorithm and the Poor Man's Data Augmentation Algorithm},
    journal         = {Journal of the American Statistical Association},
    volume          = {85},
    number          = {411},
    pages           = {699-704},
    url             = {https://www.jstor.org/stable/2290005}
}

@article{Wei-Tanner1990b,
    author          = {Greg C. G. Wei and Martin A. Tanner},
    year            = {1990},
    title           = {Posterior Computations for Censored Regression Data},
    journal         = {Journal of the American Statistical Association},
    volume          = {85},
    number          = {411},
    pages           = {829-839},
    url             = {https://www.jstor.org/stable/2290022}
}

@article{Dieblot-Robert1994,
    author          = {Jean Diebolt and Christian P. Robert},
    year            = {1994},
    title           = {Estimation of Finite Mixture Distributions through Bayesian Sampling},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {56},
    number          = {2},
    pages           = {363-375},
    url             = {https://www.jstor.org/stable/2345907}
}

@article{Lloyd1982,
    author          = {S. Lloyd},
    year            = {1982},
    title           = {Least Squares Quantization in PCM},
    journal         = {IEEE Transactions on Information Theory},
    volume          = {28},
    number          = {2},
    pages           = {129-137},
    url             = {https://ieeexplore.ieee.org/document/1056489}
}

@book{Fletcher1987,
    author         = {R. Fletcher},
    year           = {1987},
    title          = {Practical Methods of Optimization},
    series         = {},
    volume         = {},
    edition        = {2},
    url            = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118723203},
    publisher      = {John Wiley \& Sons}
}

@article{Meng-Rubin1993,
    author          = {Xiao-Li Meng and Donald B. Rubin},
    year            = {1993},
    title           = {Maximum Likelihood Estimation via the ECM Algorithm: A General Framework},
    journal         = {Biometrika},
    volume          = {80},
    number          = {2},
    pages           = {267-278},
    url             = {https://www.jstor.org/stable/2337198}
}

@book{Hrafnkelsson2023,
    author         = {},
    editor         = {Birgir Hrafnkelsson},
    year           = {2023},
    title          = {Statistical Modeling Using Bayesian Latent Gaussian Models: With Applications in Geophysics and Encironmental Sciences},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-031-39791-2},
    publisher      = {Springer Cham}
}

@inproceedings{MacQueen1967,
    author          = {J. MacQueen},
    year            = {1967},
    title           = {Some Methods for Classification and Analysis of Multivariate Observations},
    booktitle       = {Proceedings of the Fifth Berkeley Symposium on Mathematical Statistics and Probability},
    volume          = {1},
    pages           = {281-297},
    url             = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fifth-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Some-methods-for-classification-and-analysis-of-multivariate-observations/bsmsp/1200512992}
}

@book{Kapur1989,
    author         = {J. N. Kapur},
    year           = {1989},
    title          = {Maximum-Entropy Models in Science and Engineering},
    series         = {},
    volume         = {},
    edition        = {2},
    url            = {},
    publisher      = {Wiley}
}

@inbook{Fenyman+1964,
    author         = {Richard P. Feynman and R. B. Leighton and M. Sands},
    chapter        = {19. The Principle of Least Action},
    volume         = {II},
    editor         = {},
    pages          = {},
    publisher      = {Addison-Wesley},
    title          = {The Feynman Lectures of Physics},
    year           = {1964},
    url            = {https://www.feynmanlectures.caltech.edu/II_19.html},
}

@inbook{Fenyman+1963,
    author         = {Richard P. Feynman and R. B. Leighton and M. Sands},
    chapter        = {1. Atoms in Motion},
    volume         = {I},
    editor         = {},
    pages          = {},
    publisher      = {Addison-Wesley},
    title          = {The Feynman Lectures of Physics},
    year           = {1964},
    url            = {https://www.feynmanlectures.caltech.edu/I_01.html},
}

@book{Schwarz1988,
    author         = {H. R. Schwarz},
    year           = {1988},
    title          = {Finite Element Methods},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Academic Press}
}

@book{Boyd-Vandenberghe2004,
    author         = {Stephen Boyd and Lieven Vandenberghe},
    year           = {2004},
    title          = {Convex Optimization},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.cambridge.org/core/books/convex-optimization/E413CEF23D00BD463DCCE0600810D3FA},
    publisher      = {Cambridge University Press}
}

@inproceedings{Minka2001,
    author          = {Thomas P. Minka},
    year            = {2001},
    title           = {Expectation Propagation for Approximate Bayesian Inference},
    booktitle       = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
    volume          = {},
    pages           = {362-369},
    url             = {https://dl.acm.org/doi/10.5555/2074022.2074067}
}

@phdthesis{Minka2001b,
    author      = {Thomas P. Minka},
    school      = {MIT},
    title       = {A Family of Approximation Algorithms for Bayesian Inference},
    year        = {2001}
}

@article{Cichocki+2008,
    author          = {Andrzej Cichocki and Hyekyoung Lee and Yong-Deok Kim and Seungjin Choi},
    year            = {2008},
    title           = {Non-negative Matrix Factorization with $\alpha$-divergence},
    journal         = {Pattern Recognition Letters},
    volume          = {29},
    number          = {9},
    pages           = {1433-1440},
    url             = {https://www.sciencedirect.com/science/article/abs/pii/S0167865508000767}
}

@techreport{Minka2004,
    author      = {Tomas P. Minka},
    institution = {Microsoft Research Cambridge},
    title       = {Power EP},
    year        = {2004},
    url         = {https://www.microsoft.com/en-us/research/publication/power-ep/},
}

@techreport{Minka2005,
    author      = {Tomas P. Minka},
    institution = {Microsoft Research Cambridge},
    title       = {Divergence Measures and Message Passing},
    year        = {2005},
    url         = {https://www.microsoft.com/en-us/research/publication/divergence-measures-and-message-passing/},
}

@techreport{Brooks+2024,
    author      = {Tim Brooks and Bill Peebles and Connor Holmes and Will DePue and Yufei Guo and Li Jing and David Schnurr and Joe Taylor and Troy Luhman and Eric Luhman and Clarence Wing Yin Ng and Ricky Wang and Aditya Ramesh},
    institution = {OpenAI},
    title       = {Video generation models as world simulators},
    year        = {2024},
    url         = {https://openai.com/research/video-generation-models-as-world-simulators},
}

@inproceedings{Razavi+2019,
    author          = {Ali Razavi and Aaron van{\ }den{\ }Oord and Oriol Vinyals},
    year            = {2019},
    title           = {Generating Diverse High-Fidelity Images with VQ-VAE-2},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {32},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html}
}

@inproceedings{vandenOord+2017,
    author          = {Aaron van{\ }den{\ }Oord and Oriol Vinyals and Koray Kavukcuoglu},
    year            = {2017},
    title           = {Neural Discrete Representation Learning},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {30},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2017/hash/7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html}
}

@inproceedings{Wallace1992,
    author          = {G. K. Wallace},
    year            = {1992},
    title           = {The JPEG Still Picture Compression Standard},
    booktitle       = {IEEE Transactions on Consumer Electronics},
    volume          = {38},
    pages           = {1},
    url             = {https://ieeexplore.ieee.org/document/125072}
}

@inproceedings{Paisley+2012,
    author          = {John Paisley and David M. Blei and Michael I. Jordan},
    year            = {2012},
    title           = {Variational Bayesian Inference with Stochastic Search},
    booktitle       = {Proceedings of the 29th International Conference on Machine Learning},
    volume          = {},
    pages           = {1363-1370},
    url             = {https://dl.acm.org/doi/10.5555/3042573.3042748}
}

@inbook{Diebolt-Ip1996,
    author         = {J. Diebolt and E. Ip},
    chapter        = {Stochastic EM: Method and Application},
    editor         = {W. R. Gilks and S. Richardson and David Spiegelhalter},
    pages          = {259-274},
    publisher      = {Chapman and Hall},
    title          = {Markov Chain Monte Carlo in Practice},
    year           = {1996},
    url            = {https://www.taylorfrancis.com/books/mono/10.1201/b14835/markov-chain-monte-carlo-practice-david-spiegelhalter-gilks-richardson},
}

@article{Celeux-Diebolt1985,
    author          = {G. Celeux and J. Diebolt},
    year            = {1985},
    title           = {The SEM Algorithm: A Probabilistic Teacher Algorithm Derived from the EM Algorithm for the Mixture Problem},
    journal         = {Computational Statistics Quarterly},
    volume          = {2},
    number          = {},
    pages           = {73-82},
    url             = {}
}

@article{Meng-Rubin1991,
    author          = {Xiao-Li Meng and Donald B. Rubin},
    year            = {1991},
    title           = {Using EM to Obtain Asymptotic Variance-Covariance Matrices: The SEM Algorithm},
    journal         = {Journal of the American Statistical Association},
    volume          = {86},
    number          = {416},
    pages           = {899-909},
    url             = {https://www.jstor.org/stable/2290503}
}

@article{Doucet+2002,
    author          = {Arnaud Doucet and Simon J. Godsill and Christian P. Robert},
    year            = {2002},
    title           = {Marginal Maximum a Posteriori Estimation using Markov Chain Monte Carlo},
    journal         = {Statistics and Computing},
    volume          = {12},
    number          = {},
    pages           = {77-84},
    url             = {https://link.springer.com/article/10.1023/A:1013172322619}
}

@book{Bouleau-Lepingle1993,
    author         = {Nicolas Bouleau and Dominique Lépingle},
    year           = {1993},
    title          = {Numerical Methods for Stochastic Processes},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.wiley.com/en-ca/Numerical+Methods+for+Stochastic+Processes-p-9780471546412},
    publisher      = {Wiley}
}

@inbook{Geyer1996,
    author         = {C. Geyer},
    chapter        = {Estimation and Optimization of Functions},
    editor         = {W. R. Gilks and S. Richardson and David Spiegelhalter},
    pages          = {241-258},
    publisher      = {Chapman and Hall},
    title          = {Markov Chain Monte Carlo in Practice},
    year           = {1996},
    url            = {https://www.taylorfrancis.com/books/mono/10.1201/b14835/markov-chain-monte-carlo-practice-david-spiegelhalter-gilks-richardson},
}

@unpublished{Radford+2019,
    author = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
    year   = {2019},
    title  = {Language Models are Unsupervised Multitask Learners},
    url    = {https://github.com/openai/gpt-2?tab=readme-ov-file}
}

@inproceedings{Brown+2020,
    author          = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
    year            = {2020},
    title           = {Language Models are Few-Shot Learners},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {33},
    pages           = {1877-1901},
    url             = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
}

@unpublished{OpenAI2023,
    author = {OpenAI},
    year   = {2023},
    title  = {GPT-4 Technical Report},
    url    = {https://arxiv.org/abs/2303.08774}
}

@unpublished{Sutton2019,
    author = {Rich Sutton},
    year   = {2019},
    title  = {The Bitter Lesson},
    url    = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html}
}

@inproceedings{Zhou+2023,
title={{LIMA}: Less Is More for Alignment},
author={Chunting Zhou and Pengfei Liu and Puxin Xu and Srini Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and LILI YU and Susan Zhang and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer and Omer Levy},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=KBMOKmX2he}
}

@unpublished{Kaplan+2020,
author       = {Jared Kaplan and
                  Sam McCandlish and
                  Tom Henighan and
                  Tom B. Brown and
                  Benjamin Chess and
                  Rewon Child and
                  Scott Gray and
                  Alec Radford and
                  Jeffrey Wu and
                  Dario Amodei},
    year   = {2020},
    title  = {Scaling Laws for Neural Language Models},
    url    = {https://arxiv.org/abs/2001.08361}
}

@article{Bommasani+2021,
title={On the Opportunities and Risks of Foundation Models},
author={Rishi Bommasani and Drew A. Hudson and Ehsan Adeli and Russ Altman and Simran Arora and Sydney von Arx and Michael S. Bernstein and Jeannette Bohg and Antoine Bosselut and Emma Brunskill and Erik Brynjolfsson and S. Buch and Dallas Card and Rodrigo Castellon and Niladri S. Chatterji and Annie S. Chen and Kathleen A. Creel and Jared Davis and Dora Demszky and Chris Donahue and Moussa Doumbouya and Esin Durmus and Stefano Ermon and John Etchemendy and Kawin Ethayarajh and Li Fei-Fei and Chelsea Finn and Trevor Gale and Lauren E. Gillespie and Karan Goel and Noah D. Goodman and Shelby Grossman and Neel Guha and Tatsunori Hashimoto and Peter Henderson and John Hewitt and Daniel E. Ho and Jenny Hong and Kyle Hsu and Jing Huang and Thomas F. Icard and Saahil Jain and Dan Jurafsky and Pratyusha Kalluri and Siddharth Karamcheti and Geoff Keeling and Fereshte Khani and O. Khattab and Pang Wei Koh and Mark S. Krass and Ranjay Krishna and Rohith Kuditipudi and Ananya Kumar and Faisal Ladhak and Mina Lee and Tony Lee and Jure Leskovec and Isabelle Levent and Xiang Lisa Li and Xuechen Li and Tengyu Ma and Ali Malik and Christopher D. Manning and Suvir P. Mirchandani and Eric Mitchell and Zanele Munyikwa and Suraj Nair and Avanika Narayan and Deepak Narayanan and Benjamin Newman and Allen Nie and Juan Carlos Niebles and Hamed Nilforoshan and J. F. Nyarko and Giray Ogut and Laurel Orr and Isabel Papadimitriou and Joon Sung Park and Chris Piech and Eva Portelance and Christopher Potts and Aditi Raghunathan and Robert Reich and Hongyu Ren and Frieda Rong and Yusuf H. Roohani and Camilo Ruiz and Jack Ryan and Christopher R'e and Dorsa Sadigh and Shiori Sagawa and Keshav Santhanam and Andy Shih and Krishna Parasuram Srinivasan and Alex Tamkin and Rohan Taori and Armin W. Thomas and Florian Tram{\`e}r and Rose E. Wang and William Wang and Bohan Wu and Jiajun Wu and Yuhuai Wu and Sang Michael Xie and Michihiro Yasunaga and Jiaxuan You and Matei A. Zaharia and Michael Zhang and Tianyi Zhang and Xikun Zhang and Yuhui Zhang and Lucia Zheng and Kaitlyn Zhou and Percy Liang},
journal={ArXiv},
year={2021},
url={https://crfm.stanford.edu/report.html}
}

@unpublished{Hu+2021,
  author       = {Edward J. Hu and
                  Yelong Shen and
                  Phillip Wallis and
                  Zeyuan Allen{-}Zhu and
                  Yuanzhi Li and
                  Shean Wang and
                  Weizhu Chen},
    year   = {2021},
    title  = {LoRA: Low-Rank Adaptation of Large Language Models},
    url    = {https://arxiv.org/abs/2106.09685}
}

@inproceedings{Aghajanyan+2021,
    author          = {Armen Aghajanyan and Sonal Gupta and Luke Zettlemoyer},
    year            = {2021},
    title           = {Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning},
    booktitle       = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing},
    volume          = {1},
    pages           = {7319-7328},
    url             = {https://aclanthology.org/2021.acl-long.568/}
}

@inproceedings{Christiano+2017,
    author          = {Paul F. Christiano and Jan Leike and Tom Brown and Miljan Martic and Shane Legg and Dario Amodei},
    year            = {2017},
    title           = {Deep Reinforcement Learning from Human Preferences},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {30},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2017/hash/d5e2c0adad503c91f91df240d0cd4e49-Abstract.html}
}

@article{Liu+2023-PPP,
    author          = {Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig},
    year            = {2023},
    title           = {Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
    journal         = {ACM Computing Surveys},
    volume          = {55},
    number          = {9},
    pages           = {1-35},
    url             = {https://dl.acm.org/doi/full/10.1145/3560815}
}


@InProceedings{Liu+2023I2SB,
  title = 	 {{I}$^2${SB}: Image-to-Image Schrödinger Bridge},
  author =       {Liu, Guan-Horng and Vahdat, Arash and Huang, De-An and Theodorou, Evangelos and Nie, Weili and Anandkumar, Anima},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {22042--22062},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/liu23ai/liu23ai.pdf},
  url = 	 {https://proceedings.mlr.press/v202/liu23ai.html},
  abstract = 	 {We propose Image-to-Image Schrödinger Bridge (I$^2$SB), a new class of conditional diffusion models that directly learn the nonlinear diffusion processes between two given distributions. These diffusion bridges are particularly useful for image restoration, as the degraded images are structurally informative priors for reconstructing the clean images. I$^2$SB belongs to a tractable class of Schrödinger bridge, the nonlinear extension to score-based models, whose marginal distributions can be computed analytically given boundary pairs. This results in a simulation-free framework for nonlinear diffusions, where the I$^2$SB training becomes scalable by adopting practical techniques used in standard diffusion models. We validate I$^2$SB in solving various image restoration tasks, including inpainting, super-resolution, deblurring, and JPEG restoration on ImageNet 256$\times$256 and show that I$^2$SB surpasses standard conditional diffusion models with more interpretable generative processes. Moreover, I$^2$SB matches the performance of inverse methods that additionally require the knowledge of the corruption operators. Our work opens up new algorithmic opportunities for developing efficient nonlinear diffusion models on a large scale. Project page and codes: https://i2sb.github.io/}
}


@unpublished{Bubeck+2023,
      author={Sébastien Bubeck and Varun Chandrasekaran and Ronen Eldan and Johannes Gehrke and Eric Horvitz and Ece Kamar and Peter Lee and Yin Tat Lee and Yuanzhi Li and Scott Lundberg and Harsha Nori and Hamid Palangi and Marco Tulio Ribeiro and Yi Zhang},
    year   = {2023},
    title  = {Sparks of Artificial General Intelligence: Early Experiments with GPT-4},
    url    = {https://arxiv.org/abs/2303.12712}
}
@article{Bubeck2015,
	author = {S{\'e}bastien Bubeck},
	doi = {10.1561/2200000050},
	issn = {1935-8237},
	journal = {Foundations and Trends{\textregistered} in Machine Learning},
	number = {3-4},
	pages = {231-357},
	title = {Convex Optimization: Algorithms and Complexity},
	url = {http://dx.doi.org/10.1561/2200000050},
	volume = {8},
	year = {2015},
	bdsk-url-1 = {http://dx.doi.org/10.1561/2200000050}}
@book{Beck2017,
    author = {Amir Beck},
    year = {2017},
    title = {First-Order Methods in Optimization},
    series = {MOS-SIAM Series on Optimization},
    volume = {},
    edition = {},
    url = {https://doi.org/10.1137/1.9781611974997},
    publisher = {Society for Industrial and Applied Mathematics}
}
@book{Ekeland-Temam1999,
    author = {Ivar Ekeland and Roger Témam},
    year = {1999},
    title = {Convex Analysis and Variational Problems},
    series = {Classics in Applied Mathematics},
    volume = {},
    edition = {},
    url = {https://doi.org/10.1137/1.9781611971088},
    publisher = {Society for Industrial and Applied Mathematics}
}

@unpublished{Bahdanau+2015,
    author = {Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
    year   = {2015},
    title  = {Neural Machine Translation by Jointly Learning to Align and Translate},
    url    = {https://arxiv.org/abs/1409.0473}
}

@unpublished{Ba+2016,
    author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
    year   = {2016},
    title  = {Layer Normalization},
    url    = {https://arxiv.org/abs/1607.06450}
}

@unpublished{Dufter+2021,
    author = {Philipp Dufter and Martin Schmitt and Hinrich Schütze},
    year   = {2021},
    title  = {Position Information in Transformers: An Overview},
    url    = {https://arxiv.org/abs/2102.11090}
}

@unpublished{Mikolov2013,
    author = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
    year   = {2013},
    title  = {Efficient Estimation of Word Representations in Vector Space},
    url    = {https://arxiv.org/abs/1301.3781}
}


@inproceedings{Mikolov2013b,
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Distributed Representations of Words and Phrases and their Compositionality},
	url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
	volume = {26},
	year = {2013},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf}}


@inproceedings{Bengio+2000,
    author          = {Yoshua Bengio and Réjean Ducharme and Pascal Vincent},
    year            = {2000},
    title           = {A Neural Probabilistic Language Model},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {13},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2000/hash/728f206c2a01bf572b5940d7d9a8fa4c-Abstract.html}
}

@inproceedings{Sennrich+2016,
    author          = {Rico Sennrich and Barry Haddow and Alexandra Birch},
    year            = {2016},
    title           = {Neural Machine Translation of Rare Words with Subword Units},
    booktitle       = {Proceedings of the 54th Annual Meetings of the Association for Computational Linguistics},
    volume          = {1},
    pages           = {1715-1725},
    url             = {https://aclanthology.org/P16-1162/}
}

@unpublished{Zhao+2023,
          author={Wayne Xin Zhao and Kun Zhou and Junyi Li and Tianyi Tang and Xiaolei Wang and Yupeng Hou and Yingqian Min and Beichen Zhang and Junjie Zhang and Zican Dong and Yifan Du and Chen Yang and Yushuo Chen and Zhipeng Chen and Jinhao Jiang and Ruiyang Ren and Yifan Li and Xinyu Tang and Zikang Liu and Peiyu Liu and Jian-Yun Nie and Ji-Rong Wen},
    year   = {2023},
    title  = {A Survey of Large Language Models},
    url    = {https://arxiv.org/abs/2303.18223}
}

@inproceedings{Devlin+2019,
    author          = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
    year            = {2019},
    title           = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    booktitle       = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human language Technologies},
    volume          = {1},
    pages           = {4171-4186},
    url             = {https://aclanthology.org/N19-1423/}
}

@inproceedings{Holtzman+2020,
    author          = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
    year            = {2020},
    title           = {The Curious Case of Neural Text Degeneration},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://arxiv.org/abs/1904.09751}
}

@inproceedings{Mikolov+2010,
    author          = {T. Mikolov and J. Kopecky and L. Burget and J. \u{C}ernocky and S. Khudanpur},
    year            = {2010},
    title           = {Recurrent Neural Network Based Language Model},
    booktitle       = {Proceedings of Interspeech},
    volume          = {},
    pages           = {},
    url             = {http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf}
}

@article{Hochreiter-Schmidhuber1997,
    author          = {Sepp Hochreiter and Jürgen Schmidhuber},
    year            = {1997},
    title           = {Long Short-Time Memory},
    journal         = {Neural Computation},
    volume          = {9},
    number          = {8},
    pages           = {1735-1780},
    url             = {https://direct.mit.edu/neco/article-abstract/9/8/1735/6109/Long-Short-Term-Memory?redirectedFrom=fulltext}
}

@inproceedings{Cho+2014,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    editor = "Moschitti, Alessandro  and
      Pang, Bo  and
      Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1179",
    doi = "10.3115/v1/D14-1179",
    pages = "1724--1734",
    url             = {https://aclanthology.org/D14-1179/},
}

@inproceedings{Dosovitskiy+2021,
    author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
    year            = {2021},
    title           = {An Image is Worth 16×16 Words: Transformers for Image Recognition at Scale},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=YicbFdNTTy}
}

@inproceedings{Radford+2023,
author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
title = {Robust speech recognition via large-scale weak supervision},
year = {2023},
publisher = {JMLR.org},
abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1182},
numpages = {27},
location = {Honolulu, Hawaii, USA},
series = {ICML'23},
url             = {https://dl.acm.org/doi/10.5555/3618408.3619590},
}

@inproceedings{Ioffe-Szegedy2015,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch normalization: accelerating deep network training by reducing internal covariate shift},
year = {2015},
publisher = {JMLR.org},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82\% top-5 test error, exceeding the accuracy of human raters.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {448–456},
numpages = {9},
location = {Lille, France},
series = {ICML'15},
url             = {https://dl.acm.org/doi/10.5555/3045118.3045167},
}

@inproceedings{Bjorck+2018,
    author          = {Bjorck, Nils and Gomes, Carla P and Selman, Bart and Weinberger, Kilian Q},
    year            = {2018},
    title           = {Understanding Batch Normalization},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {31},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2018/hash/36072923bfc3cf47745d704feb489480-Abstract.html}
}

@inbook{LeCun+2012,
    author         = {Yann A. LeCun and Léon Bottou and Genevieve B. Orr and Klaus-Robert Müller},
    chapter        = {Efficient BackProp},
    editor         = {Grégoire Montavon and Geneviéve B. Orr and Klaus-Robert Müller},
    pages          = {9-48},
    publisher      = {Springer Berlin, Heidelberg},
    title          = {Neural Networks: Tricks of the Trade},
    year           = {2012},
    edition        = {2},
    url            = {https://link.springer.com/book/10.1007/978-3-642-35289-8},
}

@inproceedings{Lepikhin+2021,
      author       = {Dmitry Lepikhin and
                  HyoukJoong Lee and
                  Yuanzhong Xu and
                  Dehao Chen and
                  Orhan Firat and
                  Yanping Huang and
                  Maxim Krikun and
                  Noam Shazeer and
                  Zhifeng Chen},
    year            = {2021},
    title           = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=qrwe7XHTmYb}
}

@article{Fedus+2022,
    author          = {William Fedus and Barret Zoph and Noam Shazeer},
    year            = {2022},
    title           = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
    journal         = {The Journal of Machine Learning Research},
    volume          = {23},
    number          = {120},
    pages           = {1-39},
    url             = {https://jmlr.org/papers/v23/21-0998.html}
}

@techreport{Rae+2021,
    author       = {Jack W. Rae and
                  Sebastian Borgeaud and
                  Trevor Cai and
                  Katie Millican and
                  Jordan Hoffmann and
                  H. Francis Song and
                  John Aslanides and
                  Sarah Henderson and
                  Roman Ring and
                  Susannah Young and
                  Eliza Rutherford and
                  Tom Hennigan and
                  Jacob Menick and
                  Albin Cassirer and
                  Richard Powell and
                  George van den Driessche and
                  Lisa Anne Hendricks and
                  Maribeth Rauh and
                  Po{-}Sen Huang and
                  Amelia Glaese and
                  Johannes Welbl and
                  Sumanth Dathathri and
                  Saffron Huang and
                  Jonathan Uesato and
                  John Mellor and
                  Irina Higgins and
                  Antonia Creswell and
                  Nat McAleese and
                  Amy Wu and
                  Erich Elsen and
                  Siddhant M. Jayakumar and
                  Elena Buchatskaya and
                  David Budden and
                  Esme Sutherland and
                  Karen Simonyan and
                  Michela Paganini and
                  Laurent Sifre and
                  Lena Martens and
                  Xiang Lorraine Li and
                  Adhiguna Kuncoro and
                  Aida Nematzadeh and
                  Elena Gribovskaya and
                  Domenic Donato and
                  Angeliki Lazaridou and
                  Arthur Mensch and
                  Jean{-}Baptiste Lespiau and
                  Maria Tsimpoukelli and
                  Nikolai Grigorev and
                  Doug Fritz and
                  Thibault Sottiaux and
                  Mantas Pajarskas and
                  Toby Pohlen and
                  Zhitao Gong and
                  Daniel Toyama and
                  Cyprien de Masson d'Autume and
                  Yujia Li and
                  Tayfun Terzi and
                  Vladimir Mikulik and
                  Igor Babuschkin and
                  Aidan Clark and
                  Diego de Las Casas and
                  Aurelia Guy and
                  Chris Jones and
                  James Bradbury and
                  Matthew J. Johnson and
                  Blake A. Hechtman and
                  Laura Weidinger and
                  Iason Gabriel and
                  William Isaac and
                  Edward Lockhart and
                  Simon Osindero and
                  Laura Rimell and
                  Chris Dyer and
                  Oriol Vinyals and
                  Kareem Ayoub and
                  Jeff Stanway and
                  Lorrayne Bennett and
                  Demis Hassabis and
                  Koray Kavukcuoglu and
                  Geoffrey Irving},
    institution = {Google DeepMind},
    title       = {Scaling Language Models: Methods, Analysis & Insights from Training Gopher},
    year        = {2021},
    url         = {https://arxiv.org/abs/2112.11446},
}

@techreport{Radford+2018,
    author      = {Alec Radford and Karthik narasimhan and Tim Salimans and Ilya Sutskever},
    institution = {OpenAI},
    title       = {Improving Language Understanding with Unsupervised Learning},
    year        = {2018},
    url         = {https://openai.com/research/language-unsupervised},
}

@article{vandenOord+2016,
    author          = {Aäron van{\ }den{\ }Oord and Nal Kalchbrenner and Koray Kavukcuoglu},
    year            = {2016},
    title           = {Pixel Recurrent Neural Networks},
    journal         = {Proceedings of the 33rd International Conference on Machine Learning},
    volume          = {},
    number          = {},
    pages           = {},
    url             = {https://proceedings.mlr.press/v48/oord16.html}
}

@inproceedings{vandenOord+2016b,
author = {A\"{a}ron van{\ }den{\ }Oord and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
title = {Conditional image generation with PixelCNN decoders},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {4797–4805},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16},
url             = {https://dl.acm.org/doi/10.5555/3157382.3157633},
}

@article{Bengio+2013,
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
title = {Representation Learning: A Review and New Perspectives},
year = {2013},
issue_date = {August 2013},
publisher = {IEEE Computer Society},
address = {USA},
volume = {35},
number = {8},
issn = {0162-8828},
url = {https://doi.org/10.1109/TPAMI.2013.50},
doi = {10.1109/TPAMI.2013.50},
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.},
journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
month = {aug},
pages = {1798–1828},
numpages = {31},
keywords = {unsupervised learning, representation learning, neural nets, feature learning, autoencoder, Speech recognition, Neural networks, Manifolds, Machine learning, Learning systems, Feature extraction, Deep learning, Boltzmann machine, Abstracts}
}

@misc{Bengio+2013SSE,
      title={Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation}, 
      author={Yoshua Bengio and Nicholas Léonard and Aaron Courville},
      year={2013},
      eprint={1308.3432},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1308.3432}, 
}


@inproceedings{Maddison+2014,
	author = {Maddison, Chris J and Tarlow, Daniel and Minka, Tom},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {A\ast Sampling},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/309fee4e541e51de2e41f21bebb342aa-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/309fee4e541e51de2e41f21bebb342aa-Paper.pdf}}


@inproceedings{Chen+2020,
    author          = {Mark Chen and Alec Radford and Rewon Child and Jeffrey Wu and Heewoo Jun and David Luan and Ilya Sutskever},
    year            = {2020},
    title           = {Generative Pretraining from Pixels},
    booktitle       = {Proceedings of the 37th International Conference on Machine Learning},
    volume          = {},
    pages           = {},
    url             = {https://proceedings.mlr.press/v119/chen20s.html}
}

@misc{Rakhimov+2020,
      title={Latent Video Transformer}, 
      author={Ruslan Rakhimov and Denis Volkhonskiy and Alexey Artemov and Denis Zorin and Evgeny Burnaev},
      year={2020},
    url          = {https://arxiv.org/abs/2006.10704},
}

@unpublished{Yan+2021,
  author       = {Wilson Yan and
                  Yunzhi Zhang and
                  Pieter Abbeel and
                  Aravind Srinivas},
    year   = {2021},
    title  = {VideoGPT: Video Generation using VQ-VAE and Transformers},
    url    = {https://arxiv.org/abs/2104.10157}
}

@inproceedings{Micheli+2023,
    author          = {Vincent Micheli and Eloi Alonso and François Fleuret},
    year            = {2023},
    title           = {Transformers are Sample-Efficient World Models},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=vhFu1Acb0xb}
}

@inproceedings{Ha-Schmidthuber2018,
    author          = {David Ha and Jürgen Schmidhuber},
    year            = {2018},
    title           = {Recurrent World Models Facilitate Policy Evaluation},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {31},
    pages           = {},
    url             = {https://papers.nips.cc/paper_files/paper/2018/hash/2de5d16682c3c35007e4e92982f1a2ba-Abstract.html}
}

@inproceedings{Racaniere+2017,
author = {Racani\`{e}re, S\'{e}bastien and Weber, Th\'{e}ophane and Reichert, David P. and Buesing, Lars and Guez, Arthur and Rezende, Danilo and Badia, Adria Puigdom\`{e}nech and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Silver, David and Wierstra, Daan},
title = {Imagination-augmented agents for deep reinforcement learning},
year = {2017},
isbn = {9781510860964},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
pages = {5694–5705},
numpages = {12},
location = {Long Beach, California, USA},
series = {NIPS'17},
url             = {https://dl.acm.org/doi/10.5555/3295222.3295320},
}

@inproceedings{Kaiser+2020,
title={Model Based Reinforcement Learning for Atari},
author={Łukasz Kaiser and Mohammad Babaeizadeh and Piotr Miłos and Błażej Osiński and Roy H Campbell and Konrad Czechowski and Dumitru Erhan and Chelsea Finn and Piotr Kozakowski and Sergey Levine and Afroz Mohiuddin and Ryan Sepassi and George Tucker and Henryk Michalewski},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=S1xCPJHtDB}
}

@inproceedings{Hafner+2021,
title={Mastering Atari with Discrete World Models},
author={Danijar Hafner and Timothy P Lillicrap and Mohammad Norouzi and Jimmy Ba},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=0oabwyZbOu}
}

@article{Raffel+2020,
author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
year = {2020},
issue_date = {January 2020},
publisher = {JMLR.org},
volume = {21},
number = {1},
issn = {1532-4435},
abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {140},
numpages = {67},
keywords = {deep learning, attention based models, multi-task learning, natural language processing, transfer learning}
}


@inproceedings{Chang+2023,
  title = 	 {Muse: Text-To-Image Generation via Masked Generative Transformers},
  author =       {Chang, Huiwen and Zhang, Han and Barber, Jarred and Maschinot, Aaron and Lezama, Jose and Jiang, Lu and Yang, Ming-Hsuan and Murphy, Kevin Patrick and Freeman, William T. and Rubinstein, Michael and Li, Yuanzhen and Krishnan, Dilip},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {4055--4075},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/chang23b/chang23b.pdf},
  url = 	 {https://proceedings.mlr.press/v202/chang23b.html},
  abstract = 	 {We present Muse, a text-to-image Transformermodel that achieves state-of-the-art image genera-tion performance while being significantly moreefficient than diffusion or autoregressive models.Muse is trained on a masked modeling task indiscrete token space: given the text embeddingextracted from a pre-trained large language model(LLM), Muse learns to predict randomly maskedimage tokens. Compared to pixel-space diffusionmodels, such as Imagen and DALL-E 2, Muse issignificantly more efficient due to the use of dis-crete tokens and requires fewer sampling itera-tions; compared to autoregressive models such asParti, Muse is more efficient due to the use of par-allel decoding. The use of a pre-trained LLM en-ables fine-grained language understanding, whichtranslates to high-fidelity image generation andthe understanding of visual concepts such as ob-jects, their spatial relationships, pose, cardinalityetc. Our 900M parameter model achieves a newSOTA on CC3M, with an FID score of 6.06. TheMuse 3B parameter model achieves an FID of7.88 on zero-shot COCO evaluation, along with aCLIP score of 0.32. Muse also directly enables anumber of image editing applications without theneed to fine-tune or invert the model: inpainting,outpainting, and mask-free editing. More resultsand videos demonstrating editing are available at https://muse-icml.github.io/}
}


@inproceedings{Ramesh+2021,
  title = 	 {Zero-Shot Text-to-Image Generation},
  author =       {Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8821--8831},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/ramesh21a/ramesh21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/ramesh21a.html},
  abstract = 	 {Text-to-image generation has traditionally focused on finding better modeling assumptions for training on a fixed dataset. These assumptions might involve complex architectures, auxiliary losses, or side information such as object part labels or segmentation masks supplied during training. We describe a simple approach for this task based on a transformer that autoregressively models the text and image tokens as a single stream of data. With sufficient data and scale, our approach is competitive with previous domain-specific models when evaluated in a zero-shot fashion.}
}

@misc{Dhariwal+2020,
      title={Jukebox: A Generative Model for Music}, 
      author={Prafulla Dhariwal and Heewoo Jun and Christine Payne and Jong Wook Kim and Alec Radford and Ilya Sutskever},
      year={2020},
      eprint={2005.00341},
      archivePrefix={arXiv},
      primaryClass={eess.AS},
      url          = {https://arxiv.org/abs/2005.00341},
}

@misc{Child+2019,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1904.10509},
}

@inproceedings{Lewis+2020,
    author          = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K\"{u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt\"{a}schel, Tim and Riedel, Sebastian and Kiela, Douwe},
    year            = {2020},
    title           = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {33},
    pages           = {9459-9474},
    url             = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html}
}

@inproceedings{Petroni+2019,
    title = "Language Models as Knowledge Bases?",
    author = {Petroni, Fabio  and
      Rockt{\"a}schel, Tim  and
      Riedel, Sebastian  and
      Lewis, Patrick  and
      Bakhtin, Anton  and
      Wu, Yuxiang  and
      Miller, Alexander},
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1250",
    doi = "10.18653/v1/D19-1250",
    pages = "2463--2473",
    abstract = "Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as {``}fill-in-the-blank{''} cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at \url{https://github.com/facebookresearch/LAMA}.",
}

@inproceedings{Roberts+2020,
    title = "How Much Knowledge Can You Pack Into the Parameters of a Language Model?",
    author = "Roberts, Adam  and
      Raffel, Colin  and
      Shazeer, Noam",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.437",
    doi = "10.18653/v1/2020.emnlp-main.437",
    pages = "5418--5426",
    abstract = "It has recently been observed that neural language models trained on unstructured text can implicitly store and retrieve knowledge using natural language queries. In this short paper, we measure the practical utility of this approach by fine-tuning pre-trained models to answer questions without access to any external context or knowledge. We show that this approach scales with model size and performs competitively with open-domain systems that explicitly retrieve answers from an external knowledge source when answering questions. To facilitate reproducibility and future work, we release our code and trained models.",
}

@misc{Marcus2020,
      title={The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence}, 
      author={Gary Marcus},
      year={2020},
      eprint={2002.06177},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url          = {https://arxiv.org/abs/2002.06177},
}

@misc{Aghajanyan+2022,
      title={CM3: A Causal Masked Multimodal Model of the Internet}, 
      author={Armen Aghajanyan and Bernie Huang and Candace Ross and Vladimir Karpukhin and Hu Xu and Naman Goyal and Dmytro Okhonko and Mandar Joshi and Gargi Ghosh and Mike Lewis and Luke Zettlemoyer},
      year={2022},
      eprint={2201.07520},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2201.07520},
}

@misc{Alayrac+2022,
      title={Flamingo: a Visual Language Model for Few-Shot Learning}, 
      author={Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katie Millican and Malcolm Reynolds and Roman Ring and Eliza Rutherford and Serkan Cabi and Tengda Han and Zhitao Gong and Sina Samangooei and Marianne Monteiro and Jacob Menick and Sebastian Borgeaud and Andrew Brock and Aida Nematzadeh and Sahand Sharifzadeh and Mikolaj Binkowski and Ricardo Barreira and Oriol Vinyals and Andrew Zisserman and Karen Simonyan},
      year={2022},
      eprint={2204.14198},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2204.14198},
}

@misc{Chowdhery+2022,
      title={PaLM: Scaling Language Modeling with Pathways}, 
      author={Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and Parker Schuh and Kensen Shi and Sasha Tsvyashchenko and Joshua Maynez and Abhishek Rao and Parker Barnes and Yi Tay and Noam Shazeer and Vinodkumar Prabhakaran and Emily Reif and Nan Du and Ben Hutchinson and Reiner Pope and James Bradbury and Jacob Austin and Michael Isard and Guy Gur-Ari and Pengcheng Yin and Toju Duke and Anselm Levskaya and Sanjay Ghemawat and Sunipa Dev and Henryk Michalewski and Xavier Garcia and Vedant Misra and Kevin Robinson and Liam Fedus and Denny Zhou and Daphne Ippolito and David Luan and Hyeontaek Lim and Barret Zoph and Alexander Spiridonov and Ryan Sepassi and David Dohan and Shivani Agrawal and Mark Omernick and Andrew M. Dai and Thanumalayan Sankaranarayana Pillai and Marie Pellat and Aitor Lewkowycz and Erica Moreira and Rewon Child and Oleksandr Polozov and Katherine Lee and Zongwei Zhou and Xuezhi Wang and Brennan Saeta and Mark Diaz and Orhan Firat and Michele Catasta and Jason Wei and Kathy Meier-Hellstern and Douglas Eck and Jeff Dean and Slav Petrov and Noah Fiedel},
      year={2022},
      eprint={2204.02311},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2204.02311},
}

@article{Yu+2022,
title={Scaling Autoregressive Models for Content-Rich Text-to-Image Generation},
author={Jiahui Yu and Yuanzhong Xu and Jing Yu Koh and Thang Luong and Gunjan Baid and Zirui Wang and Vijay Vasudevan and Alexander Ku and Yinfei Yang and Burcu Karagol Ayan and Ben Hutchinson and Wei Han and Zarana Parekh and Xin Li and Han Zhang and Jason Baldridge and Yonghui Wu},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=AFDcYJKhND},
note={Featured Certification}
}

@inproceedings{Karpukhin+2020,
    title = "Dense Passage Retrieval for Open-Domain Question Answering",
    author = "Karpukhin, Vladimir  and
      Oguz, Barlas  and
      Min, Sewon  and
      Lewis, Patrick  and
      Wu, Ledell  and
      Edunov, Sergey  and
      Chen, Danqi  and
      Yih, Wen-tau",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.550",
    doi = "10.18653/v1/2020.emnlp-main.550",
    pages = "6769--6781",
    abstract = "Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9{\%}-19{\%} absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.",
}


@inproceedings{Radford+2021,
  title = 	 {Learning Transferable Visual Models From Natural Language Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8748--8763},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/radford21a/radford21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/radford21a.html},
  abstract = 	 {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on.}
}

@misc{Touvron+2023,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2302.13971},
}

@inproceedings{Wang+2023-Self-Instruct,
    title = "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
    author = "Wang, Yizhong  and
      Kordi, Yeganeh  and
      Mishra, Swaroop  and
      Liu, Alisa  and
      Smith, Noah A.  and
      Khashabi, Daniel  and
      Hajishirzi, Hannaneh",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.754",
    doi = "10.18653/v1/2023.acl-long.754",
    pages = "13484--13508",
    abstract = "Large {``}instruction-tuned{''} language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33{\%} absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5{\%} absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning.",
}

@inproceedings{Lewis+2020-BART,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    editor = "Jurafsky, Dan  and
      Chai, Joyce  and
      Schluter, Natalie  and
      Tetreault, Joel",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}


@inproceedings{Yasunaga+2023,
  title = 	 {Retrieval-Augmented Multimodal Language Modeling},
  author =       {Yasunaga, Michihiro and Aghajanyan, Armen and Shi, Weijia and James, Richard and Leskovec, Jure and Liang, Percy and Lewis, Mike and Zettlemoyer, Luke and Yih, Wen-Tau},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {39755--39769},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/yasunaga23a/yasunaga23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/yasunaga23a.html},
  abstract = 	 {Recent multimodal models such as DALL-E and CM3 have achieved remarkable progress in text-to-image and image-to-text generation. However, these models store all their knowledge (e.g., the appearance of the Eiffel Tower) in the model parameters, requiring increasingly larger models and training data to capture more knowledge. To integrate knowledge in a more scalable and modular way, we propose a retrieval-augmented multimodal model, which enables a base multimodal model (generator) to refer to relevant text and images fetched by a retriever from external memory (e.g., documents on the web). Specifically, for the retriever, we use a pretrained CLIP, and for the generator, we train a CM3 Transformer on the LAION dataset. Our resulting model, named Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can retrieve and generate both text and images. We show that RA-CM3 significantly outperforms baseline multimodal models such as DALL-E and CM3 on both image and caption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), while requiring much less compute for training ($&lt;$30% of DALL-E). Moreover, we show that RA-CM3 exhibits novel capabilities such as faithful image generation and multimodal in-context learning (e.g., image generation from demonstrations).}
}

@misc{Hu+2023,
      title={GAIA-1: A Generative World Model for Autonomous Driving}, 
      author={Anthony Hu and Lloyd Russell and Hudson Yeo and Zak Murez and George Fedoseev and Alex Kendall and Jamie Shotton and Gianluca Corrado},
      year={2023},
      eprint={2309.17080},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2309.17080},
}

@misc{Wang+2023-VALL-E,
      title={Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers}, 
      author={Chengyi Wang and Sanyuan Chen and Yu Wu and Ziqiang Zhang and Long Zhou and Shujie Liu and Zhuo Chen and Yanqing Liu and Huaming Wang and Jinyu Li and Lei He and Sheng Zhao and Furu Wei},
      year={2023},
      eprint={2301.02111},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2301.02111},
}

@misc{Yu+2023,
      title={Scaling Autoregressive Multi-Modal Models: Pretraining and Instruction Tuning}, 
      author={Lili Yu and Bowen Shi and Ramakanth Pasunuru and Benjamin Muller and Olga Golovneva and Tianlu Wang and Arun Babu and Binh Tang and Brian Karrer and Shelly Sheynin and Candace Ross and Adam Polyak and Russell Howes and Vasu Sharma and Puxin Xu and Hovhannes Tamoyan and Oron Ashual and Uriel Singer and Shang-Wen Li and Susan Zhang and Richard James and Gargi Ghosh and Yaniv Taigman and Maryam Fazel-Zarandi and Asli Celikyilmaz and Luke Zettlemoyer and Armen Aghajanyan},
      year={2023},
      eprint={2309.02591},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2309.02591},
}

@misc{Tamkin+2021,
      title={Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models}, 
      author={Alex Tamkin and Miles Brundage and Jack Clark and Deep Ganguli},
      year={2021},
      eprint={2102.02503},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2102.02503},
}

@misc{Leng-Yuan2023,
      title={Do LLM Agents Exhibit Social Behavior?}, 
      author={Yan Leng and Yuan Yuan},
      year={2023},
      eprint={2312.15198},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url          = {https://arxiv.org/abs/2312.15198},
}

@misc{Nakano+2022,
      title={WebGPT: Browser-assisted question-answering with human feedback}, 
      author={Reiichiro Nakano and Jacob Hilton and Suchir Balaji and Jeff Wu and Long Ouyang and Christina Kim and Christopher Hesse and Shantanu Jain and Vineet Kosaraju and William Saunders and Xu Jiang and Karl Cobbe and Tyna Eloundou and Gretchen Krueger and Kevin Button and Matthew Knight and Benjamin Chess and John Schulman},
      year={2022},
      eprint={2112.09332},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2112.09332},
}

@inproceedings{Guu+2020,
author = {Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Ming-Wei},
title = {REALM: retrieval-augmented language model pre-training},
year = {2020},
publisher = {JMLR.org},
abstract = {Language model pre-training has been shown to capture a surprising amount of world knowledge, crucial for NLP tasks such as question answering. However, this knowledge is stored implicitly in the parameters of a neural network, requiring everlarger networks to cover more facts.To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference. For the first time, we show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents.We demonstrate the effectiveness of Retrieval-Augmented Language Model pretraining (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA). We compare against state-of-the-art models for both explicit and implicit knowledge storage on three popular Open-QA benchmarks, and find that we outperform all previous methods by a significant margin (4-16\% absolute accuracy), while also providing qualitative benefits such as interpretability and modularity.},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
articleno = {368},
numpages = {10},
series = {ICML'20}
}

@inproceedings{Ouyang+2022,
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	pages = {27730--27744},
	publisher = {Curran Associates, Inc.},
	title = {Training language models to follow instructions with human feedback},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
	volume = {35},
	year = {2022},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf}}


@misc{Schulman+2017,
      title={Proximal Policy Optimization Algorithms}, 
      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},
      year={2017},
      eprint={1707.06347},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1707.06347},
}

@misc{Thoppilan+2022,
      title={LaMDA: Language Models for Dialog Applications}, 
      author={Romal Thoppilan and Daniel De Freitas and Jamie Hall and Noam Shazeer and Apoorv Kulshreshtha and Heng-Tze Cheng and Alicia Jin and Taylor Bos and Leslie Baker and Yu Du and YaGuang Li and Hongrae Lee and Huaixiu Steven Zheng and Amin Ghafouri and Marcelo Menegali and Yanping Huang and Maxim Krikun and Dmitry Lepikhin and James Qin and Dehao Chen and Yuanzhong Xu and Zhifeng Chen and Adam Roberts and Maarten Bosma and Vincent Zhao and Yanqi Zhou and Chung-Ching Chang and Igor Krivokon and Will Rusch and Marc Pickett and Pranesh Srinivasan and Laichee Man and Kathleen Meier-Hellstern and Meredith Ringel Morris and Tulsee Doshi and Renelito Delos Santos and Toju Duke and Johnny Soraker and Ben Zevenbergen and Vinodkumar Prabhakaran and Mark Diaz and Ben Hutchinson and Kristen Olson and Alejandra Molina and Erin Hoffman-John and Josh Lee and Lora Aroyo and Ravi Rajakumar and Alena Butryna and Matthew Lamm and Viktoriya Kuzmina and Joe Fenton and Aaron Cohen and Rachel Bernstein and Ray Kurzweil and Blaise Aguera-Arcas and Claire Cui and Marian Croak and Ed Chi and Quoc Le},
      year={2022},
      eprint={2201.08239},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2201.08239},
}

@misc{Schulman+2015,
      title={Trust Region Policy Optimization}, 
      author={John Schulman and Sergey Levine and Philipp Moritz and Michael I. Jordan and Pieter Abbeel},
      year={2015},
      eprint={1502.05477},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1502.05477},
}

@misc{Zheng+2023,
      title={Secrets of RLHF in Large Language Models Part I: PPO}, 
      author={Rui Zheng and Shihan Dou and Songyang Gao and Yuan Hua and Wei Shen and Binghai Wang and Yan Liu and Senjie Jin and Qin Liu and Yuhao Zhou and Limao Xiong and Lu Chen and Zhiheng Xi and Nuo Xu and Wenbin Lai and Minghao Zhu and Cheng Chang and Zhangyue Yin and Rongxiang Weng and Wensen Cheng and Haoran Huang and Tianxiang Sun and Hang Yan and Tao Gui and Qi Zhang and Xipeng Qiu and Xuanjing Huang},
      year={2023},
      eprint={2307.04964},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2307.04964},
}

@inproceedings{Radford+2023-Whisper,
author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
title = {Robust speech recognition via large-scale weak supervision},
year = {2023},
publisher = {JMLR.org},
abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
booktitle = {Proceedings of the 40th International Conference on Machine Learning},
articleno = {1182},
numpages = {27},
location = {Honolulu, Hawaii, USA},
series = {ICML'23},
url             = {https://dl.acm.org/doi/10.5555/3618408.3619590},
}

@misc{Baker+2022,
      title={Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos}, 
      author={Bowen Baker and Ilge Akkaya and Peter Zhokhov and Joost Huizinga and Jie Tang and Adrien Ecoffet and Brandon Houghton and Raul Sampedro and Jeff Clune},
      year={2022},
      eprint={2206.11795},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2206.11795},
}

@misc{Ramesh+2022,
      title={Hierarchical Text-Conditional Image Generation with CLIP Latents}, 
      author={Aditya Ramesh and Prafulla Dhariwal and Alex Nichol and Casey Chu and Mark Chen},
      year={2022},
      eprint={2204.06125},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2204.06125},
}

@techreport{Manning+2022,
    author      = {Sam Manning and Pamela Mishkin and Gillian Hadfield and Tyna Eloundou and Emily Eisne},
    institution = {OpenAI},
    title       = {A Research Agenda for Assessing the Economic Impacts of Code Generation Models},
    year        = {2022},
    url         = {https://openai.com/research/economic-impacts},
}


@inproceedings{Dickstein+2015,
  title = 	 {{Deep Unsupervised Learning using Nonequilibrium Thermodynamics}},
  author = 	 {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {2256--2265},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/sohl-dickstein15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  abstract = 	 {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.}
}

@inproceedings{Song-Ermon2020,
    author          = {Yang Song and Stefano Ermon},
    year            = {2020},
    title           = {{Improved Techniques for Training Score-Based Generative Models}},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {33},
    pages           = {},
    url             = {https://proceedings.neurips.cc/paper/2020/hash/92c3b916311a5517d9290576e3ea37ad-Abstract.html}
}


@inproceedings{Song-Ermon2019,
	author = {Song, Yang and Ermon, Stefano},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {{Generative Modeling by Estimating Gradients of the Data Distribution}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf}}


@inproceedings{Ho+2020,
    author          = {Jonathan Ho and Ajay Jain and Pieter Abbeel},
    year            = {2020},
    title           = {{Denoising Diffusion Probabilistic Models}},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {33},
    pages           = {},
    url             = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html}
}

@inproceedings{Aharoni+2019,
    title = "Massively Multilingual Neural Machine Translation",
    author = "Aharoni, Roee  and
      Johnson, Melvin  and
      Firat, Orhan",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1388",
    doi = "10.18653/v1/N19-1388",
    pages = "3874--3884",
    abstract = "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
}

@misc{Geminiteam+2023,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team and Rohan Anil and Sebastian Borgeaud and Yonghui Wu and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M. Dai and Anja Hauth and Katie Millican and David Silver and Slav Petrov and Melvin Johnson and Ioannis Antonoglou and Julian Schrittwieser and Amelia Glaese and Jilin Chen and Emily Pitler and Timothy Lillicrap and Angeliki Lazaridou and Orhan Firat and James Molloy and Michael Isard and Paul R. Barham and Tom Hennigan and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and Ryan Doherty and Eli Collins and Clemens Meyer and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and George Tucker and Enrique Piqueras and Maxim Krikun and Iain Barr and Nikolay Savinov and Ivo Danihelka and Becca Roelofs and Anaïs White and Anders Andreassen and Tamara von Glehn and Lakshman Yagati and Mehran Kazemi and Lucas Gonzalez and Misha Khalman and Jakub Sygnowski and Alexandre Frechette and Charlotte Smith and Laura Culp and Lev Proleev and Yi Luan and Xi Chen and James Lottes and Nathan Schucher and Federico Lebron and Alban Rrustemi and Natalie Clay and Phil Crone and Tomas Kocisky and Jeffrey Zhao and Bartek Perz and Dian Yu and Heidi Howard and Adam Bloniarz and Jack W. Rae and Han Lu and Laurent Sifre and Marcello Maggioni and Fred Alcober and Dan Garrette and Megan Barnes and Shantanu Thakoor and Jacob Austin and Gabriel Barth-Maron and William Wong and Rishabh Joshi and Rahma Chaabouni and Deeni Fatiha and Arun Ahuja and Ruibo Liu and Yunxuan Li and Sarah Cogan and Jeremy Chen and Chao Jia and Chenjie Gu and Qiao Zhang and Jordan Grimstad and Ale Jakse Hartman and Martin Chadwick and Gaurav Singh Tomar and Xavier Garcia and Evan Senter and Emanuel Taropa and Thanumalayan Sankaranarayana Pillai and Jacob Devlin and Michael Laskin and Diego de Las Casas and Dasha Valter and Connie Tao and Lorenzo Blanco and Adrià Puigdomènech Badia and David Reitter and Mianna Chen and Jenny Brennan and Clara Rivera and Sergey Brin and Shariq Iqbal and Gabriela Surita and Jane Labanowski and Abhi Rao and Stephanie Winkler and Emilio Parisotto and Yiming Gu and Kate Olszewska and Yujing Zhang and Ravi Addanki and Antoine Miech and Annie Louis and Laurent El Shafey and Denis Teplyashin and Geoff Brown and Elliot Catt and Nithya Attaluri and Jan Balaguer and Jackie Xiang and Pidong Wang and Zoe Ashwood and Anton Briukhov and Albert Webson and Sanjay Ganapathy and Smit Sanghavi and Ajay Kannan and Ming-Wei Chang and Axel Stjerngren and Josip Djolonga and Yuting Sun and Ankur Bapna and Matthew Aitchison and Pedram Pejman and Henryk Michalewski and Tianhe Yu and Cindy Wang and Juliette Love and Junwhan Ahn and Dawn Bloxwich and Kehang Han and Peter Humphreys and Thibault Sellam and James Bradbury and Varun Godbole and Sina Samangooei and Bogdan Damoc and Alex Kaskasoli and Sébastien M. R. Arnold and Vijay Vasudevan and Shubham Agrawal and Jason Riesa and Dmitry Lepikhin and Richard Tanburn and Srivatsan Srinivasan and Hyeontaek Lim and Sarah Hodkinson and Pranav Shyam and Johan Ferret and Steven Hand and Ankush Garg and Tom Le Paine and Jian Li and Yujia Li and Minh Giang and Alexander Neitz and Zaheer Abbas and Sarah York and Machel Reid and Elizabeth Cole and Aakanksha Chowdhery and Dipanjan Das and Dominika Rogozińska and Vitaly Nikolaev and Pablo Sprechmann and Zachary Nado and Lukas Zilka and Flavien Prost and Luheng He and Marianne Monteiro and Gaurav Mishra and Chris Welty and Josh Newlan and Dawei Jia and Miltiadis Allamanis and Clara Huiyi Hu and Raoul de Liedekerke and Justin Gilmer and Carl Saroufim and Shruti Rijhwani and Shaobo Hou and Disha Shrivastava and Anirudh Baddepudi and Alex Goldin and Adnan Ozturel and Albin Cassirer and Yunhan Xu and Daniel Sohn and Devendra Sachan and Reinald Kim Amplayo and Craig Swanson and Dessie Petrova and Shashi Narayan and Arthur Guez and Siddhartha Brahma and Jessica Landon and Miteyan Patel and Ruizhe Zhao and Kevin Villela and Luyu Wang and Wenhao Jia and Matthew Rahtz and Mai Giménez and Legg Yeung and Hanzhao Lin and James Keeling and Petko Georgiev and Diana Mincu and Boxi Wu and Salem Haykal and Rachel Saputro and Kiran Vodrahalli and James Qin and Zeynep Cankara and Abhanshu Sharma and Nick Fernando and Will Hawkins and Behnam Neyshabur and Solomon Kim and Adrian Hutter and Priyanka Agrawal and Alex Castro-Ros and George van den Driessche and Tao Wang and Fan Yang and Shuo-yiin Chang and Paul Komarek and Ross McIlroy and Mario Lučić and Guodong Zhang and Wael Farhan and Michael Sharman and Paul Natsev and Paul Michel and Yong Cheng and Yamini Bansal and Siyuan Qiao and Kris Cao and Siamak Shakeri and Christina Butterfield and Justin Chung and Paul Kishan Rubenstein and Shivani Agrawal and Arthur Mensch and Kedar Soparkar and Karel Lenc and Timothy Chung and Aedan Pope and Loren Maggiore and Jackie Kay and Priya Jhakra and Shibo Wang and Joshua Maynez and Mary Phuong and Taylor Tobin and Andrea Tacchetti and Maja Trebacz and Kevin Robinson and Yash Katariya and Sebastian Riedel and Paige Bailey and Kefan Xiao and Nimesh Ghelani and Lora Aroyo and Ambrose Slone and Neil Houlsby and Xuehan Xiong and Zhen Yang and Elena Gribovskaya and Jonas Adler and Mateo Wirth and Lisa Lee and Music Li and Thais Kagohara and Jay Pavagadhi and Sophie Bridgers and Anna Bortsova and Sanjay Ghemawat and Zafarali Ahmed and Tianqi Liu and Richard Powell and Vijay Bolina and Mariko Iinuma and Polina Zablotskaia and James Besley and Da-Woon Chung and Timothy Dozat and Ramona Comanescu and Xiance Si and Jeremy Greer and Guolong Su and Martin Polacek and Raphaël Lopez Kaufman and Simon Tokumine and Hexiang Hu and Elena Buchatskaya and Yingjie Miao and Mohamed Elhawaty and Aditya Siddhant and Nenad Tomasev and Jinwei Xing and Christina Greer and Helen Miller and Shereen Ashraf and Aurko Roy and Zizhao Zhang and Ada Ma and Angelos Filos and Milos Besta and Rory Blevins and Ted Klimenko and Chih-Kuan Yeh and Soravit Changpinyo and Jiaqi Mu and Oscar Chang and Mantas Pajarskas and Carrie Muir and Vered Cohen and Charline Le Lan and Krishna Haridasan and Amit Marathe and Steven Hansen and Sholto Douglas and Rajkumar Samuel and Mingqiu Wang and Sophia Austin and Chang Lan and Jiepu Jiang and Justin Chiu and Jaime Alonso Lorenzo and Lars Lowe Sjösund and Sébastien Cevey and Zach Gleicher and Thi Avrahami and Anudhyan Boral and Hansa Srinivasan and Vittorio Selo and Rhys May and Konstantinos Aisopos and Léonard Hussenot and Livio Baldini Soares and Kate Baumli and Michael B. Chang and Adrià Recasens and Ben Caine and Alexander Pritzel and Filip Pavetic and Fabio Pardo and Anita Gergely and Justin Frye and Vinay Ramasesh and Dan Horgan and Kartikeya Badola and Nora Kassner and Subhrajit Roy and Ethan Dyer and Víctor Campos and Alex Tomala and Yunhao Tang and Dalia El Badawy and Elspeth White and Basil Mustafa and Oran Lang and Abhishek Jindal and Sharad Vikram and Zhitao Gong and Sergi Caelles and Ross Hemsley and Gregory Thornton and Fangxiaoyu Feng and Wojciech Stokowiec and Ce Zheng and Phoebe Thacker and Çağlar Ünlü and Zhishuai Zhang and Mohammad Saleh and James Svensson and Max Bileschi and Piyush Patil and Ankesh Anand and Roman Ring and Katerina Tsihlas and Arpi Vezer and Marco Selvi and Toby Shevlane and Mikel Rodriguez and Tom Kwiatkowski and Samira Daruki and Keran Rong and Allan Dafoe and Nicholas FitzGerald and Keren Gu-Lemberg and Mina Khan and Lisa Anne Hendricks and Marie Pellat and Vladimir Feinberg and James Cobon-Kerr and Tara Sainath and Maribeth Rauh and Sayed Hadi Hashemi and Richard Ives and Yana Hasson and YaGuang Li and Eric Noland and Yuan Cao and Nathan Byrd and Le Hou and Qingze Wang and Thibault Sottiaux and Michela Paganini and Jean-Baptiste Lespiau and Alexandre Moufarek and Samer Hassan and Kaushik Shivakumar and Joost van Amersfoort and Amol Mandhane and Pratik Joshi and Anirudh Goyal and Matthew Tung and Andrew Brock and Hannah Sheahan and Vedant Misra and Cheng Li and Nemanja Rakićević and Mostafa Dehghani and Fangyu Liu and Sid Mittal and Junhyuk Oh and Seb Noury and Eren Sezener and Fantine Huot and Matthew Lamm and Nicola De Cao and Charlie Chen and Gamaleldin Elsayed and Ed Chi and Mahdis Mahdieh and Ian Tenney and Nan Hua and Ivan Petrychenko and Patrick Kane and Dylan Scandinaro and Rishub Jain and Jonathan Uesato and Romina Datta and Adam Sadovsky and Oskar Bunyan and Dominik Rabiej and Shimu Wu and John Zhang and Gautam Vasudevan and Edouard Leurent and Mahmoud Alnahlawi and Ionut Georgescu and Nan Wei and Ivy Zheng and Betty Chan and Pam G Rabinovitch and Piotr Stanczyk and Ye Zhang and David Steiner and Subhajit Naskar and Michael Azzam and Matthew Johnson and Adam Paszke and Chung-Cheng Chiu and Jaume Sanchez Elias and Afroz Mohiuddin and Faizan Muhammad and Jin Miao and Andrew Lee and Nino Vieillard and Sahitya Potluri and Jane Park and Elnaz Davoodi and Jiageng Zhang and Jeff Stanway and Drew Garmon and Abhijit Karmarkar and Zhe Dong and Jong Lee and Aviral Kumar and Luowei Zhou and Jonathan Evens and William Isaac and Zhe Chen and Johnson Jia and Anselm Levskaya and Zhenkai Zhu and Chris Gorgolewski and Peter Grabowski and Yu Mao and Alberto Magni and Kaisheng Yao and Javier Snaider and Norman Casagrande and Paul Suganthan and Evan Palmer and Geoffrey Irving and Edward Loper and Manaal Faruqui and Isha Arkatkar and Nanxin Chen and Izhak Shafran and Michael Fink and Alfonso Castaño and Irene Giannoumis and Wooyeol Kim and Mikołaj Rybiński and Ashwin Sreevatsa and Jennifer Prendki and David Soergel and Adrian Goedeckemeyer and Willi Gierke and Mohsen Jafari and Meenu Gaba and Jeremy Wiesner and Diana Gage Wright and Yawen Wei and Harsha Vashisht and Yana Kulizhskaya and Jay Hoover and Maigo Le and Lu Li and Chimezie Iwuanyanwu and Lu Liu and Kevin Ramirez and Andrey Khorlin and Albert Cui and Tian LIN and Marin Georgiev and Marcus Wu and Ricardo Aguilar and Keith Pallo and Abhishek Chakladar and Alena Repina and Xihui Wu and Tom van der Weide and Priya Ponnapalli and Caroline Kaplan and Jiri Simsa and Shuangfeng Li and Olivier Dousse and Fan Yang and Jeff Piper and Nathan Ie and Minnie Lui and Rama Pasumarthi and Nathan Lintz and Anitha Vijayakumar and Lam Nguyen Thiet and Daniel Andor and Pedro Valenzuela and Cosmin Paduraru and Daiyi Peng and Katherine Lee and Shuyuan Zhang and Somer Greene and Duc Dung Nguyen and Paula Kurylowicz and Sarmishta Velury and Sebastian Krause and Cassidy Hardin and Lucas Dixon and Lili Janzer and Kiam Choo and Ziqiang Feng and Biao Zhang and Achintya Singhal and Tejasi Latkar and Mingyang Zhang and Quoc Le and Elena Allica Abellan and Dayou Du and Dan McKinnon and Natasha Antropova and Tolga Bolukbasi and Orgad Keller and David Reid and Daniel Finchelstein and Maria Abi Raad and Remi Crocker and Peter Hawkins and Robert Dadashi and Colin Gaffney and Sid Lall and Ken Franko and Egor Filonov and Anna Bulanova and Rémi Leblond and Vikas Yadav and Shirley Chung and Harry Askham and Luis C. Cobo and Kelvin Xu and Felix Fischer and Jun Xu and Christina Sorokin and Chris Alberti and Chu-Cheng Lin and Colin Evans and Hao Zhou and Alek Dimitriev and Hannah Forbes and Dylan Banarse and Zora Tung and Jeremiah Liu and Mark Omernick and Colton Bishop and Chintu Kumar and Rachel Sterneck and Ryan Foley and Rohan Jain and Swaroop Mishra and Jiawei Xia and Taylor Bos and Geoffrey Cideron and Ehsan Amid and Francesco Piccinno and Xingyu Wang and Praseem Banzal and Petru Gurita and Hila Noga and Premal Shah and Daniel J. Mankowitz and Alex Polozov and Nate Kushman and Victoria Krakovna and Sasha Brown and MohammadHossein Bateni and Dennis Duan and Vlad Firoiu and Meghana Thotakuri and Tom Natan and Anhad Mohananey and Matthieu Geist and Sidharth Mudgal and Sertan Girgin and Hui Li and Jiayu Ye and Ofir Roval and Reiko Tojo and Michael Kwong and James Lee-Thorp and Christopher Yew and Quan Yuan and Sumit Bagri and Danila Sinopalnikov and Sabela Ramos and John Mellor and Abhishek Sharma and Aliaksei Severyn and Jonathan Lai and Kathy Wu and Heng-Tze Cheng and David Miller and Nicolas Sonnerat and Denis Vnukov and Rory Greig and Jennifer Beattie and Emily Caveness and Libin Bai and Julian Eisenschlos and Alex Korchemniy and Tomy Tsai and Mimi Jasarevic and Weize Kong and Phuong Dao and Zeyu Zheng and Frederick Liu and Fan Yang and Rui Zhu and Mark Geller and Tian Huey Teh and Jason Sanmiya and Evgeny Gladchenko and Nejc Trdin and Andrei Sozanschi and Daniel Toyama and Evan Rosen and Sasan Tavakkol and Linting Xue and Chen Elkind and Oliver Woodman and John Carpenter and George Papamakarios and Rupert Kemp and Sushant Kafle and Tanya Grunina and Rishika Sinha and Alice Talbert and Abhimanyu Goyal and Diane Wu and Denese Owusu-Afriyie and Cosmo Du and Chloe Thornton and Jordi Pont-Tuset and Pradyumna Narayana and Jing Li and Sabaer Fatehi and John Wieting and Omar Ajmeri and Benigno Uria and Tao Zhu and Yeongil Ko and Laura Knight and Amélie Héliou and Ning Niu and Shane Gu and Chenxi Pang and Dustin Tran and Yeqing Li and Nir Levine and Ariel Stolovich and Norbert Kalb and Rebeca Santamaria-Fernandez and Sonam Goenka and Wenny Yustalim and Robin Strudel and Ali Elqursh and Balaji Lakshminarayanan and Charlie Deck and Shyam Upadhyay and Hyo Lee and Mike Dusenberry and Zonglin Li and Xuezhi Wang and Kyle Levin and Raphael Hoffmann and Dan Holtmann-Rice and Olivier Bachem and Summer Yue and Sho Arora and Eric Malmi and Daniil Mirylenka and Qijun Tan and Christy Koh and Soheil Hassas Yeganeh and Siim Põder and Steven Zheng and Francesco Pongetti and Mukarram Tariq and Yanhua Sun and Lucian Ionita and Mojtaba Seyedhosseini and Pouya Tafti and Ragha Kotikalapudi and Zhiyu Liu and Anmol Gulati and Jasmine Liu and Xinyu Ye and Bart Chrzaszcz and Lily Wang and Nikhil Sethi and Tianrun Li and Ben Brown and Shreya Singh and Wei Fan and Aaron Parisi and Joe Stanton and Chenkai Kuang and Vinod Koverkathu and Christopher A. Choquette-Choo and Yunjie Li and TJ Lu and Abe Ittycheriah and Prakash Shroff and Pei Sun and Mani Varadarajan and Sanaz Bahargam and Rob Willoughby and David Gaddy and Ishita Dasgupta and Guillaume Desjardins and Marco Cornero and Brona Robenek and Bhavishya Mittal and Ben Albrecht and Ashish Shenoy and Fedor Moiseev and Henrik Jacobsson and Alireza Ghaffarkhah and Morgane Rivière and Alanna Walton and Clément Crepy and Alicia Parrish and Yuan Liu and Zongwei Zhou and Clement Farabet and Carey Radebaugh and Praveen Srinivasan and Claudia van der Salm and Andreas Fidjeland and Salvatore Scellato and Eri Latorre-Chimoto and Hanna Klimczak-Plucińska and David Bridson and Dario de Cesare and Tom Hudson and Piermaria Mendolicchio and Lexi Walker and Alex Morris and Ivo Penchev and Matthew Mauger and Alexey Guseynov and Alison Reid and Seth Odoom and Lucia Loher and Victor Cotruta and Madhavi Yenugula and Dominik Grewe and Anastasia Petrushkina and Tom Duerig and Antonio Sanchez and Steve Yadlowsky and Amy Shen and Amir Globerson and Adam Kurzrok and Lynette Webb and Sahil Dua and Dong Li and Preethi Lahoti and Surya Bhupatiraju and Dan Hurt and Haroon Qureshi and Ananth Agarwal and Tomer Shani and Matan Eyal and Anuj Khare and Shreyas Rammohan Belle and Lei Wang and Chetan Tekur and Mihir Sanjay Kale and Jinliang Wei and Ruoxin Sang and Brennan Saeta and Tyler Liechty and Yi Sun and Yao Zhao and Stephan Lee and Pandu Nayak and Doug Fritz and Manish Reddy Vuyyuru and John Aslanides and Nidhi Vyas and Martin Wicke and Xiao Ma and Taylan Bilal and Evgenii Eltyshev and Daniel Balle and Nina Martin and Hardie Cate and James Manyika and Keyvan Amiri and Yelin Kim and Xi Xiong and Kai Kang and Florian Luisier and Nilesh Tripuraneni and David Madras and Mandy Guo and Austin Waters and Oliver Wang and Joshua Ainslie and Jason Baldridge and Han Zhang and Garima Pruthi and Jakob Bauer and Feng Yang and Riham Mansour and Jason Gelman and Yang Xu and George Polovets and Ji Liu and Honglong Cai and Warren Chen and XiangHai Sheng and Emily Xue and Sherjil Ozair and Adams Yu and Christof Angermueller and Xiaowei Li and Weiren Wang and Julia Wiesinger and Emmanouil Koukoumidis and Yuan Tian and Anand Iyer and Madhu Gurumurthy and Mark Goldenson and Parashar Shah and MK Blake and Hongkun Yu and Anthony Urbanowicz and Jennimaria Palomaki and Chrisantha Fernando and Kevin Brooks and Ken Durden and Harsh Mehta and Nikola Momchev and Elahe Rahimtoroghi and Maria Georgaki and Amit Raul and Sebastian Ruder and Morgan Redshaw and Jinhyuk Lee and Komal Jalan and Dinghua Li and Ginger Perng and Blake Hechtman and Parker Schuh and Milad Nasr and Mia Chen and Kieran Milan and Vladimir Mikulik and Trevor Strohman and Juliana Franco and Tim Green and Demis Hassabis and Koray Kavukcuoglu and Jeffrey Dean and Oriol Vinyals},
      year={2023},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2312.11805},
}

@inproceedings{Shazeer+2017,
      title={Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer}, 
      author={Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
      year={2017},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=B1ckMDqlg}
}


@inproceedings{Nichol+2022,
  title = 	 {{GLIDE}: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla and Ramesh, Aditya and Shyam, Pranav and Mishkin, Pamela and Mcgrew, Bob and Sutskever, Ilya and Chen, Mark},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {16784--16804},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/nichol22a/nichol22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/nichol22a.html},
  abstract = 	 {Diffusion models have recently been shown to generate high-quality synthetic images, especially when paired with a guidance technique to trade off diversity for fidelity. We explore diffusion models for the problem of text-conditional image synthesis and compare two different guidance strategies: CLIP guidance and classifier-free guidance. We find that the latter is preferred by human evaluators for both photorealism and caption similarity, and often produces photorealistic samples. Samples from a 3.5&nbsp;billion parameter text-conditional diffusion model using classifier-free guidance are favored by human evaluators to those from DALL-E, even when the latter uses expensive CLIP reranking. Additionally, we find that our models can be fine-tuned to perform image inpainting, enabling powerful text-driven image editing. We train a smaller model on a filtered dataset and release the code and weights at https://github.com/openai/glide-text2im.}
}

@inproceedings{Lin+2022,
    author          = {Stephanie Lin and Jacob Hilton and Owain Evans},
    year            = {2022},
    title           = {Teaching Models to Express Their Uncertainty in Words},
    booktitle       = {Transactions on Machine Learning Research},
    volume          = {},
    pages           = {},
    url             = {https://openai.com/research/teaching-models-to-express-their-uncertainty-in-words}
}

@techreport{Weng-Brockman2022,
    author      = {Lilian Weng and Greg Brockman},
    institution = {OpenAI},
    title       = {Techniques for Training Large Neural Networks},
    url        = {https://openai.com/research/techniques-for-training-large-neural-networks},
    year       = {2022}
}

@techreport{Nichol2022,
    author      = {Alex Nichol},
    institution = {OpenAI},
    title       = {DALL-E2 Pre-training Mitigations},
    year        = {2022},
    url         = {https://openai.com/research/dall-e-2-pre-training-mitigations},
}

@inbook{Lehman+2024,
    author         = {Joel Lehman and Jonathan Gordon Shawn Jain and Kamal Ndousse and Cathy Yah and Kenneth O. Stanley},
    chapter        = {Evolution through Large Models},
    editor         = {Wolfgang Banzhaf and Penousal Machado and Mengjie Zhang},
    pages          = {331-366},
    publisher      = {Springer Singapore},
    title          = {Handbook of Evolutionary Machine Learning},
    year           = {2024},
    url            = {https://link.springer.com/chapter/10.1007/978-981-99-3814-8_11},
}

@misc{Saunders+2022,
      title={Self-critiquing models for assisting human evaluators}, 
      author={William Saunders and Catherine Yeh and Jeff Wu and Steven Bills and Long Ouyang and Jonathan Ward and Jan Leike},
      year={2022},
      eprint={2206.05802},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://openai.com/research/critiques},
}

@misc{Khlaaf+2022,
      title={A Hazard Analysis Framework for Code Synthesis Large Language Models}, 
      author={Heidy Khlaaf and Pamela Mishkin and Joshua Achiam and Gretchen Krueger and Miles Brundage},
      year={2022},
      eprint={2207.14157},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url          = {https://openai.com/research/a-hazard-analysis-framework-for-code-synthesis-large-language-models},
}

@misc{Goldstein+2023,
      title={Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations}, 
      author={Josh A. Goldstein and Girish Sastry and Micah Musser and Renee DiResta and Matthew Gentzel and Katerina Sedova},
      year={2023},
      eprint={2301.04246},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url          = {https://openai.com/research/forecasting-misuse},
}

@misc{Eloundou+2023,
      title={GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models}, 
      author={Tyna Eloundou and Sam Manning and Pamela Mishkin and Daniel Rock},
      year={2023},
      eprint={2303.10130},
      archivePrefix={arXiv},
      primaryClass={econ.GN},
      url          = {https://openai.com/research/gpts-are-gpts},
}

@misc{Chen+2021,
      title={Evaluating Large Language Models Trained on Code}, 
      author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
      year={2021},
      eprint={2107.03374},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2107.03374},
}

@techreport{Leike+2022,
    author      = {Jan Leike and John Schulman and Jeffrey Wu},
    institution = {OpenAI},
    title       = {Our Approach to Alignment Research},
    year        = {2022},
    url         = {https://openai.com/blog/our-approach-to-alignment-research},
}

@techreport{Leike+2023,
    author      = {Jan Leike and Jeffrey Wu and Steven Bills and William Saunders and Leo Gao and Henk Tillman and Daniel Mossing},
    institution = {OpenAI},
    title       = {Language Models Can Explain Neurons in Language Models},
    year        = {2023},
    url         = {https://openai.com/research/language-models-can-explain-neurons-in-language-models},
}

@misc{Anderljung+2023,
      title={Frontier AI Regulation: Managing Emerging Risks to Public Safety}, 
      author={Markus Anderljung and Joslyn Barnhart and Anton Korinek and Jade Leung and Cullen O'Keefe and Jess Whittlestone and Shahar Avin and Miles Brundage and Justin Bullock and Duncan Cass-Beggs and Ben Chang and Tantum Collins and Tim Fist and Gillian Hadfield and Alan Hayes and Lewis Ho and Sara Hooker and Eric Horvitz and Noam Kolt and Jonas Schuett and Yonadav Shavit and Divya Siddarth and Robert Trager and Kevin Wolf},
      year={2023},
      eprint={2307.03718},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      institution = {OpenAI},
      url          = {https://openai.com/research/frontier-ai-regulation},
}

@misc{Shoker+2023,
      title={Confidence-Building Measures for Artificial Intelligence: Workshop Proceedings}, 
      author={Sarah Shoker and Andrew Reddie and Sarah Barrington and Ruby Booth and Miles Brundage and Husanjot Chahal and Michael Depp and Bill Drexel and Ritwik Gupta and Marina Favaro and Jake Hecla and Alan Hickey and Margarita Konaev and Kirthi Kumar and Nathan Lambert and Andrew Lohn and Cullen O'Keefe and Nazneen Rajani and Michael Sellitto and Robert Trager and Leah Walker and Alexa Wehsener and Jessica Young},
      year={2023},
      eprint={2308.00862},
      archivePrefix={arXiv},
      primaryClass={cs.CY},
      url          = {https://openai.com/research/confidence-building-measures-for-artificial-intelligence},
}

@techreport{OpenAI2023-GPT4V,
    author      = {OpenAI},
    institution = {OpenAI},
    title       = {GPT-4V(ision) System Card},
    year        = {2023},
    url         = {https://openai.com/research/gpt-4v-system-card},
}

@misc{Yang+2023-GPT-4V,
      title={The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)}, 
      author={Zhengyuan Yang and Linjie Li and Kevin Lin and Jianfeng Wang and Chung-Ching Lin and Zicheng Liu and Lijuan Wang},
      year={2023},
      eprint={2309.17421},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2309.17421},
}

@misc{Perot+2023,
      title={LMDX: Language Model-based Document Information Extraction and Localization}, 
      author={Vincent Perot and Kai Kang and Florian Luisier and Guolong Su and Xiaoyu Sun and Ramya Sree Boppana and Zilong Wang and Jiaqi Mu and Hao Zhang and Nan Hua},
      year={2023},
      eprint={2309.10952},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2309.10952},
}

@techreport{OpenAI2023DallE3,
    author      = {OpenAI},
    institution = {OpenAI},
    title       = {DALL-E3 System Card},
    year        = {2023},
    url         = {https://openai.com/research/dall-e-3-system-card},
}

@techreport{Shavit+2023,
    author      = {Yonadav Shavit and Sandhini Agarwal and Miles Brundage},
    institution = {OpenAI},
    title       = {Practices for Governing Agentic AI Systems},
    year        = {2023},
    url         = {https://openai.com/research/practices-for-governing-agentic-ai-systems},
}

@misc{Burns+2023,
      title={Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision}, 
      author={Collin Burns and Pavel Izmailov and Jan Hendrik Kirchner and Bowen Baker and Leo Gao and Leopold Aschenbrenner and Yining Chen and Adrien Ecoffet and Manas Joglekar and Jan Leike and Ilya Sutskever and Jeff Wu},
      year={2023},
      eprint={2312.09390},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://openai.com/research/weak-to-strong-generalization},
}

@techreport{Patwardhan+2024,
    author      = {Tejal Patwardhan and Kevin Liu and Todor Markov and Neil Chowdhury and Dillon Leet and Natalie Cone and Caitlin Maltbie and Joost Huizinga and Carroll Wainwright and Shawn (Froggi) Jackson and Steven Adler and Rocco Casagrande and Aleksander Mandry},
    institution = {OpenAI},
    title       = {Building an early warning system for LLM-aided biological threat creation},
    year        = {2024},
    url         = {https://openai.com/research/building-an-early-warning-system-for-llm-aided-biological-threat-creation},
}

@inproceedings{Fu+2023,
title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
author={Daniel Y Fu and Tri Dao and Khaled Kamal Saab and Armin W Thomas and Atri Rudra and Christopher Re},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=COZDy0WYGg}
}

@inproceedings{Gu+2022,
title={Efficiently Modeling Long Sequences with Structured State Spaces},
author={Albert Gu and Karan Goel and Christopher Re},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=uYLFoz1vlAC}
}

@article{Zhou+2020,
	author = {Jie Zhou and Ganqu Cui and Shengding Hu and Zhengyan Zhang and Cheng Yang and Zhiyuan Liu and Lifeng Wang and Changcheng Li and Maosong Sun},
	journal = {AI Open},
	pages = {57-81},
	title = {Graph neural networks: A review of methods and applications},
	volume = {1},
	year = {2020},
    url             = {https://www.sciencedirect.com/science/article/pii/S2666651021000012},
}

@article{Wu+2021,
   title={A Comprehensive Survey on Graph Neural Networks},
   volume={32},
   ISSN={2162-2388},
   url={http://dx.doi.org/10.1109/TNNLS.2020.2978386},
   DOI={10.1109/tnnls.2020.2978386},
   number={1},
   journal={IEEE Transactions on Neural Networks and Learning Systems},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
   year={2021},
   month={jan}, pages={4–24} 
}

@article{Velickovic2023,
	author = {Petar Veli{\v c}kovi{\'c}},
	journal = {Current Opinion in Structural Biology},
	pages = {102538},
	title = {Everything is connected: Graph neural networks},
	volume = {79},
	year = {2023},
    url             = {https://www.sciencedirect.com/science/article/pii/S0959440X2300012X},
}

@inproceedings{Dhariwal-Nichol2021,
title={Diffusion Models Beat {GAN}s on Image Synthesis},
author={Prafulla Dhariwal and Alexander Quinn Nichol},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=AAWuCvzaVt}
}

@misc{Luo2022,
      title={Understanding Diffusion Models: A Unified Perspective}, 
      author={Calvin Luo},
      year={2022},
      eprint={2208.11970},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2208.11970},
}

@article{Hyvarinen2005,
    author          = {Aapo Hyvärinen},
    year            = {2005},
    title           = {Estimation of Non-Normalized Statistical Models by Score Matching},
    journal         = {Journal of Machine Learning Research},
    volume          = {6},
    number          = {24},
    pages           = {695-709},
    url             = {https://jmlr.org/papers/v6/hyvarinen05a.html}
}

@inproceedings{Rombach+2022,
    author          = {Robin Rombach and Andreas Blattmann and Dominik Lorenz and Patrick Esser and Björn Ommer},
    year            = {2022},
    title           = {High-Resolution Image Systhesis with Latent Diffusion Models},
    booktitle       = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    volume          = {},
    pages           = {10684-10695},
    url             = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html}
}
@InProceedings{Esser2021,
    author    = {Esser, Patrick and Rombach, Robin and Ommer, Bjorn},
    title     = {Taming Transformers for High-Resolution Image Synthesis},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {12873-12883}
}

@inproceedings{Ronneberger+2015,
	address = {Cham},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	booktitle = {Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015},
	editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
	pages = {234--241},
	publisher = {Springer International Publishing},
	title = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
	year = {2015},
    url             = {https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28},
}


@article{Yang+2023-Diff,
	author = {Yang, Ruihan and Srivastava, Prakhar and Mandt, Stephan},
	journal = {Entropy},
	number = {10},
	title = {Diffusion Probabilistic Modeling for Video Generation},
	volume = {25},
	year = {2023},
    url             = {https://www.mdpi.com/1099-4300/25/10/1469},
}

@article{Kobyzev+2021,
author = {I. Kobyzev and S. D. Prince and M. A. Brubaker},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Normalizing Flows: An Introduction and Review of Current Methods},
year = {2021},
volume = {43},
number = {11},
issn = {1939-3539},
pages = {3964-3979},
abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
keywords = {estimation;jacobian matrices;mathematical model;training;computational modeling;context modeling;random variables},
doi = {10.1109/TPAMI.2020.2992934},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {nov},
url             = {https://www.computer.org/csdl/journal/tp/2021/11/09089305/1jDwlyVxAwE},
}

@article{Papamakarios+2021,
author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
title = {Normalizing flows for probabilistic modeling and inference},
year = {2021},
issue_date = {January 2021},
publisher = {JMLR.org},
volume = {22},
number = {1},
issn = {1532-4435},
abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
journal = {J. Mach. Learn. Res.},
month = {jan},
articleno = {57},
numpages = {64},
keywords = {generative models, probabilistic inference, probabilistic modeling, invertible neural networks, normalizing flows},
url             = {https://dl.acm.org/doi/abs/10.5555/3546258.3546315},
}

@misc{Brock+2019,
      title={Large Scale GAN Training for High Fidelity Natural Image Synthesis}, 
      author={Andrew Brock and Jeff Donahue and Karen Simonyan},
      year={2019},
      eprint={1809.11096},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1809.11096},
}

@inproceedings{Saharia+2022,
title={Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding},
author={Chitwan Saharia and William Chan and Saurabh Saxena and Lala Li and Jay Whang and Emily Denton and Seyed Kamyar Seyed Ghasemipour and Raphael Gontijo-Lopes and Burcu Karagol Ayan‎ and Tim Salimans and Jonathan Ho and David J. Fleet and Mohammad Norouzi},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=08Yk-n5l2Al}
}


@inproceedings{Jaegle+2021,
  title = 	 {Perceiver: General Perception with Iterative Attention},
  author =       {Jaegle, Andrew and Gimeno, Felix and Brock, Andy and Vinyals, Oriol and Zisserman, Andrew and Carreira, Joao},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {4651--4664},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/jaegle21a/jaegle21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/jaegle21a.html},
  abstract = 	 {Biological systems understand the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver {–} a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.}
}

@inproceedings{Huang+2021,
title={{A Variational Perspective on Diffusion-Based Generative Models and Score Matching}},
author={Chin-Wei Huang and Jae Hyun Lim and Aaron Courville},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=bXehDYUjjXi}
}

@article{Vincent2011,
    author          = {Pascal Vincent},
    year            = {2011},
    title           = {A Connection between Score Matching and Denoising Autoencoders},
    journal         = {Neural Computation},
    volume          = {23},
    number          = {7},
    pages           = {1661-1674},
    url             = {https://direct.mit.edu/neco/article/23/7/1661/7677/A-Connection-Between-Score-Matching-and-Denoising}
}

@inproceedings{Song+2021NeurIPS,
    author          = {Yang Song and Conor Durkan and Iain Murray and Stefano Ermon},
    year            = {2021},
    title           = {{Maximum Likelihood Training of Score-Based Diffusion Models}},
    booktitle       = {Advances in Neural Information Processing Systems},
    volume          = {34},
    pages           = {},
    url             = {https://proceedings.neurips.cc/paper/2021/hash/0a9fdbb17feb6ccb7ec405cfb85222c4-Abstract.html},
}

@inproceedings{Durkan+2019,
 author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Neural Spline Flows},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/7ac71d433f282034e088473244df8c02-Paper.pdf},
 volume = {32},
 year = {2019}
}


@misc{Tzen-Raginsky2019,
      title={{Neural Stochastic Differential Equations: Deep Latent Gaussian Models in the Diffusion Limit}}, 
      author={Belinda Tzen and Maxim Raginsky},
      year={2019},
      eprint={1905.09883},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1905.09883},
}

@inproceedings{Song+2021ICLR,
title={{Score-Based Generative Modeling through Stochastic Differential Equations}},
author={Yang Song and Jascha Sohl-Dickstein and Diederik P Kingma and Abhishek Kumar and Stefano Ermon and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PxTIG12RRHS}
}

@inproceedings{Jang+2017,
title={Categorical Reparameterization with Gumbel-Softmax},
author={Eric Jang and Shixiang Gu and Ben Poole},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rkE3y85ee}
}

@inproceedings{Maddison+2017,
title={The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables},
author={Chris J. Maddison and Andriy Mnih and Yee Whye Teh},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=S1jE5L5gl}
}


@inproceedings{Reed+2016,
  title = 	 {Generative Adversarial Text to Image Synthesis},
  author = 	 {Reed, Scott and Akata, Zeynep and Yan, Xinchen and Logeswaran, Lajanugen and Schiele, Bernt and Lee, Honglak},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1060--1069},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/reed16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/reed16.html},
  abstract = 	 {Automatic synthesis of realistic images from text would be interesting and useful, but current AI systems are still far from this goal. However, in recent years generic and powerful recurrent neural network architectures have been developed to learn discriminative text feature representations. Meanwhile, deep convolutional generative adversarial networks (GANs) have begun to generate highly compelling images of specific categories such as faces, album covers, room interiors and flowers. In this work, we develop a novel deep architecture and GAN formulation to effectively bridge these advances in text and image modeling, translating visual concepts from characters to pixels. We demonstrate the capability of our model to generate plausible images of birds and flowers from detailed text descriptions.}
}

@inproceedings{Peebles-Xie2023,
    author    = {Peebles, William and Xie, Saining},
    title     = {Scalable Diffusion Models with Transformers},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    month     = {October},
    year      = {2023},
    pages     = {4195-4205},
    url             = {https://openaccess.thecvf.com/content/ICCV2023/html/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.html},
}

@inproceedings{Arnab+2021,
  author={Arnab, Anurag and Dehghani, Mostafa and Heigold, Georg and Sun, Chen and Lučić, Mario and Schmid, Cordelia},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={ViViT: A Video Vision Transformer}, 
  year={2021},
  volume={},
  number={},
  pages={6816-6826},
  keywords={Training;Computer vision;Three-dimensional displays;Benchmark testing;Transformers;Spatiotemporal phenomena;Kinetic theory;Video analysis and understanding;Action and behavior recognition},
  doi={10.1109/ICCV48922.2021.00676},
  url             = {https://ieeexplore.ieee.org/abstract/document/9710415},
}

@inproceedings{Dehghani+2023,
title={Patch n{\textquoteright} Pack: NaViT, a Vision Transformer for any Aspect Ratio and Resolution},
author={Mostafa Dehghani and Basil Mustafa and Josip Djolonga and Jonathan Heek and Matthias Minderer and Mathilde Caron and Andreas Peter Steiner and Joan Puigcerver and Robert Geirhos and Ibrahim Alabdulmohsin and Avital Oliver and Piotr Padlewski and Alexey A. Gritsenko and Mario Lucic and Neil Houlsby},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
url={https://openreview.net/forum?id=VpGFHmI7e5}
}

@misc{Ma+2024,
      title={SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers}, 
      author={Nanye Ma and Mark Goldstein and Michael S. Albergo and Nicholas M. Boffi and Eric Vanden-Eijnden and Saining Xie},
      year={2024},
      eprint={2401.08740},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2401.08740},
}

@inproceedings{Albergo-Vanden-Eijnden2023,
title={{Building Normalizing Flows with Stochastic Interpolants}},
author={Michael Samuel Albergo and Eric Vanden-Eijnden},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=li7qeBbCR1t}
}

@misc{Albergo+2023,
      title={{Stochastic Interpolants: A Unifying Framework for Flows and Diffusions}}, 
      author={Michael S. Albergo and Nicholas M. Boffi and Eric Vanden-Eijnden},
      year={2023},
      eprint={2303.08797},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2303.08797},
}

@inproceedings{Lipman+2023,
title={Flow Matching for Generative Modeling},
author={Yaron Lipman and Ricky T. Q. Chen and Heli Ben-Hamu and Maximilian Nickel and Matthew Le},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=PqvMRDCJT9t}
}

@inproceedings{Liu+2023-Flow,
title={Flow Straight and Fast: Learning to Generate and Transfer Data with Rectified Flow},
author={Xingchao Liu and Chengyue Gong and qiang liu},
booktitle={The Eleventh International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=XVjTT1nw5z}
}

@book{伊藤-加藤2005,
    author         = {伊藤正己 and 加藤一郎},
    year           = {2005},
    month          = {3},
    title          = {現代法学入門},
    series         = {有斐閣双書},
    volume         = {},
    edition        = {4},
    url            = {https://www.yuhikaku.co.jp/books/detail/4641112568},
    publisher      = {有斐閣}
}

@book{渡辺洋三1993,
    author         = {渡辺洋三},
    year           = {1993},
    month          = {6},
    title          = {法の常識},
    series         = {有斐閣双書},
    volume         = {},
    edition        = {3},
    url            = {https://www.yuhikaku.co.jp/books/detail/4641111014},
    publisher      = {有斐閣}
}

@inbook{実質的違法論について2004,
    author         = {厚生労働省},
    chapter        = {実質的違法論について},
    editor         = {},
    pages          = {},
    publisher      = {},
    title          = {在宅及び養護学校における日常的な医療の医学的・法律学的整理に関する研究会（第１回）},
    year           = {2004},
    url            = {https://www.mhlw.go.jp/shingi/2004/05/s0531-11b4.html},
}

@article{Bezanson+2017,
    author          = {Jeff Bezanson and Alan Edelman and Stefan Karpinski and Viral B. Shah},
    year            = {2017},
    title           = {Julia: A Fresh Approach to Numerical Computing},
    journal         = {SIAM Review},
    volume          = {59},
    number          = {1},
    pages           = {65-98},
    url             = {https://epubs.siam.org/doi/10.1137/141000671}
}

@inproceedings{Simonyan-Zisserman2015,
    author          = {Simonyan, K., and Zisserman, A.},
    year            = {2015},
    title           = {Very deep convolutional networks for large-scale image recognition},
    booktitle       = {International Conference on Learning Representations},
    volume          = {3},
    pages           = {1-14},
    url             = {https://ora.ox.ac.uk/objects/uuid:60713f18-a6d1-4d97-8f45-b60ad8aebbce}
}


@article{Broderick+2023,
	abstract = {Probabilistic machine learning increasingly informs critical decisions in medicine, economics, politics, and beyond. To aid the development of trust in these decisions, we develop a taxonomy delineating where trust in an analysis can break down: (i) in the translation of real-world goals to goals on a particular set of training data, (ii) in the translation of abstract goals on the training data to a concrete mathematical problem, (iii) in the use of an algorithm to solve the stated mathematical problem, and (iv) in the use of a particular code implementation of the chosen algorithm. We detail how trust can fail at each step and illustrate our taxonomy with two case studies. Finally, we describe a wide variety of methods that can be used to increase trust at each step of our taxonomy. The use of our taxonomy highlights not only steps where existing research work on trust tends to concentrate and but also steps where building trust is particularly challenging. A taxonomy delineates where trust can break down in a probabilistic\&amp;nbsp;machine learning workflow that informs critical decisions.},
	author = {Tamara Broderick and Andrew Gelman and Rachael Meager and Anna L. Smith and Tian Zheng},
	doi = {10.1126/sciadv.abn3999},
	eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.abn3999},
	journal = {Science Advances},
	number = {7},
	pages = {eabn3999},
	title = {Toward a taxonomy of trust for probabilistic machine learning},
	url = {https://www.science.org/doi/abs/10.1126/sciadv.abn3999},
	volume = {9},
	year = {2023},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/sciadv.abn3999},
	bdsk-url-2 = {https://doi.org/10.1126/sciadv.abn3999}}


@inproceedings{Li+2018,
	author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Visualizing the Loss Landscape of Neural Nets},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a41b3bb3e6b050b6c9067c67f663b915-Paper.pdf}}


@InProceedings{Balduzzi+2017,
  title = 	 {The Shattered Gradients Problem: If resnets are the answer, then what is the question?},
  author =       {David Balduzzi and Marcus Frean and Lennox Leary and J. P. Lewis and Kurt Wan-Duo Ma and Brian McWilliams},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {342--350},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/balduzzi17b/balduzzi17b.pdf},
  url = 	 {https://proceedings.mlr.press/v70/balduzzi17b.html},
  abstract = 	 {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new “looks linear” (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.}
}

@article{Wolpert1996,
    author          = {David H. Wolpert},
    year            = {1996},
    title           = {The Lack of a Priori Distinctions between Learning Algorithms},
    journal         = {Neural Computation},
    volume          = {8},
    number          = {7},
    pages           = {1341-130},
    url             = {https://doi.org/10.1162/neco.1996.8.7.1341}
}

@misc{Bronstein+2021,
      title={Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges}, 
      author={Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
      year={2021},
      eprint={2104.13478},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2104.13478},
}


@inproceedings{Simard+1991,
	author = {Simard, Patrice and Victorri, Bernard and LeCun, Yann and Denker, John},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Moody and S. Hanson and R.P. Lippmann},
	publisher = {Morgan-Kaufmann},
	title = {Tangent Prop - A formalism for specifying selected invariances in an adaptive network},
	url = {https://proceedings.neurips.cc/paper_files/paper/1991/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf},
	volume = {4},
	year = {1991},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/1991/file/65658fde58ab3c2b6e5132a39fae7cb9-Paper.pdf}}

@inproceedings{Zhou-Chellappa1988,
    author          = {Y. Zhou and R. Chellappa},
    year            = {1988},
    title           = {Computation of Optic Flow Using a Neural Network},
    booktitle       = {IEEE 1988 International Conference on Neural Networks},
    volume          = {},
    pages           = {},
    url             = {https://ieeexplore.ieee.org/document/23914}
}

@INPROCEEDINGS{Deng+2009,
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Kai Li and Li Fei-Fei},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={ImageNet: A large-scale hierarchical image database}, 
  year={2009},
  volume={},
  number={},
  pages={248-255},
  keywords={Large-scale systems;Image databases;Explosions;Internet;Robustness;Information retrieval;Image retrieval;Multimedia databases;Ontologies;Spine},
  doi={10.1109/CVPR.2009.5206848}}

@misc{Dodge-Karam2017,
      title={A Study and Comparison of Human and Deep Learning Recognition Performance Under Visual Distortions}, 
      author={Samuel Dodge and Lina Karam},
      year={2017},
      eprint={1705.02498},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/1705.02498},
}

@INPROCEEDINGS{Long+2015,
author = {J. Long and E. Shelhamer and T. Darrell},
booktitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Fully convolutional networks for semantic segmentation},
year = {2015},
volume = {},
issn = {1063-6919},
pages = {3431-3440},
keywords = {},
doi = {10.1109/CVPR.2015.7298965},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2015.7298965},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}

@INPROCEEDINGS {Noh+2015,
author = {H. Noh and S. Hong and B. Han},
booktitle = {2015 IEEE International Conference on Computer Vision (ICCV)},
title = {Learning Deconvolution Network for Semantic Segmentation},
year = {2015},
volume = {},
issn = {2380-7504},
pages = {1520-1528},
abstract = {We propose a novel semantic segmentation algorithm by learning a deep deconvolution network. We learn the network on top of the convolutional layers adopted from VGG 16-layer net. The deconvolution network is composed of deconvolution and unpooling layers, which identify pixelwise class labels and predict segmentation masks. We apply the trained network to each proposal in an input image, and construct the final semantic segmentation map by combining the results from all proposals in a simple manner. The proposed algorithm mitigates the limitations of the existing methods based on fully convolutional networks by integrating deep deconvolution network and proposal-wise prediction, our segmentation method typically identifies detailed structures and handles objects in multiple scales naturally. Our network demonstrates outstanding performance in PASCAL VOC 2012 dataset, and we achieve the best accuracy (72.5%) among the methods trained without using Microsoft COCO dataset through ensemble with the fully convolutional network.},
keywords = {deconvolution;semantics;image segmentation;visualization;feature extraction;shape;image reconstruction},
doi = {10.1109/ICCV.2015.178},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2015.178},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {dec}
}

@ARTICLE{Badrinarayanan+2017,
  author={Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}, 
  year={2017},
  volume={39},
  number={12},
  pages={2481-2495},
  keywords={Decoding;Neural networks;Training;Computer architecture;Image segmentation;Semantics;Convolutional codes;Deep convolutional neural networks;semantic pixel-wise segmentation;indoor scenes;road scenes;encoder;decoder;pooling;upsampling},
  doi={10.1109/TPAMI.2016.2644615},
  url             = {https://ieeexplore.ieee.org/document/7803544},}

@book{Clark2018,
    author         = {Michael Clark},
    year           = {2018},
    title          = {Graphical \& Latent Variable Modeling},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://m-clark.github.io/sem/},
    publisher      = {}
}

@article{Joreskog70,
    author          = {Jöreskog, Karl Gustav},
    title           = {A general method for analysis of covariance structures},
    year            = {1970},
    journal         = {Biometrika},
    volume          = {57},
    number          = {2},
    pages           = {239-251},
    url             = {https://www.jstor.org/stable/2334833},
}


@article{Joreskog1969,
	abstract = {We describe a general procedure by which any number of parameters of the factor analytic model can be held fixed at any values and the remaining free parameters estimated by the maximum likelihood method. The generality of the approach makes it possible to deal with all kinds of solutions: orthogonal, oblique and various mixtures of these. By choosing the fixed parameters appropriately, factors can be defined to have desired properties and make subsequent rotation unnecessary. The goodness of fit of the maximum likelihood solution under the hypothesis represented by the fixed parameters is tested by a large samplex2 test based on the likelihood ratio technique. A by-product of the procedure is an estimate of the variance-covariance matrix of the estimated parameters. From this, approximate confidence intervals for the parameters can be obtained. Several examples illustrating the usefulness of the procedure are given.},
	author = {J{\"o}reskog, K.  G. },
	date = {1969/06/01},
	date-added = {2024-08-13 11:49:03 +0900},
	date-modified = {2024-08-13 11:49:03 +0900},
	doi = {10.1007/BF02289343},
	id = {J{\"o}reskog1969},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {2},
	pages = {183--202},
	title = {A general approach to confirmatory maximum likelihood factor analysis},
	url = {https://doi.org/10.1007/BF02289343},
	volume = {34},
	year = {1969},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289343}}


@article{Glivenko1933,
    author          = {Valery Ivanovich Glivenko},
    year            = {1933},
    title           = {Sulla determinazione empirica della leggi di probabilità},
    journal         = {Giornale dell'Instituo Italiano degli Attuari},
    volume          = {4},
    number          = {1},
    pages           = {92-99},
    url             = {}
}

@article{Cantelli1933,
    author          = {F. P. Cantelli},
    year            = {1933},
    title           = {Sulla determinazione empirica della leggi di probabilità},
    journal         = {Giornale dell'Instituo Italiano degli Attuari},
    volume          = {4},
    number          = {1},
    pages           = {421-424},
    url             = {}
}


@article{Shalev-Shwartz2010,
	author = {Shai Shalev-Shwartz and Ohad Shamir and Nathan Srebro and Karthik Sridharan},
	journal = {Journal of Machine Learning Research},
	number = {90},
	pages = {2635--2670},
	title = {Learnability, Stability and Uniform Convergence},
	url = {http://jmlr.org/papers/v11/shalev-shwartz10a.html},
	volume = {11},
	year = {2010},
	bdsk-url-1 = {http://jmlr.org/papers/v11/shalev-shwartz10a.html}}

@misc{Andreas2022,
      title={Language Models as Agent Models}, 
      author={Jacob Andreas},
      year={2022},
      eprint={2212.01681},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2212.01681},
}

@misc{Hashimoto2024,
    author       = {Tatsunori Hashimoto},
    howpublished = {Lecture at MLSS2024},
    title        = {Large Language Models},
    year         = {2024}
}

@misc{Kingma-Ba2017,
      title={Adam: A Method for Stochastic Optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1412.6980},
}


@article{Ackley+1985,
	abstract = {The computational power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections can allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in a very short time. One kind of computation for which massively parallel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the constraints in the domain being searched. We describe a general parallel search method, based on statistical mechanics, and we show how it leads to a general learning rule for modifying the connection strengths so as to incorporate knowledge about a task domain in an efficient way. We describe some simple examples in which the learning algorithm creates internal representations that are demonstrably the most efficient way of using the preexisting connectivity structure.},
	author = {David H. Ackley and Geoffrey E. Hinton and Terrence J. Sejnowski},
	doi = {https://doi.org/10.1016/S0364-0213(85)80012-4},
	issn = {0364-0213},
	journal = {Cognitive Science},
	number = {1},
	pages = {147-169},
	title = {A learning algorithm for boltzmann machines},
	url = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
	volume = {9},
	year = {1985},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0364021385800124},
	bdsk-url-2 = {https://doi.org/10.1016/S0364-0213(85)80012-4}}

@misc{Baykal+2023,
      title={EdVAE: Mitigating Codebook Collapse with Evidential Discrete Variational Autoencoders}, 
      author={Gulcin Baykal and Melih Kandemir and Gozde Unal},
      year={2023},
      eprint={2310.05718},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2310.05718},
}


@inproceedings{Sensoy+2018,
	author = {Sensoy, Murat and Kaplan, Lance and Kandemir, Melih},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Evidential Deep Learning to Quantify Classification Uncertainty},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/a981f2b708044d6fb4a71a1463242520-Paper.pdf}}

@misc{Amini+2020,
      title={Deep Evidential Regression}, 
      author={Alexander Amini and Wilko Schwarting and Ava Soleimany and Daniela Rus},
      year={2020},
      eprint={1910.02600},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1910.02600},
}

@misc{Hestness+2017,
      title={Deep Learning Scaling is Predictable, Empirically}, 
      author={Joel Hestness and Sharan Narang and Newsha Ardalani and Gregory Diamos and Heewoo Jun and Hassan Kianinejad and Md. Mostofa Ali Patwary and Yang Yang and Yanqi Zhou},
      year={2017},
      eprint={1712.00409},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1712.00409},
}

@misc{Dai-Le2015,
      title={Semi-supervised Sequence Learning}, 
      author={Andrew M. Dai and Quoc V. Le},
      year={2015},
      eprint={1511.01432},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1511.01432},
}

@misc{Hoffmann+2022,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url          = {https://arxiv.org/abs/2203.15556},
}

@misc{Lieber+2021,
    author       = {Opher Lieber and Or Sharir and Barak Lenz and Yoav Shoham},
    note         = {White Paper},
    title        = {Jurrassic-1: Technical Details and Evaluation},
    year         = {2021},
    url          = {https://www.ai21.com/blog/announcing-ai21-studio-and-jurassic-1},
    institution = {AI21},
}

@misc{Mohri-Hashimoto2024,
      title={Language Models with Conformal Factuality Guarantees}, 
      author={Christopher Mohri and Tatsunori Hashimoto},
      year={2024},
      eprint={2402.10978},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2402.10978},
}

@misc{Kuditipudi+2023,
      title={Robust Distortion-free Watermarks for Language Models}, 
      author={Rohith Kuditipudi and John Thickstun and Tatsunori Hashimoto and Percy Liang},
      year={2023},
      eprint={2307.15593},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/2402.10978},
}

@misc{Gu-Dao2024,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2024},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2312.00752}, 
}

@inproceedings{Smith+2023,
title={Simplified State Space Layers for Sequence Modeling},
author={Jimmy T.H. Smith and Andrew Warrington and Scott Linderman},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Ai8Hw3AXqks}
}

@inproceedings{Garg+2022,
title={What Can Transformers Learn In-Context? A Case Study of Simple Function Classes},
author={Shivam Garg and Dimitris Tsipras and Percy Liang and Gregory Valiant},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=flNZJ2eOet}
}

@inproceedings{vanOswald+2023,
title	= {Transformers learn in-context by gradient descent},author	= {Johannes von{\ }Oswald and Eyvind Niklasson and Ettore Randazzo and João Sacramento and Alexander Mordvintsev and Andrey Zhmoginov and Max Vladymyrov},year	= {2023},URL	= {https://research.google/pubs/transformers-learn-in-context-by-gradient-descent/},pages	= {35151--35174}}

@inproceedings{Akyurek+2023,
title={​​What learning algorithm is in-context learning? Investigations with linear models},
author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=0g0X4H8yN4I}
}

@inproceedings{Bai+2023,
title={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
author={Yu Bai and Fan Chen and Huan Wang and Caiming Xiong and Song Mei},
booktitle={Workshop on Efficient Systems for Foundation Models @ ICML2023},
year={2023},
url={https://openreview.net/forum?id=vlCG5HKEkI}
}

@inproceedings{Guo+2024,
title={How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations},
author={Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=ikwEDva1JZ}
}

@misc{Kim-Suzuki2024,
      title={Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape}, 
      author={Juno Kim and Taiji Suzuki},
      year={2024},
      eprint={2402.01258},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url          = {https://arxiv.org/abs/2402.01258},
}


@InProceedings{Daniely+2011,
  title = 	 {Multiclass Learnability and the ERM principle},
  author = 	 {Daniely, Amit and Sabato, Sivan and Ben-David, Shai and Shalev-Shwartz, Shai},
  booktitle = 	 {Proceedings of the 24th Annual Conference on Learning Theory},
  pages = 	 {207--232},
  year = 	 {2011},
  editor = 	 {Kakade, Sham M. and von Luxburg, Ulrike},
  volume = 	 {19},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Budapest, Hungary},
  month = 	 {09--11 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v19/daniely11a/daniely11a.pdf},
  url = 	 {https://proceedings.mlr.press/v19/daniely11a.html},
  abstract = 	 {Multiclass learning is an area of growing practical relevance, for which the currently available theory is still far from providing satisfactory understanding.  We study the learnability of multiclass prediction, and derive upper and lower bounds on the sample complexity of multiclass hypothesis classes in different learning models: batch/online, realizable/unrealizable,full information/bandit feedback.  Our analysis reveals a surprising phenomenon: In the multiclass setting, in sharp contrast to binary classification, not all Empirical Risk Minimization (ERM) algorithms are equally successful. We show that there exist hypotheses classes for which some ERM learners have lower sample complexity than others. Furthermore, there are classes that are learnable by some ERM learners, while other ERM learner will fail to learn them. We propose a principle for designing good ERM learners, and use this principle to prove tight bounds on the sample complexity of learning symmetric multiclass hypothesis classes (that is, classes that are invariant under any permutation of label names). We demonstrate the relevance of the theory by analyzing the sample complexity of two widely used hypothesis classes: generalized linear multiclass models and reduction trees. We also obtain some practically relevant conclusions.}
}

@misc{Clerico+2023,
      title={Generalisation under gradient descent via deterministic PAC-Bayes}, 
      author={Eugenio Clerico and Tyler Farghly and George Deligiannidis and Benjamin Guedj and Arnaud Doucet},
      year={2023},
      eprint={2209.02525},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url          = {https://arxiv.org/abs/2209.02525},
}


@article{岡島義憲2020,
	author = {岡島義憲},
	doi = {10.11517/jsaisigtwo.2020.AGI-015_08},
	journal = {人工知能学会第二種研究会資料},
	number = {AGI-015},
	pages = {08},
	title = {Spiking Neural Network 技術の現状と課題に関する考察},
	volume = {2020},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.11517/jsaisigtwo.2020.AGI-015_08}}


@misc{Wang+2023-BitNet,
	abstract = {The increasing size of large language models has posed challenges for deployment and raised concerns about environmental impact due to high energy consumption. In this work, we introduce BitNet, a scalable and stable 1-bit Transformer architecture designed for large language models. Specifically, we introduce BitLinear as a drop-in replacement of the nn.Linear layer in order to train 1-bit weights from scratch. Experimental results on language modeling show that BitNet achieves competitive performance while substantially reducing memory footprint and energy consumption, compared to state-of-the-art 8-bit quantization methods and FP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to full-precision Transformers, suggesting its potential for effective scaling to even larger language models while maintaining efficiency and performance benefits.},
	author = {Wang, Hongyu and Ma, Shuming and Dong, Li and Huang, Shaohan and Wang, Huaijie and Ma, Lingxiao and Yang, Fan and Wang, Ruiping and Wu, Yi and Wei, Furu},
	howpublished = {arXiv},
	month = {October},
	title = {BitNet: Scaling 1-bit Transformers for Large Language Models},
	url = {https://www.microsoft.com/en-us/research/publication/bitnet-scaling-1-bit-transformers-for-large-language-models/},
	year = {2023},
	bdsk-url-1 = {https://www.microsoft.com/en-us/research/publication/bitnet-scaling-1-bit-transformers-for-large-language-models/}}


@article{Maass1997,
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
	author = {Wolfgang Maass},
	doi = {https://doi.org/10.1016/S0893-6080(97)00011-7},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Spiking neuron, Integrate-and-fire neutron, Computational complexity, Sigmoidal neural nets, Lower bounds},
	number = {9},
	pages = {1659-1671},
	title = {Networks of spiking neurons: The third generation of neural network models},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608097000117},
	volume = {10},
	year = {1997},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608097000117},
	bdsk-url-2 = {https://doi.org/10.1016/S0893-6080(97)00011-7}}


@article{Tavanaei+2019,
	abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.},
	author = {Amirhossein Tavanaei and Masoud Ghodrati and Saeed Reza Kheradpisheh and Timoth{\'e}e Masquelier and Anthony Maida},
	doi = {https://doi.org/10.1016/j.neunet.2018.12.002},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Deep learning, Spiking neural network, Biological plausibility, Machine learning, Power-efficient architecture},
	pages = {47-63},
	title = {Deep learning in spiking neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608018303332},
	volume = {111},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608018303332},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2018.12.002}}

@inproceedings{Ujvary+2023,
title={Estimating optimal {PAC}-Bayes bounds with Hamiltonian Monte Carlo},
author={Szilvia Ujv{\'a}ry and Gergely Flamich and Vincent Fortuin and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
booktitle={NeurIPS 2023 Workshop on Mathematics of Modern Machine Learning},
year={2023},
url={https://openreview.net/forum?id=6ZUH4KRM1o}
}

@misc{Dziugaite-Roy2017,
      title={Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data}, 
      author={Gintare Karolina Dziugaite and Daniel M. Roy},
      year={2017},
      eprint={1703.11008},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1703.11008},
}

@misc{Jiang+2024,
      title={MegaScale: Scaling Large Language Model Training to More Than 10,000 GPUs}, 
      author={Ziheng Jiang and Haibin Lin and Yinmin Zhong and Qi Huang and Yangrui Chen and Zhi Zhang and Yanghua Peng and Xiang Li and Cong Xie and Shibiao Nong and Yulu Jia and Sun He and Hongmin Chen and Zhihao Bai and Qi Hou and Shipeng Yan and Ding Zhou and Yiyao Sheng and Zhuo Jiang and Haohan Xu and Haoran Wei and Zhang Zhang and Pengfei Nie and Leqi Zou and Sida Zhao and Liang Xiang and Zherui Liu and Zhe Li and Xiaoying Jia and Jianxi Ye and Xin Jin and Xin Liu},
      year={2024},
      eprint={2402.15627},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{gal2016bayesian,

      eprint={1506.02158},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{Gal-Ghahramani2016,
      title={Bayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference}, 
      author={Yarin Gal and Zoubin Ghahramani},
      year={2016},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=3QxqXoJEyfp7y9wltP11}
}


@InProceedings{Gal-Ghahramani2016-Dropout,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/gal16.html},
  abstract = 	 {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs – extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout’s uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout’s uncertainty in deep reinforcement learning.}
}


@article{Krzywinski-Altman2013,
	abstract = {Statistics does not tell us whether we are right. It tells us the chances of being wrong.},
	author = {Krzywinski, Martin and Altman, Naomi},
	date = {2013/09/01},
	date-added = {2024-03-06 15:09:25 +0900},
	date-modified = {2024-03-06 15:09:25 +0900},
	doi = {10.1038/nmeth.2613},
	id = {Krzywinski2013},
	isbn = {1548-7105},
	journal = {Nature Methods},
	number = {9},
	pages = {809--810},
	title = {Importance of being uncertain},
	url = {https://doi.org/10.1038/nmeth.2613},
	volume = {10},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1038/nmeth.2613}}

@book{Feynman1998,
    author         = {Richard P. Feynman},
    year           = {1998},
    title          = {The Meaning of It All: Thoughts of a Citizen Scientist},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Addison-Wesley}
}

@unpublished{Feynman1964,
    author = {Richard P. Feynman},
    year   = {1964},
    title  = {Seeking New Laws},
    url    = {https://www.feynmanlectures.caltech.edu/messenger.html}
}

@article{Herzog-Ostwald2013,
	author = {Herzog, Stefan and Ostwald, Dirk},
	date = {2013/02/01},
	date-added = {2024-03-06 16:10:57 +0900},
	date-modified = {2024-03-06 16:10:57 +0900},
	doi = {10.1038/494035b},
	id = {Herzog2013},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7435},
	pages = {35--35},
	title = {Sometimes Bayesian statistics are better},
	url = {https://doi.org/10.1038/494035b},
	volume = {494},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1038/494035b}}


@article{Trafimow-Marks2015,
	author = {David Trafimow and Michael Marks},
	doi = {10.1080/01973533.2015.1012991},
	eprint = {https://doi.org/10.1080/01973533.2015.1012991},
	journal = {Basic and Applied Social Psychology},
	number = {1},
	pages = {1-2},
	publisher = {Routledge},
	title = {Editorial},
	url = {https://doi.org/10.1080/01973533.2015.1012991},
	volume = {37},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1080/01973533.2015.1012991}}


@article{Nuzzo2014,
	abstract = {P values, the 'gold standard' of statistical validity, are not as reliable as many scientists assume.},
	author = {Nuzzo, Regina},
	date = {2014/02/01},
	date-added = {2024-03-06 16:40:33 +0900},
	date-modified = {2024-03-06 16:40:33 +0900},
	doi = {10.1038/506150a},
	id = {Nuzzo2014},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7487},
	pages = {150--152},
	title = {Scientific method: Statistical errors},
	url = {https://doi.org/10.1038/506150a},
	volume = {506},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1038/506150a}}

@article{平石-中村2022,
    author          = {平石界 and 中村大輝},
    year            = {2022},
    title           = {心理学における再現性危機の10年―危機は克服されたのか，克服され得るのか―},
    journal         = {科学哲学},
    volume          = {54},
    number          = {2},
    pages           = {27-50},
    url             = {https://www.jstage.jst.go.jp/article/jpssj/54/2/54_27/_article/-char/ja}
}

@article{Angrist-Pischke2010,
 ISSN = {08953309},
 URL = {http://www.jstor.org/stable/25703496},
 author = {Joshua D. Angrist and Jörn-Steffen Pischke},
 journal = {The Journal of Economic Perspectives},
 number = {2},
 pages = {3--30},
 publisher = {American Economic Association},
 title = {The Credibility Revolution in Empirical Economics: How Better Research Design is Taking the Con out of Econometrics},
 urldate = {2024-03-06},
 volume = {24},
 year = {2010}
}

@misc{Arjovsky+2020,
      title={Invariant Risk Minimization}, 
      author={Martin Arjovsky and Léon Bottou and Ishaan Gulrajani and David Lopez-Paz},
      year={2020},
      eprint={1907.02893},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
url          = {https://arxiv.org/abs/1907.02893},
}

@ARTICLE {Wang+2023-Domain,
author = {J. Wang and C. Lan and C. Liu and Y. Ouyang and T. Qin and W. Lu and Y. Chen and W. Zeng and P. S. Yu},
journal = {IEEE Transactions on Knowledge &amp; Data Engineering},
title = {Generalizing to Unseen Domains: A Survey on Domain Generalization},
year = {2023},
volume = {35},
number = {08},
issn = {1558-2191},
pages = {8052-8072},
abstract = {Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets, applications, and our open-sourced codebase for fair evaluation. Finally, we summarize existing literature and present some potential research topics for the future.},
keywords = {training;task analysis;data models;predictive models;multitasking;computational modeling;adaptation models},
doi = {10.1109/TKDE.2022.3178128},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {aug}
}

@book{Wang-Chen2023,
    author         = {Jindong Wang and Yiqiang Chen},
    year           = {2023},
    title          = {Introduction to Transfer Learning: Algorithms and Practice},
    series         = {Machine Learning: Foundations, Methodologies, and Applications},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-981-19-7584-4},
    publisher      = {Springer Singapore}
}


@article{Caruana1997,
	abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	author = {Caruana, Rich},
	date = {1997/07/01},
	date-added = {2024-03-10 14:19:51 +0900},
	date-modified = {2024-03-10 14:19:51 +0900},
	doi = {10.1023/A:1007379606734},
	id = {Caruana1997},
	isbn = {1573-0565},
	journal = {Machine Learning},
	number = {1},
	pages = {41--75},
	title = {Multitask Learning},
	url = {https://doi.org/10.1023/A:1007379606734},
	volume = {28},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1023/A:1007379606734}}

@ARTICLE{Zhuang+2021,
  author={Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  journal={Proceedings of the IEEE}, 
  title={A Comprehensive Survey on Transfer Learning}, 
  year={2021},
  volume={109},
  number={1},
  pages={43-76},
  keywords={Transfer learning;Semisupervised learning;Data models;Covariance matrices;Machine learning;Adaptation models;Domain adaptation;interpretation;machine learning;transfer learning},
  doi={10.1109/JPROC.2020.3004555}}


@article{Wang-Deng2018,
	abstract = {Deep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.},
	author = {Mei Wang and Weihong Deng},
	doi = {https://doi.org/10.1016/j.neucom.2018.05.083},
	issn = {0925-2312},
	journal = {Neurocomputing},
	keywords = {Deep domain adaptation, Deep networks, Transfer learning, Computer vision applications},
	pages = {135-153},
	title = {Deep visual domain adaptation: A survey},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231218306684},
	volume = {312},
	year = {2018},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0925231218306684},
	bdsk-url-2 = {https://doi.org/10.1016/j.neucom.2018.05.083}}

@ARTICLE {Hospedales+2022,
author = {T. Hospedales and A. Antoniou and P. Micaelli and A. Storkey},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Meta-Learning in Neural Networks: A Survey},
year = {2022},
volume = {44},
number = {09},
issn = {1939-3539},
pages = {5149-5169},
abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where tasks are solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many conventional challenges of deep learning, including data and computation bottlenecks, as well as generalization. This survey describes the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning such as few-shot learning and reinforcement learning. Finally, we discuss outstanding challenges and promising areas for future research.},
keywords = {task analysis;optimization;training;machine learning algorithms;predictive models;neural networks;deep learning},
doi = {10.1109/TPAMI.2021.3079209},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep},
url             = {https://www.computer.org/csdl/journal/tp/2022/09/09428530/1twaJR3AcJW},
}

@misc{Vanschoren2018,
      title={Meta-Learning: A Survey}, 
      author={Joaquin Vanschoren},
      year={2018},
      eprint={1810.03548},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1810.03548},
}

@inproceedings{Biesialska+2020,
    title = "Continual Lifelong Learning in Natural Language Processing: A Survey",
    author = "Biesialska, Magdalena  and
      Biesialska, Katarzyna  and
      Costa-juss{\`a}, Marta R.",
    editor = "Scott, Donia  and
      Bel, Nuria  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.574",
    doi = "10.18653/v1/2020.coling-main.574",
    pages = "6523--6541",
    abstract = "Continual learning (CL) aims to enable information systems to learn from a continuous data stream across time. However, it is difficult for existing deep learning architectures to learn a new task without largely forgetting previously acquired knowledge. Furthermore, CL is particularly challenging for language learning, as natural language is ambiguous: it is discrete, compositional, and its meaning is context-dependent. In this work, we look at the problem of CL through the lens of various NLP tasks. Our survey discusses major challenges in CL and current methods applied in neural network models. We also provide a critical review of the existing CL evaluation methods and datasets in NLP. Finally, we present our outlook on future research directions.",
}

@article{Toyota-Fukumizu2024,
title={Out-of-Distribution Optimality of Invariant Risk Minimization},
author={Shoji Toyota and Kenji Fukumizu},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=pWsfWDnJDa},
note={}
}


@InProceedings{Zhao+2019,
  title = 	 {On Learning Invariant Representations for Domain Adaptation},
  author =       {Zhao, Han and Combes, Remi Tachet Des and Zhang, Kun and Gordon, Geoffrey},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7523--7532},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/zhao19a/zhao19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/zhao19a.html},
  abstract = 	 {Due to the ability of deep neural nets to learn rich representations, recent advances in unsupervised domain adaptation have focused on learning domain-invariant features that achieve a small error on the source domain. The hope is that the learnt representation, together with the hypothesis learnt from the source domain, can generalize to the target domain. In this paper, we first construct a simple counterexample showing that, contrary to common belief, the above conditions are not sufficient to guarantee successful domain adaptation. In particular, the counterexample exhibits <em>conditional shift</em>: the class-conditional distributions of input features change between source and target domains. To give a sufficient condition for domain adaptation, we propose a natural and interpretable generalization upper bound that explicitly takes into account the aforementioned shift. Moreover, we shed new light on the problem by proving an information-theoretic lower bound on the joint error of <em>any</em> domain adaptation method that attempts to learn invariant representations. Our result characterizes a fundamental tradeoff between learning invariant representations and achieving small joint error on both domains when the marginal label distributions differ from source to target. Finally, we conduct experiments on real-world datasets that corroborate our theoretical findings. We believe these insights are helpful in guiding the future design of domain adaptation and representation learning algorithms.}
}


@article{Ganin+2016,
	author = {Yaroslav Ganin and Evgeniya Ustinova and Hana Ajakan and Pascal Germain and Hugo Larochelle and Fran{\c{c}}ois Laviolette and Mario March and Victor Lempitsky},
	journal = {Journal of Machine Learning Research},
	number = {59},
	pages = {1--35},
	title = {Domain-Adversarial Training of Neural Networks},
	url = {http://jmlr.org/papers/v17/15-239.html},
	volume = {17},
	year = {2016},
	bdsk-url-1 = {http://jmlr.org/papers/v17/15-239.html}}

@misc{Gatys+2015,
      title={A Neural Algorithm of Artistic Style}, 
      author={Leon A. Gatys and Alexander S. Ecker and Matthias Bethge},
      year={2015},
      eprint={1508.06576},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/1508.06576},
}

@INPROCEEDINGS{Gatys+2016,
  author={Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Image Style Transfer Using Convolutional Neural Networks}, 
  year={2016},
  volume={},
  number={},
  pages={2414-2423},
  keywords={Image reconstruction;Neural networks;Image representation;Semantics;Neuroscience;Feature extraction;Visualization},
  doi={10.1109/CVPR.2016.265}}

@article{McAllister+2017, title={Concrete Problems for Autonomous Vehicle Safety: Advantages of Bayesian Deep Learning}, url={https://www.repository.cam.ac.uk/handle/1810/266683}, DOI={10.17863/CAM.12760}, publisher={International Joint Conferences on Artificial Intelligence, Inc.}, author={McAllister, RT and Gal, Y and Kendall, A and Van Der Wilk, M and Shah, A and Cipolla, R and Weller, A}, year={2017}, keywords={technical: techniques, technical: models, social: challenges, social: human-machine interaction} }


@article{Bensal+2019,
	abstractnote = {&lt;p&gt;AI systems are being deployed to support human decision making in high-stakes domains such as healthcare and criminal justice. In many cases, the human and AI form a team, in which the human makes decisions after reviewing the AI's inferences. A successful partnership requires that the human develops insights into the performance of the AI system, including its failures. We study the influence of &lt;em&gt;updates&lt;/em&gt; to an AI system in this setting. While updates can increase the AI's predictive performance, they may also lead to behavioral changes that are at odds with the user's prior experiences and confidence in the AI's inferences. We show that updates that increase AI performance may actually hurt &lt;em&gt;team&lt;/em&gt; performance. We introduce the notion of the &lt;em&gt;compatibility&lt;/em&gt; of an AI update with prior user experience and present methods for studying the role of compatibility in human-AI teams. Empirical results on three high-stakes classification tasks show that current machine learning algorithms do not produce compatible updates. We propose a re-training objective to improve the compatibility of an update by penalizing new errors. The objective offers full leverage of the performance/compatibility tradeoff across different datasets, enabling more compatible yet accurate updates.&lt;/p&gt;},
	author = {Bansal, Gagan and Nushi, Besmira and Kamar, Ece and Weld, Daniel S. and Lasecki, Walter S. and Horvitz, Eric},
	doi = {10.1609/aaai.v33i01.33012429},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	month = {Jul.},
	number = {01},
	pages = {2429-2437},
	title = {Updates in Human-AI Teams: Understanding and Addressing the Performance/Compatibility Tradeoff},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/4087},
	volume = {33},
	year = {2019},
	bdsk-url-1 = {https://ojs.aaai.org/index.php/AAAI/article/view/4087},
	bdsk-url-2 = {https://doi.org/10.1609/aaai.v33i01.33012429}}

@inproceedings{Amershi+2019,
author = {Amershi, Saleema and Weld, Dan and Vorvoreanu, Mihaela and Fourney, Adam and Nushi, Besmira and Collisson, Penny and Suh, Jina and Iqbal, Shamsi and Bennett, Paul N. and Inkpen, Kori and Teevan, Jaime and Kikin-Gil, Ruth and Horvitz, Eric},
title = {Guidelines for Human-AI Interaction},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300233},
doi = {10.1145/3290605.3300233},
abstract = {Advances in artificial intelligence (AI) frame opportunities and challenges for user interface design. Principles for human-AI interaction have been discussed in the human-computer interaction community for over two decades, but more study and innovation are needed in light of advances in AI and the growing uses of AI technologies in human-facing applications. We propose 18 generally applicable design guidelines for human-AI interaction. These guidelines are validated through multiple rounds of evaluation including a user study with 49 design practitioners who tested the guidelines against 20 popular AI-infused products. The results verify the relevance of the guidelines over a spectrum of interaction scenarios and reveal gaps in our knowledge, highlighting opportunities for further research. Based on the evaluations, we believe the set of design guidelines can serve as a resource to practitioners working on the design of applications and features that harness AI technologies, and to researchers interested in the further development of human-AI interaction design principles.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {ai-infused systems, design guidelines, human-ai interaction},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@misc{Horita+2023,
      title={A Structure-Guided Diffusion Model for Large-Hole Image Completion}, 
      author={Daichi Horita and Jiaolong Yang and Dong Chen and Yuki Koyama and Kiyoharu Aizawa and Nicu Sebe},
      year={2023},
      eprint={2211.10437},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url          = {https://arxiv.org/abs/2211.10437},
}

@inproceedings{Higgins+2017,
title={beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
author={Irina Higgins and Loic Matthey and Arka Pal and Christopher Burgess and Xavier Glorot and Matthew Botvinick and Shakir Mohamed and Alexander Lerchner},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=Sy2fzU9gl}
}

@inproceedings{Burns-Fukai2023,
title={Simplicial Hopfield networks},
author={Thomas F Burns and Tomoki Fukai},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=_QLsH8gatwx}
}

@article{山本-尾崎2018,
    author          = {山本龍彦 and 尾崎愛美},
    year            = {2018},
    title           = {アルゴリズムと公正：State v. Loomis判決を素材に},
    journal         = {科学技術社会論研究},
    volume          = {16},
    number          = {},
    pages           = {96-107},
    url             = {https://www.jstage.jst.go.jp/article/jnlsts/16/0/16_96/_article/-char/ja}
}

@unpublished{深谷賢治1997,
    author = {深谷賢治},
    year   = {1997},
    title  = {「位相的場の理論」 集中講義ノート},
    url    = {https://www.math.kyoto-u.ac.jp/~fukaya/},
    note   = {静岡大学}
}


@inbook{Gromov2001,
	abstract = {Here are a few brief remarks on possible trends in mathematics for the coming decades.},
	address = {Berlin, Heidelberg},
	author = {Gromov, Mikhael},
	booktitle = {Mathematics Unlimited --- 2001 and Beyond},
	doi = {10.1007/978-3-642-56478-9_26},
	editor = {Engquist, Bj{\"o}rn and Schmid, Wilfried},
	isbn = {978-3-642-56478-9},
	pages = {525--527},
	publisher = {Springer Berlin Heidelberg},
	title = {Possible Trends in Mathematics in the Coming Decades},
	url = {https://doi.org/10.1007/978-3-642-56478-9_26},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1007/978-3-642-56478-9_26}}

@unpublished{福水健次2024,
    author = {福水健次},
    year   = {2024},
    title  = {Machine Learning with Group Theory},
    url    = {},
    note   = {MLSS2024 Lecture}
}


@article{Azulay-Weiss2019,
	author = {Aharon Azulay and Yair Weiss},
	journal = {Journal of Machine Learning Research},
	number = {184},
	pages = {1--25},
	title = {Why do deep convolutional networks generalize so poorly to small image transformations?},
	url = {http://jmlr.org/papers/v20/19-519.html},
	volume = {20},
	year = {2019},
	bdsk-url-1 = {http://jmlr.org/papers/v20/19-519.html}}


@InProceedings{Cohen-Welling2016,
  title = 	 {Group Equivariant Convolutional Networks},
  author = 	 {Cohen, Taco and Welling, Max},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2990--2999},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/cohenc16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/cohenc16.html},
  abstract = 	 {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.}
}

@article{Lafarge+2021,
	abstract = {Rotation-invariance is a desired property of machine-learning models for medical image analysis and in particular for computational pathology applications. We propose a framework to encode the geometric structure of the special Euclidean motion group SE(2) in convolutional networks to yield translation and rotation equivariance via the introduction of SE(2)-group convolution layers. This structure enables models to learn feature representations with a discretized orientation dimension that guarantees that their outputs are invariant under a discrete set of rotations. Conventional approaches for rotation invariance rely mostly on data augmentation, but this does not guarantee the robustness of the output when the input is rotated. At that, trained conventional CNNs may require test-time rotation augmentation to reach their full capability. This study is focused on histopathology image analysis applications for which it is desirable that the arbitrary global orientation information of the imaged tissues is not captured by the machine learning models. The proposed framework is evaluated on three different histopathology image analysis tasks (mitosis detection, nuclei segmentation and tumor detection). We present a comparative analysis for each problem and show that consistent increase of performances can be achieved when using the proposed framework.},
	author = {Maxime W. Lafarge and Erik J. Bekkers and Josien P.W. Pluim and Remco Duits and Mitko Veta},
	doi = {https://doi.org/10.1016/j.media.2020.101849},
	issn = {1361-8415},
	journal = {Medical Image Analysis},
	keywords = {Group convolutional neural network, Roto-translation equivariance, Computational pathology, Mitosis detection, Tumor detection, Nuclei segmentation},
	pages = {101849},
	title = {Roto-translation equivariant convolutional networks: Application to histopathology image analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S1361841520302139},
	volume = {68},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1361841520302139},
	bdsk-url-2 = {https://doi.org/10.1016/j.media.2020.101849}}

@inproceedings{Cohen-Welling2017,
title={Steerable {CNN}s},
author={Taco S. Cohen and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rJQKYt5ll}
}

@InProceedings{Weiler+2018,
author = {Weiler, Maurice and Hamprecht, Fred A. and Storath, Martin},
title = {Learning Steerable Filters for Rotation Equivariant CNNs},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
} 

@misc{Levine2018,
      title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review}, 
      author={Sergey Levine},
      year={2018},
      eprint={1805.00909},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url          = {https://arxiv.org/abs/1805.00909},
}


@InProceedings{Bellemare+2017,
  title = 	 {A Distributional Perspective on Reinforcement Learning},
  author =       {Marc G. Bellemare and Will Dabney and R{\'e}mi Munos},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {449--458},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/bellemare17a/bellemare17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/bellemare17a.html},
  abstract = 	 {In this paper we argue for the fundamental importance of the value distribution: the distribution of the random return received by a reinforcement learning agent. This is in contrast to the common approach to reinforcement learning which models the expectation of this return, or value. Although there is an established body of literature studying the value distribution, thus far it has always been used for a specific purpose such as implementing risk-aware behaviour. We begin with theoretical results in both the policy evaluation and control settings, exposing a significant distributional instability in the latter. We then use the distributional perspective to design a new algorithm which applies Bellman’s equation to the learning of approximate value distributions. We evaluate our algorithm using the suite of games from the Arcade Learning Environment. We obtain both state-of-the-art results and anecdotal evidence demonstrating the importance of the value distribution in approximate reinforcement learning. Finally, we combine theoretical and empirical evidence to highlight the ways in which the value distribution impacts learning in the approximate setting.}
}

@book{Bellemare+2023,
    author         = {Marc G. Bellemare and Will Dabney and Mark Rowland},
    year           = {2023},
    title          = {Distributional Reinforcement Learning},
    series         = {Adaptive Computation and Machine Learning series},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.7551/mitpress/14207.001.0001},
    publisher      = {The MIT Press}
}


@article{Silver+2016,
	abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks'to evaluate board positions and `policy networks'to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	date = {2016/01/01},
	date-added = {2024-03-12 12:07:42 +0900},
	date-modified = {2024-03-12 12:07:42 +0900},
	doi = {10.1038/nature16961},
	id = {Silver2016},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7587},
	pages = {484--489},
	title = {Mastering the game of Go with deep neural networks and tree search},
	url = {https://doi.org/10.1038/nature16961},
	volume = {529},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1038/nature16961}}


@article{Silver+2017,
	abstract = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100--0 against the previously published, champion-defeating AlphaGo.},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
	date = {2017/10/01},
	date-added = {2024-03-12 12:09:45 +0900},
	date-modified = {2024-03-12 12:09:45 +0900},
	doi = {10.1038/nature24270},
	id = {Silver2017},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7676},
	pages = {354--359},
	title = {Mastering the game of Go without human knowledge},
	url = {https://doi.org/10.1038/nature24270},
	volume = {550},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1038/nature24270}}


@InProceedings{Chizat-Bach2020,
  title = 	 {Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss},
  author =       {Chizat, L\'ena\"ic  and Bach, Francis},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {1305--1338},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/chizat20a.html},
  abstract = 	 { Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding this phenomenon, we analyze the training and generalization behavior of infinitely wide two-layer neural networks with homogeneous activations. We show that the limits of the gradient flow on exponentially tailed losses can be fully characterized as a max-margin classifier in a certain non-Hilbertian space of functions. In presence of hidden low-dimensional structures, the resulting margin is independent of the ambiant dimension, which leads to strong generalization bounds. In contrast, training only the output layer implicitly solves a kernel support vector machine, which a priori does not enjoy such an adaptivity. Our analysis of training is non-quantitative in terms of running time but we prove computational guarantees in simplified settings by showing equivalences with online mirror descent. Finally, numerical experiments suggest that our analysis describes well the practical behavior of two-layer neural networks with ReLU activation and confirm the statistical benefits of this implicit bias.}
}


@inproceedings{Moroshko+2020,
	author = {Moroshko, Edward and Woodworth, Blake E and Gunasekar, Suriya and Lee, Jason D and Srebro, Nati and Soudry, Daniel},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {22182--22193},
	publisher = {Curran Associates, Inc.},
	title = {Implicit Bias in Deep Linear Classification: Initialization Scale vs Training Accuracy},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/fc2022c89b61c76bbef978f1370660bf-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/fc2022c89b61c76bbef978f1370660bf-Paper.pdf}}

@ARTICLE{Song+2013,
  author={Song, Le and Fukumizu, Kenji and Gretton, Arthur},
  journal={IEEE Signal Processing Magazine}, 
  title={{Kernel Embeddings of Conditional Distributions: A Unified Kernel Framework for Nonparametric Inference in Graphical Models}}, 
  year={2013},
  volume={30},
  number={4},
  pages={98-111},
  keywords={Machine learning;Learning systems;Kernel;Computer vision;Computational biology;Parametric statistics},
  doi={10.1109/MSP.2013.2252713}}


@inproceedings{Li+2022,
	author = {Li, Zhu and Meunier, Dimitri and Mollenhauer, Mattes and Gretton, Arthur},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	pages = {4433--4445},
	publisher = {Curran Associates, Inc.},
	title = {Optimal Rates for Regularized Conditional Mean Embedding Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/1c71cd4032da425409d8ada8727bad42-Paper-Conference.pdf},
	volume = {35},
	year = {2022},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2022/file/1c71cd4032da425409d8ada8727bad42-Paper-Conference.pdf}}


@inproceedings{Oark-Muandet2020,
	author = {Park, Junhyung and Muandet, Krikamol},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {21247--21259},
	publisher = {Curran Associates, Inc.},
	title = {A Measure-Theoretic Approach to Kernel Conditional Mean Embeddings},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f340f1b1f65b6df5b5e3f94d95b11daf-Paper.pdf}}

@inproceedings{Song+2009,
author = {Song, Le and Huang, Jonathan and Smola, Alex and Fukumizu, Kenji},
title = {Hilbert space embeddings of conditional distributions with applications to dynamical systems},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553497},
doi = {10.1145/1553374.1553497},
abstract = {In this paper, we extend the Hilbert space embedding approach to handle conditional distributions. We derive a kernel estimate for the conditional embedding, and show its connection to ordinary embeddings. Conditional embeddings largely extend our ability to manipulate distributions in Hilbert spaces, and as an example, we derive a nonparametric method for modeling dynamical systems where the belief state of the system is maintained as a conditional embedding. Our method is very general in terms of both the domains and the types of distributions that it can handle, and we demonstrate the effectiveness of our method in various dynamical systems. We expect that conditional embeddings will have wider applications beyond modeling dynamical systems.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {961–968},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@ARTICLE{Wamg+2024,
  author={Wang, Liyuan and Zhang, Xingxing and Su, Hang and Zhu, Jun},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Comprehensive Survey of Continual Learning: Theory, Method and Application}, 
  year={2024},
  volume={},
  number={},
  pages={1-20},
  keywords={Task analysis;Training;Surveys;Testing;Complexity theory;Stability analysis;Visualization;Continual learning;incremental learning;lifelong learning;catastrophic forgetting},
  doi={10.1109/TPAMI.2024.3367329}}

@misc{Novello+2024,
      title={Out-of-Distribution Detection Should Use Conformal Prediction (and Vice-versa?)}, 
      author={Paul Novello and Joseba Dalmau and Léo Andeol},
      year={2024},
      eprint={2403.11532},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url          = {https://arxiv.org/abs/2403.11532},
}

@misc{Farquhar-Gal2019,
      title={A Unifying Bayesian View of Continual Learning}, 
      author={Sebastian Farquhar and Yarin Gal},
      year={2019},
      eprint={1902.06494},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url          = {https://arxiv.org/abs/1902.06494},
}

@article{Efron1986,
 ISSN = {00031305},
 URL = {http://www.jstor.org/stable/2683105},
 abstract = {Originally a talk delivered at a conference on Bayesian statistics, this article attempts to answer the following question: why is most scientific data analysis carried out in a non-Bayesian framework? The argument consists mainly of some practical examples of data analysis, in which the Bayesian approach is difficult but Fisherian/frequentist solutions are relatively easy. There is a brief discussion of objectivity in statistical analyses and of the difficulties of achieving objectivity within a Bayesian framework. The article ends with a list of practical advantages of Fisherian/frequentist methods, which so far seem to have outweighed the philosophical superiority of Bayesianism.},
 author = {B. Efron},
 journal = {The American Statistician},
 number = {1},
 pages = {1--5},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Why Isn't Everyone a Bayesian?},
 urldate = {2024-03-19},
 volume = {40},
 year = {1986}
}


@article{Rubin1984,
	author = {Donald B. Rubin},
	doi = {10.1214/aos/1176346785},
	journal = {The Annals of Statistics},
	keywords = {62-07, Calibration, Empirical Bayes, inference, model monitoring, operating characteristics, posterior predictive checks, Stopping rules},
	number = {4},
	pages = {1151 -- 1172},
	publisher = {Institute of Mathematical Statistics},
	title = {{Bayesianly Justifiable and Relevant Frequency Calculations for the Applied Statistician}},
	url = {https://doi.org/10.1214/aos/1176346785},
	volume = {12},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1176346785}}

@book{Frey1998,
    author         = {Brendan J. Frey},
    year           = {1998},
    title          = {Graphical Models for Machine Learning and Digital Communication},
    series         = {Adaptive Computation and Machine Learning Series},
    volume         = {},
    edition        = {},
    url            = {https://mitpress.mit.edu/9780262062022/graphical-models-for-machine-learning-and-digital-communication/},
    publisher      = {The MIT Press}
}

@inproceedings{Deisenroth-Rasmussen2011,
author = {Deisenroth, Marc Peter and Rasmussen, Carl Edward},
title = {PILCO: a model-based and data-efficient approach to policy search},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper, we introduce PILCO, a practical, data-efficient model-based policy search method. PILCO reduces model bias, one of the key problems of model-based reinforcement learning, in a principled way. By learning a probabilistic dynamics model and explicitly incorporating model uncertainty into long-term planning, PILCO can cope with very little data and facilitates learning from scratch in only a few trials. Policy evaluation is performed in closed form using state-of-the-art approximate inference. Furthermore, policy gradients are computed analytically for policy improvement. We report unprecedented learning efficiency on challenging and high-dimensional control tasks.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {465–472},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}


@article{Hagen1991,
	abstract = {Bayesian quadrature treats the problem of numerical integration as one of statistical inference. A prior Gaussian process distribution is assumed for the integrand, observations arise from evaluating the integrand at selected points, and a posterior distribution is derived for the integrand and the integral. Methods are developed for quadrature in Rp. A particular application is integrating the posterior density arising from some other Bayesian analysis. Simulation results are presented, to show that the resulting Bayes--Hermite quadrature rules may perform better than the conventional Gauss--Hermite rules for this application. A key result is derived for product designs, which makes Bayesian quadrature practically useful for integrating in several dimensions. Although the method does not at present provide a solution to the more difficult problem of quadrature in high dimensions, it does seem to offer real improvements over existing methods in relatively low dimensions.},
	author = {A. O'Hagan},
	doi = {https://doi.org/10.1016/0378-3758(91)90002-V},
	issn = {0378-3758},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Bayesian quadrature, numerical integration, Gaussian process, product rule, Gaussian quadrature},
	number = {3},
	pages = {245-260},
	title = {Bayes--Hermite quadrature},
	url = {https://www.sciencedirect.com/science/article/pii/037837589190002V},
	volume = {29},
	year = {1991},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/037837589190002V},
	bdsk-url-2 = {https://doi.org/10.1016/0378-3758(91)90002-V}}

@INPROCEEDINGS{Steinruecken+2015,
  author={Steinruecken, Christian and Ghahramani, Zoubin and MacKay, David},
  booktitle={2015 Data Compression Conference}, 
  title={Improving PPM with Dynamic Parameter Updates}, 
  year={2015},
  volume={},
  number={},
  pages={193-202},
  keywords={Context;Prediction algorithms;Mathematical model;Probabilistic logic;Heuristic algorithms;Predictive models;Probability distribution;PPM;blending;escape mechanism;gradients;dynamic updates;data compression},
  doi={10.1109/DCC.2015.77}}

@inproceedings{Lloyd+2014,
author = {Lloyd, James Robert and Duvenaud, David and Grosse, Roger and Tenenbaum, Joshua B. and Ghahramani, Zoubin},
title = {Automatic construction and natural-language description of nonparametric regression models},
year = {2014},
publisher = {AAAI Press},
abstract = {This paper presents the beginnings of an automatic statistician, focusing on regression problems. Our system explores an open-ended space of statistical models to discover a good explanation of a data set, and then produces a detailed report with figures and natural language text.Our approach treats unknown regression functions nonparametrically using Gaussian processes, which has two important consequences. First, Gaussian processes can model functions in terms of high-level properties (e.g. smoothness, trends, periodicity, changepoints). Taken together with the compositional structure of our language of models this allows us to automatically describe functions in simple terms. Second, the use of flexible nonparametric models and a rich language for composing them in an open-ended manner also results in state-of-the-art extrapolation performance evaluated over 13 real time series data sets from various domains.},
booktitle = {Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence},
pages = {1242–1250},
numpages = {9},
location = {Qu\'{e}bec City, Qu\'{e}bec, Canada},
series = {AAAI'14}
}


@article{Khan-Rue2023,
	author = {Mohammad Emtiyaz Khan and H{\aa}vard Rue},
	journal = {Journal of Machine Learning Research},
	number = {281},
	pages = {1--46},
	title = {The Bayesian Learning Rule},
	url = {http://jmlr.org/papers/v24/22-0291.html},
	volume = {24},
	year = {2023},
	bdsk-url-1 = {http://jmlr.org/papers/v24/22-0291.html}}


@inproceedings{Kim+2022,
	author = {Kim, Kyurae and Oh, Jisu and Gardner, Jacob and Dieng, Adji Bousso and Kim, Hongseok},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
	pages = {34802--34816},
	publisher = {Curran Associates, Inc.},
	title = {Markov Chain Score Ascent: A Unifying Framework of Variational Inference with Markovian Gradients},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e0fbc0f2e35e58aeffe5524a69ba90e5-Paper-Conference.pdf},
	volume = {35},
	year = {2022},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e0fbc0f2e35e58aeffe5524a69ba90e5-Paper-Conference.pdf}}


@inproceedings{Naesseth+2020,
	author = {Naesseth, Christian and Lindsten, Fredrik and Blei, David},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {15499--15510},
	publisher = {Curran Associates, Inc.},
	title = {Markovian Score Climbing: Variational Inference with KL(p\vert \vert q)},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b20706935de35bbe643733f856d9e5d6-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/b20706935de35bbe643733f856d9e5d6-Paper.pdf}}


@InProceedings{Ou-Song2020,
  title = 	 {Joint Stochastic Approximation and Its Application to Learning Discrete Latent Variable Models},
  author =       {Ou, Zhijian and Song, Yunfu},
  booktitle = 	 {Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)},
  pages = 	 {929--938},
  year = 	 {2020},
  editor = 	 {Peters, Jonas and Sontag, David},
  volume = 	 {124},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {03--06 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v124/ou20a/ou20a.pdf},
  url = 	 {https://proceedings.mlr.press/v124/ou20a.html},
  abstract = 	 {Although with progress in introducing auxiliary amortized inference models, learning discrete latent variable models is still challenging. In this paper, we show that the annoying difficulty of obtaining reliable stochastic gradients for the inference model and the drawback of indirectly optimizing the target log-likelihood can be gracefully addressed in a new method based on stochastic approximation (SA) theory of the Robbins-Monro type. Specifically, we propose to directly maximize the target log-likelihood and simultaneously minimize the inclusive divergence between the posterior and the inference model. The resulting learning algorithm is called joint SA (JSA). To the best of our knowledge, JSA represents the first method that couples an SA version of the EM (expectation-maximization) algorithm (SAEM) with an adaptive MCMC procedure. Experiments on several benchmark generative modeling and structured prediction tasks show that JSA consistently outperforms recent competitive algorithms, with faster convergence, better final likelihoods, and lower variance of gradient estimates.}
}
@misc{Papamarkou+2024,
      title={Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI}, 
      author={Theodore Papamarkou and Maria Skoularidou and Konstantina Palla and Laurence Aitchison and Julyan Arbel and David Dunson and Maurizio Filippone and Vincent Fortuin and Philipp Hennig and Jose Miguel Hernandez Lobato and Aliaksandr Hubin and Alexander Immer and Theofanis Karaletsos and Mohammad Emtiyaz Khan and Agustinus Kristiadi and Yingzhen Li and Stephan Mandt and Christopher Nemeth and Michael A. Osborne and Tim G. J. Rudner and David Rügamer and Yee Whye Teh and Max Welling and Andrew Gordon Wilson and Ruqi Zhang},
      year={2024},
      eprint={2402.00809},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url             = {https://arxiv.org/abs/2402.00809},
}

@misc{Chen+2023,
      title={Probabilistic Uncertainty Quantification of Prediction Models with Application to Visual Localization}, 
      author={Junan Chen and Josephine Monica and Wei-Lun Chao and Mark Campbell},
      year={2023},
      eprint={2305.20044},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url          = {https://arxiv.org/abs/2305.20044},
}

@techreport{内山貴之2015,
    author      = {内山貴之},
    institution = {東芝},
    title       = {半導体向けEUVリソグラフィの現状と展望},
    year        = {2015},
    url         = {https://www.heas.jp/lecture/files/uchiyama.pdf},
}

@inbook{斎藤毅2010,
    author         = {斎藤毅},
    chapter        = {グロタンディーク},
    editor         = {},
    pages          = {8-13},
    publisher      = {日本評論社},
    title          = {数学セミナー},
    year           = {2010},
    month          = {5},
    url            = {https://www.ms.u-tokyo.ac.jp/~t-saito/jd/gr.pdf},
}

@article{Kulik-Scheutzow2015,
    author          = {Alexei Kulik and Michael Scheutzow},
    year            = {2015},
    title           = {A Coupling Approach to Doob's Theorem},
    journal         = {Rendiconti Lincei Matematica e Applicazioni},
    volume          = {26},
    number          = {1},
    pages           = {83-92},
    url             = {https://ems.press/journals/rlm/articles/12955}
}

@inproceedings{Hairer-Mattingly2011,
	abstract = {The aim of this note is to present an elementary proof of a variation of Harris' ergodic theorem of Markov chains.},
	address = {Basel},
	author = {Hairer, Martin and Mattingly, Jonathan C.},
	booktitle = {Seminar on Stochastic Analysis, Random Fields and Applications VI},
	editor = {Dalang, Robert and Dozzi, Marco and Russo, Francesco},
	isbn = {978-3-0348-0021-1},
	pages = {109--117},
	publisher = {Springer Basel},
	title = {Yet Another Look at Harris' Ergodic Theorem for Markov Chains},
	year = {2011}}

@unpublished{Hairer2021-Convergence,
    author = {Martin Hairer},
    note   = {Lecture Note},
    title  = {Convergence of Markov Processes},
    year   = {2021},
    url    = {https://www.hairer.org/notes/Convergence.pdf}
}


@InProceedings{Gao+2020,
author = {Gao, Ruiqi and Nijkamp, Erik and Kingma, Diederik P. and Xu, Zhen and Dai, Andrew M. and Wu, Ying Nian},
title = {Flow Contrastive Estimation of Energy-Based Models},
booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020},
url             = {https://openaccess.thecvf.com/content_CVPR_2020/html/Gao_Flow_Contrastive_Estimation_of_Energy-Based_Models_CVPR_2020_paper},
}

@inbook{LeCun+2007,
    author         = {Yann LeCun and Sumit Chopra and Raia Hadsell and Marc' Aurelio Ranzato and Fu Jie Huang},
    chapter        = {Energy-Based Models},
    editor         = {Gökhan Baklr and Thomas Hofmann and Bernhard Schölkopf and Alexander J. Smola and Ben Taskar and S. V. N. Vishwanathan},
    pages          = {191-246},
    publisher      = {The MIT Press},
    title          = {Predicting Structured Data},
    year           = {2007},
    url            = {https://ieeexplore.ieee.org/document/6270201},
}

@inproceedings{Kim-Bengio2016,
    author          = {Taesup Kim and Yoshua Bengio},
    year            = {2016},
    title           = {Deep Directed Generative Models with Energy-Based Probability Estimation},
    booktitle       = {International Conference on Learning Representations},
    volume          = {},
    pages           = {},
    url             = {https://openreview.net/forum?id=BNYAGZZj5S7PwR1riXzA}
}

@book{Mezard-Montanari2009,
    author         = {Marc Mézard and Andrea Montanari},
    year           = {2009},
    title          = {Information, Physics, and Computation},
    series         = {Oxford Graduate Texts},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1093/acprof:oso/9780198570837.001.0001},
    publisher      = {Oxford University Press}
}

@unpublished{Chewi2024,
    author = {Sinho Chewi},
    year   = {2024},
    title  = {Log-Concave Sampling},
    url    = {https://chewisinho.github.io/}
}

@misc{Fearnhead+2017,
      title={Continious-time Importance Sampling: Monte Carlo Methods which Avoid Time-discretisation Error}, 
      author={Paul Fearnhead and Krzystof Latuszynski and Gareth O. Roberts and Giorgos Sermaidis},
      year={2017},
      eprint={1712.06201},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url          = {https://arxiv.org/abs/1712.06201},
}

@article{Dai+2019,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/45277571},
 author = {Hongsheng Dai and Murray Pollock and Gareth Roberts},
 journal = {Journal of Applied Probability},
 number = {1},
 pages = {174--191},
 publisher = {Applied Probability Trust},
 title = {{Monte Carlo Fusion}},
 urldate = {2024-04-01},
 volume = {56},
 year = {2019}
}


@article{Durmus+2016,
	author = {Alain Durmus and Gersende Fort and {\'E}ric Moulines},
	doi = {10.1214/15-AIHP699},
	journal = {Annales de l'Institut Henri Poincar{\'e}, Probabilit{\'e}s et Statistiques},
	keywords = {Markov chain Monte Carlo in infinite dimension, Markov chains, Subgeometric ergodicity, Wasserstein distance},
	number = {4},
	pages = {1799 -- 1822},
	publisher = {Institut Henri Poincar{\'e}},
	title = {{Subgeometric rates of convergence in Wasserstein distance for Markov chains}},
	url = {https://doi.org/10.1214/15-AIHP699},
	volume = {52},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1214/15-AIHP699}}


@article{Butkovsky-Veretennikov2013,
	abstract = {We prove that strong ergodicity of a Markov process is linked with a spectral radius of a certain ``associated'' semigroup operator, although, not a ``natural'' one. We also give sufficient conditions for weak ergodicity and provide explicit estimates of the convergence rate. To establish these results we construct a modification of the Vaserstein coupling. Some applications including mixing properties are also discussed.},
	author = {O.A. Butkovsky and A.Yu. Veretennikov},
	doi = {10.1016/j.spa.2013.04.016},
	issn = {0304-4149},
	journal = {Stochastic Processes and their Applications},
	keywords = {Markov process, Exponential convergence, Polynomial convergence, Vaserstein coupling, Mixing, Strong ergodicity},
	number = {9},
	pages = {3518-3541},
	title = {On asymptotics for Vaserstein coupling of Markov chains},
	url = {https://www.sciencedirect.com/science/article/pii/S0304414913001129},
	volume = {123},
	year = {2013},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0304414913001129},
	bdsk-url-2 = {https://doi.org/10.1016/j.spa.2013.04.016}}


@article{Diaconis-Stroock1991,
	author = {Persi Diaconis and Daniel Stroock},
	doi = {10.1214/aoap/1177005980},
	journal = {The Annals of Applied Probability},
	keywords = {Eigenvalues, Markov chains, Random walk},
	number = {1},
	pages = {36 -- 61},
	publisher = {Institute of Mathematical Statistics},
	title = {{Geometric Bounds for Eigenvalues of Markov Chains}},
	url = {https://doi.org/10.1214/aoap/1177005980},
	volume = {1},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1214/aoap/1177005980}}

@article{Doeblin1938,
    author          = {Wolfgang Doeblin},
    year            = {1938},
    title           = {Exposé de la Théorie des Chaînes Simple Constantes de Markov á un Nombre Fini d'États},
    journal         = {Revue Mathematique de l'Union Interbalkanique},
    volume          = {2},
    number          = {},
    pages           = {77-105},
    url             = {}
}

@article{Vaserstein1969,
    author          = {L. N. Vaserstein},
    year            = {1969},
    title           = {Markov processes on countable product spaces describing large systems of automata},
    journal         = {Problemy Peredachi Informatsii},
    volume          = {5},
    number          = {3},
    pages           = {64-72},
    url             = {https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=ppi&paperid=1811&option_lang=eng}
}

@article{Pitman1976,
	author = {Pitman, J.  W. },
	date = {1976/12/01},
	date-added = {2024-04-04 17:37:01 +0900},
	date-modified = {2024-04-04 17:37:01 +0900},
	doi = {10.1007/BF00532957},
	id = {Pitman1976},
	isbn = {1432-2064},
	journal = {Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
	number = {4},
	pages = {315--322},
	title = {On coupling of Markov chains},
	url = {https://doi.org/10.1007/BF00532957},
	volume = {35},
	year = {1976},
	bdsk-url-1 = {https://doi.org/10.1007/BF00532957}}

@book{Nummelin1984,
    author         = {Esa Nummelin},
    year           = {1984},
    title          = {{General Irreducible Markov Chains and Non-Negative Operators}},
    series         = {Cambridge Tracts in Mathematics},
    volume         = {83},
    edition        = {},
    url            = {https://doi.org/10.1017/CBO9780511526237},
    publisher      = {Cambridge University Press}
}

@article{Griffeath1975a,
	author = {Griffeath, David},
	date = {1975/06/01},
	date-added = {2024-04-04 17:40:09 +0900},
	date-modified = {2024-04-04 17:40:09 +0900},
	doi = {10.1007/BF00539434},
	id = {Griffeath1975},
	isbn = {1432-2064},
	journal = {Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
	number = {2},
	pages = {95--106},
	title = {A maximal coupling for Markov chains},
	url = {https://doi.org/10.1007/BF00539434},
	volume = {31},
	year = {1975},
	bdsk-url-1 = {https://doi.org/10.1007/BF00539434}}

@book{Lindvall1992,
    author         = {Torgny Lindvall},
    year           = {1992},
    title          = {Lectures on the Coupling method},
    series         = {Wiley Series in Probability and Statistics},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {John Wiley \& Sons}
}

@book{Breiman1969,
    author         = {Leo Breiman},
    year           = {1969},
    title          = {Probability and Stochastic Processes: with a View Toward Applications},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Houghton Miffin}
}

@book{Hoel+1986,
    author         = {Paul G. Hoel and Sidney C. Port and Charles J. Stone},
    year           = {1986},
    title          = {Introduction to Stochastic Processes},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Waveland Press}
}

@article{Rosenthal2002,
	author = {Jeffrey Rosenthal},
	doi = {10.1214/ECP.v7-1054},
	journal = {Electronic Communications in Probability},
	keywords = {convergence rate, drift condition, Markov chain, minorisation condition, mixing time, total variation distance},
	number = {none},
	pages = {123 -- 128},
	publisher = {Institute of Mathematical Statistics and Bernoulli Society},
	title = {{Quantitative Convergence Rates of Markov Chains: A Simple Account}},
	url = {https://doi.org/10.1214/ECP.v7-1054},
	volume = {7},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1214/ECP.v7-1054}}
@article{Tweedie1981,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3213172},
 abstract = {For regular Markov processes on a countable space, we provide criteria for the forms of ergodicity in the title in terms of the existence of solutions to inequalities involving the Q-matrix of the process. An application to birth-death processes is given.},
 author = {R. L. Tweedie},
 journal = {Journal of Applied Probability},
 number = {1},
 pages = {122--130},
 publisher = {Applied Probability Trust},
 title = {Criteria for Ergodicity, Exponential Ergodicity and Strong Ergodicity of Markov Processes},
 urldate = {2024-04-04},
 volume = {18},
 year = {1981}
}

@article{Tweedie1994,
	abstract = {This paper describes the role of continuous components in linking the topological and measuretheoretic (or regenerative) analysis of Markov chains and processes. Under Condition{\$}{\$}{$\backslash$}mathcal{\{}T{\}}{\$}{\$}below we show the following parallel results for both discrete and continuous time models:(i)when the model is open set irreducible it is ϕ-irreducible;(ii)under (i), the measure-theoretic classification of the model as Harris recurrent or positive Harris recurrent is equivalent to a topological classification in terms of not leaving compact sets or of tightness of transition kernels;(iii)under (i), the `global'classification of the model as transient, recurrent or positive recurrent is given by a ``local'classification of any individual reachable point;(iv)under (i), every compact set is a small set, so that through the Nummelin splitting there is pseudo-regeneration within compact sets, and compact sets are `test sets'for stability;(v)even without irreducibility, there is always a Doeblin decomposition into a countable disjoint collection of Harris sets and a transient set. We conclude with a guide to verifying Condition{\$}{\$}{$\backslash$}mathcal{\{}T{\}}{\$}{\$}and indicate that it holds under very mild constraints for a wide range of specific models: in particular a ϕ-irreducible Feller chain satisfies Condition{\$}{\$}{$\backslash$}mathcal{\{}T{\}}{\$}{\$}provided only that the support of ϕhas nonempty interior.},
	author = {Tweedie, R.  L. },
	date = {1994/02/01},
	date-added = {2024-06-05 11:15:28 +0900},
	date-modified = {2024-06-05 11:15:28 +0900},
	doi = {10.1007/BF00994264},
	id = {Tweedie1994},
	isbn = {1572-9036},
	journal = {Acta Applicandae Mathematica},
	number = {1},
	pages = {175--188},
	title = {Topological conditions enabling use of harris methods in discrete and continuous time},
	url = {https://doi.org/10.1007/BF00994264},
	volume = {34},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1007/BF00994264}}


@article{Douc+2004,
	author = {R. Douc and E. Moulines and Jeffrey S. Rosenthal},
	doi = {10.1214/105051604000000620},
	journal = {The Annals of Applied Probability},
	keywords = {convergence rate, coupling, f-total variation, Markov chain Monte Carlo, simulated annealing},
	number = {4},
	pages = {1643 -- 1665},
	publisher = {Institute of Mathematical Statistics},
	title = {{Quantitative bounds on convergence of time-inhomogeneous Markov chains}},
	url = {https://doi.org/10.1214/105051604000000620},
	volume = {14},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1214/105051604000000620}}

@article{Kalashinikov1973,
    author          = {V. V. Kalashinikov},
    year            = {1973},
    title           = {The Property of $\gamma$-reflexivity for Markov Sequences},
    journal         = {Soviet Mathematics Doklady},
    volume          = {14},
    number          = {},
    pages           = {1869-1873},
    url             = {}
}

@article{Lamperti1960,
	author = {John Lamperti},
	doi = {https://doi.org/10.1016/0022-247X(60)90005-6},
	issn = {0022-247X},
	journal = {Journal of Mathematical Analysis and Applications},
	number = {3},
	pages = {314-330},
	title = {Criteria for the recurrence or transience of stochastic process. I},
	url = {https://www.sciencedirect.com/science/article/pii/0022247X60900056},
	volume = {1},
	year = {1960},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0022247X60900056},
	bdsk-url-2 = {https://doi.org/10.1016/0022-247X(60)90005-6}}

@misc{Johnston+2024,
      title={Taming the Interacting Particle Langevin Algorithm -- the superlinear case}, 
      author={Tim Johnston and Nikolaos Makras and Sotirios Sabanis},
      year={2024},
      eprint={2403.19587},
      archivePrefix={arXiv},
      primaryClass={math.PR},
      url          = {https://arxiv.org/abs/2403.19587},
}

@article{Chau+2021,
	abstract = { We consider the problem of sampling from a target distribution, which is not necessarily log-concave, in the context of empirical risk minimization and stochastic optimization as presented in [M. Raginsky, A. Rakhlin, and M. Telgarsky, Proc. Mach. Learn. Res., 65 (2017), pp. 1674--1703]. Non-asymptotic results are established in the \$L^1\$-Wasserstein distance for the behavior of stochastic gradient Langevin dynamics algorithms. We allow gradient estimates based on dependent data streams. Our convergence estimates are sharper and uniform in the number of iterations, in contrast to those in previous studies. },
	author = {Chau, Ngoc Huy and Moulines, \'{E}ric and R\'{a}sonyi, Mikl\'{o}s and Sabanis, Sotirios and Zhang, Ying},
	doi = {10.1137/20M1355392},
	eprint = {https://doi.org/10.1137/20M1355392},
	journal = {SIAM Journal on Mathematics of Data Science},
	number = {3},
	pages = {959-986},
	title = {On Stochastic Gradient Langevin Dynamics with Dependent Data Streams: The Fully Nonconvex Case},
	url = {https://doi.org/10.1137/20M1355392},
	volume = {3},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1137/20M1355392}}

@book{Rapaport2004,
    author         = {Dennis C Rapaport},
    year           = {2004},
    title          = {The Art of Molecular Dynamics Simulation},
    series         = {},
    volume         = {},
    edition        = {2},
    url            = {https://www.cambridge.org/core/books/art-of-molecular-dynamics-simulation/57D40C5ECE9B7EA17C0E77E7754F5874},
    publisher      = {Cambridge University Press},
    doi            = {10.1017/CBO9780511816581},
}
@article{Alder-Wainwright1957,
    author = {Alder, B. J. and Wainwright, T. E.},
    title = "{Phase Transition for a Hard Sphere System}",
    journal = {The Journal of Chemical Physics},
    volume = {27},
    number = {5},
    pages = {1208-1209},
    year = {1957},
    month = {11},
    issn = {0021-9606},
    doi = {10.1063/1.1743957},
    url = {https://doi.org/10.1063/1.1743957},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/27/5/1208/18812897/1208\_1\_online.pdf},
}

@inproceedings{Alder-Wainwright1958,
    author = {Alder, B. J. and Wainwright, T. E.},
    year            = {1958},
    title           = {Molecular Dynamics by Electronic Computers},
    booktitle       = {Proceedings of the International Symposium on Transport Processes in Statistical Mechanics. Held in Brussels, August 27-31, 1956},
    volume          = {},
    pages           = {97},
    url             = {},
    editor          = {I. Prigogine},
}

@article{Alder-Wainwright1959,
    author = {Alder, B. J. and Wainwright, T. E.},
    title = "{Studies in Molecular Dynamics. I. General Method}",
    journal = {The Journal of Chemical Physics},
    volume = {31},
    number = {2},
    pages = {459-466},
    year = {1959},
    month = {08},
    abstract = "{A method is outlined by which it is possible to calculate exactly the behavior of several hundred interacting classical particles. The study of this many‐body problem is carried out by an electronic computer which solves numerically the simultaneous equations of motion. The limitations of this numerical scheme are enumerated and the important steps in making the program efficient on the computers are indicated. The applicability of this method to the solution of many problems in both equilibrium and nonequilibrium statistical mechanics is discussed.}",
    issn = {0021-9606},
    doi = {10.1063/1.1730376},
    url = {https://doi.org/10.1063/1.1730376},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/31/2/459/18817177/459\_1\_online.pdf},
}

@book{Griebel+2007,
    author         = {Michael Griebel and Gerhard Zumbusch and Stephan Knapek},
    year           = {2007},
    title          = {Numerical Simulation in Molecular Dynamics: Numerics, Algorithms, Parallelization, Applications},
    series         = {Texts in Computational Science and Engineering},
    volume         = {5},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-540-68095-6},
    publisher      = {Springer Belin, Heidelberg}
}

@book{Liu2004,
    author         = {Jun S. Liu},
    year           = {2004},
    title          = {Monte Carlo Strategies in Scientific Computing},
    series         = {Springer Series in Statistics},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Springer New York},
    doi            = {10.1007/978-0-387-76371-2},
}

@article{Hukushima-Nemoto1996,
author = {Hukushima ,Koji and Nemoto ,Koji},
title = {Exchange Monte Carlo Method and  Application to Spin Glass Simulations},
journal = {Journal of the Physical Society of Japan},
volume = {65},
number = {6},
pages = {1604-1608},
year = {1996},
doi = {10.1143/JPSJ.65.1604},
URL = {https://doi.org/10.1143/JPSJ.65.1604},
}

@article{Duane+1987,
title = {Hybrid Monte Carlo},
journal = {Physics Letters B},
volume = {195},
number = {2},
pages = {216-222},
year = {1987},
issn = {0370-2693},
doi = {10.1016/0370-2693(87)91197-X},
url = {https://www.sciencedirect.com/science/article/pii/037026938791197X},
author = {Simon Duane and A.D. Kennedy and Brian J. Pendleton and Duncan Roweth},
abstract = {We present a new method for the numerical simulation of lattice field theory. A hybrid (molecular dynamics/Langevin) algorithm is used to guide a Monte Carlo simulation. There are no discretization errors even for large step sizes. The method is especially efficient for systems such as quantum chromodynamics which contain fermionic degrees of freedom. Detailed results are presented for four-dimensional compact quantum electrodynamics including the dynamical effects of electrons.}
}
@article{用語解説2022,
  title={用語解説},
  author={栗﨑 and 田中},
  journal={生物物理},
  volume={62},
  number={4},
  pages={250-250},
  year={2022},
  doi={10.2142/biophys.62.250}
}
@article{Torrie-Valleau1977,
title = {Nonphysical sampling distributions in Monte Carlo free-energy estimation: Umbrella sampling},
journal = {Journal of Computational Physics},
volume = {23},
number = {2},
pages = {187-199},
year = {1977},
issn = {0021-9991},
doi = {10.1016/0021-9991(77)90121-8},
url = {https://www.sciencedirect.com/science/article/pii/0021999177901218},
author = {G.M. Torrie and J.P. Valleau},
abstract = {The free energy difference between a model system and some reference system can easily be written as an ensemble average, but the conventional Monte Carlo methods of obtaining such averages are inadequate for the free-energy case. That is because the Boltzmann-weighted sampling distribution ordinarily used is extremely inefficient for the purpose. This paper describes the use of arbitrary sampling distributions chosen to facilitate such estimates. The methods have been tested successfully on the Lennard-Jones system over a wide range of temperature and density, including the gas-liquid coexistence region, and are found to be extremely powerful and economical.}
}


@article{Iba2001b,
	abstract = { "Extended Ensemble Monte Carlo" is a generic term that indicates a set of algorithms, which are now popular in a variety of fields in physics and statistical information processing. Exchange Monte Carlo (Metropolis-Coupled Chain, Parallel Tempering), Simulated Tempering (Expanded Ensemble Monte Carlo) and Multicanonical Monte Carlo (Adaptive Umbrella Sampling) are typical members of this family. Here, we give a cross-disciplinary survey of these algorithms with special emphasis on the great flexibility of the underlying idea. In Sec. 2, we discuss the background of Extended Ensemble Monte Carlo. In Secs. 3, 4 and 5, three types of the algorithms, i.e., Exchange Monte Carlo, Simulated Tempering, Multicanonical Monte Carlo, are introduced. In Sec. 6, we give an introduction to Replica Monte Carlo algorithm by Swendsen and Wang. Strategies for the construction of special-purpose extended ensembles are discussed in Sec. 7. We stress that an extension is not necessary restricted to the space of energy or temperature. Even unphysical (unrealizable) configurations can be included in the ensemble, if the resultant fast mixing of the Markov chain offsets the increasing cost of the sampling procedure. Multivariate (multicomponent) extensions are also useful in many examples. In Sec. 8, we give a survey on extended ensembles with a state space whose dimensionality is dynamically varying. In the appendix, we discuss advantages and disadvantages of three types of extended ensemble algorithms. },
	author = {Iba, Yukito},
	doi = {10.1142/S0129183101001912},
	eprint = {https://doi.org/10.1142/S0129183101001912},
	journal = {International Journal of Modern Physics C},
	number = {05},
	pages = {623-656},
	title = {Extended Ensemble Monte Carlo},
	url = {https://doi.org/10.1142/S0129183101001912},
	volume = {12},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1142/S0129183101001912}}

@article{Iba1999,
doi = {10.1088/0305-4470/32/21/302},
url = {https://dx.doi.org/10.1088/0305-4470/32/21/302},
year = {1999},
month = {may},
publisher = {},
volume = {32},
number = {21},
pages = {3875},
author = {Yukito Iba},
title = {{The Nishimori line and Bayesian statistics}},
journal = {Journal of Physics A: Mathematical and General},
abstract = {The `Nishimori line' is a line or hypersurface in the parameter space of systems with quenched disorder, where simple expressions of the averages of physical quantities over the quenched random variables are obtained. It has been playing an important role in the theoretical studies of the random frustrated systems since its discovery in around 1980. In this paper, an interpretation of the Nishimori line from the viewpoint of statistical information processing is developed. Our main aim is the reconstruction of the whole theory of the Nishimori line from the viewpoint of Bayesian statistics, or, almost equivalently, from the viewpoint of the theory of error-correcting codes. As a byproduct of the interpretation, counterparts of the Nishimori line in models without gauge invariance are given. We also discussed the issues on the `finite-temperature decoding' of error-correcting codes and clarify the role of gauge invariance in this topic.}
}

@article{Gilks+1994,
 ISSN = {00390526, 14679884},
 URL = {http://www.jstor.org/stable/2348942},
 abstract = {Markov chain Monte Carlo (MCMC) techniques, such as the Gibbs sampler, are increasingly being used for Bayesian inference. We propose a new MCMC method: adaptive direction sampling (ADS) which, unlike the Gibbs sampler, involves sampling in directions which adapt to the target density. We present non-technically the essence of ADS, but with sufficient detail to allow the practitioner to apply the method. We demonstrate irreducibility of the snooker algorithm, a special case of ADS. We compare the performance of special cases of ADS, including the snooker algorithm and the Gibbs sampler, in a simple test example.},
 author = {W. R. Gilks and G. O. Roberts and E. I. George},
 journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
 number = {1},
 pages = {179--189},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Adaptive Direction Sampling},
 urldate = {2024-04-07},
 volume = {43},
 year = {1994}
}
@article{Liang-Wong2000,
 ISSN = {10170405, 19968507},
 URL = {http://www.jstor.org/stable/24306722},
 abstract = {Motivated by the success of genetic algorithms and simulated annealing in hard optimization problems, the authors propose a new Markov chain Monte Carlo (MCMC) algorithm called an evolutionary Monte Carlo algorithm. This algorithm has incorporated several attractive features of genetic algorithms and simulated annealing into the framework of MCMC. It works by simulating a population of Markov chains in parallel, where a different temperature is attached to each chain. The population is updated by mutation (Metropolis update), crossover (partial state swapping) and exchange operators (full state swapping). The algorithm is illustrated through examples of Cp-based model selection and change-point identification. The numerical results and the extensive comparisons show that evolutionary Monte Carlo is a promising approach for simulation and optimization.},
 author = {Faming Liang and Wing Hung Wong},
 journal = {Statistica Sinica},
 number = {2},
 pages = {317--342},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {EVOLUTIONARY MONTE CARLO: APPLICATIONS TO C
          p
          MODEL SAMPLING AND CHANGE POINT PROBLEM},
 urldate = {2024-04-07},
 volume = {10},
 year = {2000}
}

@article{Liang-Wong2001,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2670304},
 abstract = {We propose an evolutionary Monte Carlo algorithm to sample from a target distribution with real-valued parameters. The attractive features of the algorithm include the ability to learn from the samples obtained in previous steps and the ability to improve the mixing of a system by sampling along a temperature ladder. The effectiveness of the algorithm is examined through three multimodal examples and Bayesian neural networks. The numerical results confirm that the real-coded evolutionary algorithm is a promising general approach for simulation and optimization.},
 author = {Faming Liang and Wing Hung Wong},
 journal = {Journal of the American Statistical Association},
 number = {454},
 pages = {653--666},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Real-Parameter Evolutionary Monte Carlo with Applications to Bayesian Mixture Models},
 urldate = {2024-04-07},
 volume = {96},
 year = {2001}
}

@article{Berg-Neuhaus1991,
	abstract = {Monte Carlo simulations are discussed for systems of volume V = Ld which undergo a first order phase transition in the finite volume limit. Conventional canonical, local Monte Carlo algorithms suffer from exponentially fast slowing down ≈V2 exp (cLd−1). Here we present a class of multicanonical Monte Carlo algorithms which can reduce the slowing down to a quadratic power law ≈V2.},
	author = {Bernd A. Berg and Thomas Neuhaus},
	doi = {10.1016/0370-2693(91)91256-U},
	issn = {0370-2693},
	journal = {Physics Letters B},
	number = {2},
	pages = {249-253},
	title = {Multicanonical algorithms for first order phase transitions},
	url = {https://www.sciencedirect.com/science/article/pii/037026939191256U},
	volume = {267},
	year = {1991},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/037026939191256U},
	bdsk-url-2 = {https://doi.org/10.1016/0370-2693(91)91256-U}}
@article{Fichthorn-Weinberg1991,
    author = {Fichthorn, Kristen A. and Weinberg, W. H.},
    title = "{Theoretical foundations of dynamical Monte Carlo simulations}",
    journal = {The Journal of Chemical Physics},
    volume = {95},
    number = {2},
    pages = {1090-1096},
    year = {1991},
    month = {07},
    abstract = "{Monte Carlo methods are utilized as computational tools in many areas of chemical physics. In this paper, we present the theoretical basis for a dynamical Monte Carlo method in terms of the theory of Poisson processes. We show that if: (1) a ‘‘dynamical hierarchy’’ of transition probabilities is created which also satisfy the detailed‐balance criterion; (2) time increments upon successful events are calculated appropriately; and (3) the effective independence of various events comprising the system can be achieved, then Monte Carlo methods may be utilized to simulate the Poisson process and both static and dynamic properties of model Hamiltonian systems may be obtained and interpreted consistently.}",
    issn = {0021-9606},
    doi = {10.1063/1.461138},
    url = {https://doi.org/10.1063/1.461138},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/95/2/1090/18992969/1090\_1\_online.pdf},
}

@article{Bernard+2009,
  title = {Event-chain Monte Carlo algorithms for hard-sphere systems},
  author = {Bernard, Etienne P. and Krauth, Werner and Wilson, David B.},
  journal = {Phys. Rev. E},
  volume = {80},
  issue = {5},
  pages = {056704},
  numpages = {5},
  year = {2009},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.80.056704},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.80.056704}
}
@article{Bernard-Krauth2011,
  title = {Two-Step Melting in Two Dimensions: First-Order Liquid-Hexatic Transition},
  author = {Bernard, Etienne P. and Krauth, Werner},
  journal = {Phys. Rev. Lett.},
  volume = {107},
  issue = {15},
  pages = {155704},
  numpages = {4},
  year = {2011},
  month = {Oct},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.107.155704},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.107.155704}
}

@book{西森秀稔2003,
    author         = {西森秀稔},
    year           = {2003},
    title          = {スピングラスと連想記憶},
    series         = {岩波講座物理の世界},
    volume         = {},
    edition        = {},
    url            = {https://www.iwanami.co.jp/book/b476276.html},
    publisher      = {岩波書店}
}

@book{Minlos2000,
    author         = {R. A. Minlos},
    year           = {2000},
    title          = {Introduction to Mathematical Statistical Physics},
    series         = {University Lecture Series},
    volume         = {19},
    edition        = {},
    url            = {},
    doi            = {10.1090/ulect/019},
    publisher      = {American Mathematical Society}
}
@inbook{Altieri-Baity-Jesi2024,
   title={An introduction to the theory of spin glasses},
   ISBN={9780323914086},
   url={http://dx.doi.org/10.1016/B978-0-323-90800-9.00249-3},
   DOI={10.1016/b978-0-323-90800-9.00249-3},
   booktitle={Encyclopedia of Condensed Matter Physics},
   publisher={Elsevier},
   author={Altieri, Ada and Baity-Jesi, Marco},
   year={2024},
   pages={361–370} }
@article{Edwards-Anderson1975,
doi = {10.1088/0305-4608/5/5/017},
url = {https://dx.doi.org/10.1088/0305-4608/5/5/017},
year = {1975},
month = {may},
publisher = {},
volume = {5},
number = {5},
pages = {965},
author = {S F Edwards and  P W Anderson},
title = {Theory of spin glasses},
journal = {Journal of Physics F: Metal Physics},
abstract = {A new theory of the class of dilute magnetic alloys, called the spin glasses, is proposed which offers a simple explanation of the cusp found experimentally in the susceptibility. The argument is that because the interaction between the spins dissolved in the matrix oscillates in sign according to distance, there will be no mean ferro- or antiferromagnetism, but there will be a ground state with the spins aligned in definite directions, even if these directions appear to be at random. At the critical temperature the existence of these preferred directions affects the orientation of the spins, leading to a cusp in the susceptibility. This cusp is smoothed by an external field. Although the behaviour at low t needs a quantum mechanical treatment, it is interesting to complete the classical calculations down to t=0. Classically the susceptibility tends to a constant value at t=0, and the specific heat to a constant value.}
}
@misc{Chatterjee2023,
      title={Spin glass phase at zero temperature in the Edwards-Anderson model}, 
      author={Sourav Chatterjee},
      year={2023},
      eprint={2301.04112},
      archivePrefix={arXiv},
      primaryClass={math-ph},
      url            = {https://arxiv.org/abs/2301.04112},
}
@article{Sherrington-Kirkpatrick1975,
  title = {Solvable Model of a Spin-Glass},
  author = {Sherrington, David and Kirkpatrick, Scott},
  journal = {Phys. Rev. Lett.},
  volume = {35},
  issue = {26},
  pages = {1792--1796},
  numpages = {0},
  year = {1975},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.35.1792},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.35.1792}
}

@article{田中利幸2007,
    author          = {田中利幸},
    year            = {2007},
    title           = {レプリカ法における解析接続について(情報物理学の数学的構造)},
    journal         = {数理解析研究所講究録},
    volume          = {1532},
    number          = {},
    pages           = {118-129},
    url             = {http://hdl.handle.net/2433/58952}
}
@article{Cannella-Mydosh1972,
  title = {Magnetic Ordering in Gold-Iron Alloys},
  author = {Cannella, V. and Mydosh, J. A.},
  journal = {Phys. Rev. B},
  volume = {6},
  issue = {11},
  pages = {4220--4237},
  numpages = {0},
  year = {1972},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevB.6.4220},
  url = {https://link.aps.org/doi/10.1103/PhysRevB.6.4220}
}

@article{Panchenko2012,
   title={The Sherrington-Kirkpatrick Model: An Overview},
   volume={149},
   ISSN={1572-9613},
   url={http://dx.doi.org/10.1007/s10955-012-0586-7},
   DOI={10.1007/s10955-012-0586-7},
   number={2},
   journal={Journal of Statistical Physics},
   publisher={Springer Science and Business Media LLC},
   author={Panchenko, Dmitry},
   year={2012},
   month=sep, pages={362–383} }

@article{Parisi1980,
doi = {10.1088/0305-4470/13/4/009},
url = {https://dx.doi.org/10.1088/0305-4470/13/4/009},
year = {1980},
month = {apr},
publisher = {},
volume = {13},
number = {4},
pages = {L115},
author = {G Parisi},
title = {A sequence of approximated solutions to the S-K model for spin glasses},
journal = {Journal of Physics A: Mathematical and General},
abstract = {In the framework of the new version of the replica theory, a sequence of approximated solutions is computed for the Sherrington-Kirkpatrick model (see Phys. Rev. Lett., vol.35, p.1972, 1975) of spin glasses.}
}


@article{Parisi1981,
	abstract = {If the equilibrium properties of a statistical system are obtained by solving numerically the associated Langevin equation describing the approach to equilibrium, the connected correlation functions can be computed directly with small effort and high precision.},
	author = {G. Parisi},
	doi = {https://doi.org/10.1016/0550-3213(81)90056-0},
	issn = {0550-3213},
	journal = {Nuclear Physics B},
	number = {3},
	pages = {378-384},
	title = {Correlation functions and computer simulations},
	url = {https://www.sciencedirect.com/science/article/pii/0550321381900560},
	volume = {180},
	year = {1981},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0550321381900560},
	bdsk-url-2 = {https://doi.org/10.1016/0550-3213(81)90056-0}}


@article{都福仁1977,
    author          = {都福仁},
    year            = {1977},
    title           = {スピングラス},
    journal         = {日本物理学会誌},
    volume          = {32},
    number          = {6},
    pages           = {463-473},
    url             = {https://doi.org/10.11316/butsuri1946.32.463}
}

@book{久保亮五2003,
    author         = {久保亮五},
    year           = {2003},
    title          = {新装版統計力学},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.kyoritsu-pub.co.jp/book/b10011230.html},
    publisher      = {共立出版}
}

@book{田中宏和2019,
    author         = {田中宏和},
    year           = {2019},
    title          = {計算論的神経科学：脳の運動制御・感覚処理機構の理論的理解へ},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.morikita.co.jp/books/mid/085161},
    publisher      = {森北出版}
}

@article{Bolthausen2014,
	abstract = {We propose an iterative scheme for the solutions of the TAP-equations in the Sherrington--Kirkpatrick model which is shown to converge up to and including the de Almeida--Thouless line. The main tool is a representation of the iterations which reveals an interesting structure of them. This representation does not depend on the temperature parameter, but for temperatures below the de Almeida--Thouless line, it contains a part which does not converge to zero in the limit.},
	author = {Bolthausen, Erwin},
	date = {2014/01/01},
	date-added = {2024-04-08 16:31:48 +0900},
	date-modified = {2024-04-08 16:31:48 +0900},
	doi = {10.1007/s00220-013-1862-3},
	id = {Bolthausen2014},
	isbn = {1432-0916},
	journal = {Communications in Mathematical Physics},
	number = {1},
	pages = {333--366},
	title = {An Iterative Construction of Solutions of the TAP Equations for the Sherrington--Kirkpatrick Model},
	url = {https://doi.org/10.1007/s00220-013-1862-3},
	volume = {325},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1007/s00220-013-1862-3}}

@article{Thouless+1977,
	author = {D. J. Thouless and P. W. Anderson and R. G. Palmer},
	doi = {10.1080/14786437708235992},
	journal = {Philosophical Magazine},
	number = {3},
	pages = {593--601},
	title = {Solution of 'Solvable Model of a Spin Glass'},
	volume = {35},
	year = {1977},
    url  = {https://doi.org/10.1080/14786437708235992}
}

@book{Talagrand2003,
    author         = {Michael Talagrand},
    year           = {2003},
    title          = {Spin Glasses: A Challenge for Mathematicians: Cavity and Mean Field Models},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/9783540003564},
    publisher      = {Springer Berlin, Heidelberg}
}

@article{Chatterjee2010,
	abstract = {We introduce some applications of Stein's method in the high temperature analysis of spin glasses. Stein's method allows the direct analysis of the Gibbs measure without having to eate a cavity. Another advantage is that it gives limit theorems with total variation error bounds, although the bounds can be suboptimal. A surprising byproduct of our analysis is a relatively transparent explanation of the Thouless--Anderson--Palmer system of equations. Along the way, we develop Stein's method for mixtures of two Gaussian densities.},
	author = {Chatterjee, Sourav},
	date = {2010/11/01},
	date-added = {2024-04-08 16:55:40 +0900},
	date-modified = {2024-04-08 16:55:40 +0900},
	doi = {10.1007/s00440-009-0240-8},
	id = {Chatterjee2010},
	isbn = {1432-2064},
	journal = {Probability Theory and Related Fields},
	number = {3},
	pages = {567--600},
	title = {Spin glasses and Stein's method},
	url = {https://doi.org/10.1007/s00440-009-0240-8},
	volume = {148},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1007/s00440-009-0240-8}}


@article{Donoho-Montanari2016,
	abstract = {In a recent article, El Karoui et al. (Proc Natl Acad Sci 110(36):14557--14562, 2013) study the distribution of robust regression estimators in the regime in which the number of parameters p is of the same order as the number of samples n. Using numerical simulations and `highly plausible'heuristic arguments, they unveil a striking new phenomenon. Namely, the regression coefficients contain an extra Gaussian noise component that is not explained by classical concepts such as the Fisher information matrix. We show here that that this phenomenon can be characterized rigorously using techniques that were developed by the authors for analyzing the Lasso estimator under high-dimensional asymptotics. We introduce an approximate message passing (AMP) algorithm to compute M-estimators and deploy state evolution to evaluate the operating characteristics of AMP and so also M-estimates. Our analysis clarifies that the `extra Gaussian noise'encountered in this problem is fundamentally similar to phenomena already studied for regularized least squares in the setting {\$}{\$}n<p{\$}{\$}.},
	author = {David Donoho and Andrea Montanari},
	date = {2016/12/01},
	date-added = {2024-04-08 17:07:35 +0900},
	date-modified = {2024-04-08 17:07:35 +0900},
	doi = {10.1007/s00440-015-0675-z},
	id = {Donoho2016},
	isbn = {1432-2064},
	journal = {Probability Theory and Related Fields},
	number = {3},
	pages = {935--969},
	title = {High dimensional robust M-estimation: asymptotic variance via approximate message passing},
	url = {https://doi.org/10.1007/s00440-015-0675-z},
	volume = {166},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1007/s00440-015-0675-z}}

@article{Griffeath1975b,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/3212726},
 abstract = {The Markov-Dobrush in condition for (weak) ergodicity of non-homogeneous discrete-time Markov chains, and an analogous criterion for continuous chains, are derived by means of coupling techniques.},
 author = {David Griffeath},
 journal = {Journal of Applied Probability},
 number = {4},
 pages = {753--762},
 publisher = {Applied Probability Trust},
 title = {Uniform Coupling of Non-Homogeneous Markov Chains},
 urldate = {2024-04-11},
 volume = {12},
 year = {1975}
}

@article{Metropolis1987,
    author          = {N. Metropolis},
    year            = {1987},
    title           = {The Beginning of the Monte Carlo Method},
    journal         = {Los Alamos Science Special Issue},
    volume          = {15},
    number          = {},
    pages           = {125-130},
    url             = {https://library.lanl.gov/cgi-bin/getfile?00326866.pdf}
}

@article{Eckhardt1987,
    author          = {Roger Eckhardt},
    year            = {1987},
    title           = {{Stan Ulam, John von Neumann, and the Monte Carlo Method}},
    journal         = {Los Alamos Science Special Issue},
    volume          = {15},
    number          = {},
    pages           = {131-143},
    url             = {http://www-star.st-and.ac.uk/~kw25/teaching/mcrt/MC_history_3.pdf}
}

@article{Tartero-Krauth2023,
    author = {Tartero, Gabriele and Krauth, Werner},
    title = {{Concepts in Monte Carlo Sampling}},
    journal = {American Journal of Physics},
    volume = {92},
    number = {1},
    pages = {65-77},
    year = {2024},
    month = {01},
    abstract = {We discuss contemporary ideas in Monte Carlo algorithms in the simplified setting of the one-dimensional anharmonic oscillator. After reviewing the connection between molecular dynamics and Monte Carlo, we introduce the Metropolis and the factorized Metropolis algorithms and lifted non-reversible Markov chains. We, furthermore, illustrate the concept of thinning, where moves are accepted by simple bounding potentials rather than the harmonic and quartic contributions to the anharmonic oscillator. We point out the multiple connections of our example algorithms with real-world sampling problems. This paper is self-contained, and Python implementations are provided.},
    issn = {0002-9505},
    doi = {10.1119/5.0176853},
    url = {https://doi.org/10.1119/5.0176853},
    eprint = {https://pubs.aip.org/aapt/ajp/article-pdf/92/1/65/20107789/65\_1\_5.0176853.pdf},
}

@article{Robert-Casella2011,
 ISSN = {08834237},
 URL = {http://www.jstor.org/stable/23059158},
 abstract = {We attempt to trace the history and development of Markov chain Monte Carlo (MCMC) from its early inception in the late 1940s through its use today. We see how the earlier stages of Monte Carlo (MC, not MCMC) research have led to the algorithms currently in use. More importantly, we see how the development of this methodology has not only changed our solutions to problems, but has changed the way we think about problems.},
 author = {Christian Robert and George Casella},
 journal = {Statistical Science},
 number = {1},
 pages = {102--115},
 publisher = {Institute of Mathematical Statistics},
 title = {A Short History of Markov Chain Monte Carlo: Subjective Recollections from Incomplete Data},
 urldate = {2024-04-18},
 volume = {26},
 year = {2011}
}

@book{砂田利一2004,
    author         = {砂田利一},
    year           = {2004},
    title          = {数学から見た統計学と熱力学},
    series         = {岩波講座物理の世界},
    volume         = {4},
    edition        = {},
    url            = {https://www.iwanami.co.jp/book/b476285.html},
    publisher      = {岩波書店}
}

@book{Khinchin1949,
    author         = {A. Khinchin},
    year           = {1949},
    title          = {Mathematical Foundations of Statistical Mechanics},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Courier Corporation}
}

@article{Diaconis+2000,
 ISSN = {10505164},
 URL = {http://www.jstor.org/stable/2667319},
 abstract = {We analyze the convergence to stationarity of a simple nonreversible Markov chain that serves as a model for several nonreversible Markov chain sampling methods that are used in practice. Our theoretical and numerical results show that nonreversibility can indeed lead to improvements over the diffusive behavior of simple Markov chain sampling schemes. The analysis uses both probabilistic techniques and an explicit diagonalization.},
 author = {Persi Diaconis and Susan Holmes and Radford M. Neal},
 journal = {The Annals of Applied Probability},
 number = {3},
 pages = {726--752},
 publisher = {Institute of Mathematical Statistics},
 title = {Analysis of a Nonreversible Markov Chain Sampler},
 urldate = {2024-04-19},
 volume = {10},
 year = {2000}
}
@inproceedings{Chen+1999,
author = {Chen, Fang and Lov\'{a}sz, L\'{a}szl\'{o} and Pak, Igor},
title = {Lifting Markov chains to speed up mixing},
year = {1999},
isbn = {1581130678},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/301250.301315},
doi = {10.1145/301250.301315},
booktitle = {Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing},
pages = {275–281},
numpages = {7},
location = {Atlanta, Georgia, USA},
series = {STOC '99}
}


@article{Lewis-Shedler1979,
	abstract = {Abstract A simple and relatively efficient method for simulating one-dimensional and two-dimensional nonhomogeneous Poisson processes is presented The method is applicable for any rate function and is based on controlled deletion of points in a Poisson process whose rate function dominates the given rate function In its simplest implementation, the method obviates the need for numerical integration of the rate function, for ordering of points, and for generation of Poisson variates.},
	author = {Lewis, P. A. W and Shedler, G. S.},
	doi = {10.1002/nav.3800260304},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/nav.3800260304},
	journal = {Naval Research Logistics Quarterly},
	number = {3},
	pages = {403-413},
	title = {Simulation of nonhomogeneous poisson processes by thinning},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800260304},
	volume = {26},
	year = {1979},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/nav.3800260304},
	bdsk-url-2 = {https://doi.org/10.1002/nav.3800260304}}


@article{Duane-Kogut1986,
	abstract = {The theory of hybrid stochastic algorithms is developed. A generalized Fokker-Planck equation is derived and is used to prove that the correct equilibrium distribution is generated by the algorithm. Systematic errors following from the discrete time-step used the numerical implementation of the scheme are computed. Hybrid algorithms which simulate lattice gauge theory with dynamical fermions are presented. They are optimized in computer simulations and their systematic errors and efficiencies are studied.},
	author = {S. Duane and J.B. Kogut},
	doi = {10.1016/0550-3213(86)90606-1},
	issn = {0550-3213},
	journal = {Nuclear Physics B},
	number = {3},
	pages = {398-420},
	title = {The theory of hybrid stochastic algorithms},
	url = {https://www.sciencedirect.com/science/article/pii/0550321386906061},
	volume = {275},
	year = {1986},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0550321386906061},
	bdsk-url-2 = {https://doi.org/10.1016/0550-3213(86)90606-1}}


@article{Duane1985,
	abstract = {The aim of this paper is to shed further light on the relation between two non-standard formulations of field theory: stochastic quantization and the microcanonical ensemble. One involves a first-order (Langevin) differential equation in a fictitious ``time'', and the other a second-order ordinary differential equation. I analyze a scheme which is a particular example of a canonical ensemble, and which reduces to the old schemes in different limits. For a gaussian degree of freedom it turns out that the autocorrelation function in the new scheme undergoes damped harmonic motion, and the scheme is optimized (for numerical simulation) at critical damping. This is a clear improvement over the old limits, which correspond to maximal and zero damping. For non-gaussian systems I argue that the new proposal always represents a significant improvement over a Langevin simulation, and may even improve over the microcanonical method, in which case only a trivial code modification is required. A useful by-product of the discussion is a better estimate of systematic errors in microcanonical simulations.},
	author = {Simon Duane},
	doi = {10.1016/0550-3213(85)90369-4},
	issn = {0550-3213},
	journal = {Nuclear Physics B},
	pages = {652-662},
	title = {Stochastic quantization versus the microcanonical ensemble: Getting the best of both worlds},
	url = {https://www.sciencedirect.com/science/article/pii/0550321385903694},
	volume = {257},
	year = {1985},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0550321385903694},
	bdsk-url-2 = {https://doi.org/10.1016/0550-3213(85)90369-4}}

@article{Parisi-Wu1981,
  author = "G. Parisi and Yongshi Wu",
  title = {PERTURBATION THEORY WITHOUT GAUGE FIXING},
  journal = "Scientia Sinica",
  year = "1981",
  volume = "24",
  number = "4",
  pages = "483-",
  url = {https://www.sciengine.com/Math%20A0/doi/10.1360/ya1981-24-4-483},
}

@phdthesis{酒井佑士2017,
    author      = {酒井佑士},
    school      = {東京大学},
    title       = {マルコフ連鎖モンテカルロ法における詳細つり合い条件の破れの効果と応用},
    year        = {2017},
    url         = {https://repository.dl.itc.u-tokyo.ac.jp/records/50422},
}

@article{Turitsyn+2011,
title = "{Irreversible Monte Carlo algorithms for Efficient Sampling}",
abstract = "Equilibrium systems evolve according to Detailed Balance (DB). This principle guided the development of Monte Carlo sampling techniques, of which the Metropolis-Hastings (MH) algorithm is the famous representative. It is also known that DB is sufficient but not necessary. We construct irreversible deformation of a given reversible algorithm capable of dramatic improvement of sampling from known distribution. Our transformation modifies transition rates keeping the structure of transitions intact. To illustrate the general scheme we design an Irreversible version of Metropolis-Hastings (IMH) and test it on an example of a spin cluster. Standard MH for the model suffers from critical slowdown, while IMH is free from critical slowdown. Published by Elsevier B.V.",
author = "Turitsyn, {Konstantin S.} and Michael Chertkov and Marija Vucelja",
note = "US Department of Energy at Los Alamos National Laboratory [DE-AC52-06NA25396]The authors are grateful to V. Chernyak, F. Krzakala, J. Machta, D. Shah and T. Witten for inspiring discussions and useful remarks. The work at LANL was carried out under the auspices of the National Nuclear Security Administration of the US Department of Energy at Los Alamos National Laboratory under Contract No. DE-AC52-06NA25396. MC also acknowledges the Weston Visiting Professorship Program supporting his stay at the Weizmann Institute, where part of this work was done.",
year = "2011",
month = feb,
doi = "10.1016/j.physd.2010.10.003",
language = "English",
volume = "240",
pages = "410--414",
journal = "Physica D-Nonlinear Phenomena",
issn = "0167-2789",
publisher = "Elsevier Science",
number = "5-Apr",}

@article{Sugita-Okamoto1999,
	abstract = {We have developed a formulation for molecular dynamics algorithm for the replica-exchange method. The effectiveness of the method for the protein-folding problem is tested with the penta-peptide Met-enkephalin. The method can overcome the multiple-minima problem by exchanging non-interacting replicas of the system at several temperatures. From only one simulation run, one can obtain probability distributions in canonical ensemble for a wide temperature range using multiple-histogram reweighting techniques, which allows the calculation of any thermodynamic quantity as a function of temperature in that range.},
	author = {Yuji Sugita and Yuko Okamoto},
	doi = {https://doi.org/10.1016/S0009-2614(99)01123-9},
	issn = {0009-2614},
	journal = {Chemical Physics Letters},
	number = {1},
	pages = {141-151},
	title = {Replica-exchange molecular dynamics method for protein folding},
	url = {https://www.sciencedirect.com/science/article/pii/S0009261499011239},
	volume = {314},
	year = {1999},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0009261499011239},
	bdsk-url-2 = {https://doi.org/10.1016/S0009-2614(99)01123-9}}

@article{岡本祐幸2010,
	author = {岡本祐幸},
	doi = {10.11436/mssj.12.2_6},
	journal = {アンサンブル},
	number = {2},
	pages = {2_6-2_7},
	title = {「拡張アンサンブル法」の特集にあたって},
	volume = {12},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.11436/mssj.12.2_6}}


@article{Lyubartsev+1992,
    author = {Lyubartsev, A. P. and Martsinovski, A. A. and Shevkunov, S. V. and Vorontsov‐Velyaminov, P. N.},
    title = "{New approach to Monte Carlo calculation of the free energy: Method of expanded ensembles}",
    journal = {The Journal of Chemical Physics},
    volume = {96},
    number = {3},
    pages = {1776-1783},
    year = {1992},
    month = {02},
    abstract = "{We propose a new effective Monte Carlo (MC) procedure for direct calculation of the free energy in a single MC run. The partition function of the expanded ensemble is introduced including a sum of canonical partition functions with a set of temperatures and additive factors (modification). Random walk in the space of both particle coordinates and temperatures provides calculation of free energy in a wide range of T. The method was applied to a primitive model of electrolyte including the region of low temperatures. In similar way other variants of expanded ensembles are constructed (e.g., over the number of particles N or volume V). Its facilities in quantum statistics (path integral Monte Carlo) and some other applications are also discussed.}",
    issn = {0021-9606},
    doi = {10.1063/1.462133},
    url = {https://doi.org/10.1063/1.462133},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/96/3/1776/18997202/1776\_1\_online.pdf},
}

@article{Roberts-Rosenthal1998,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/2985986},
 abstract = {We consider the optimal scaling problem for proposal distributions in Hastings--Metropolis algorithms derived from Langevin diffusions. We propose an asymptotic diffusion limit theorem and show that the relative efficiency of the algorithm can be characterized by its overall acceptance rate, independently of the target distribution. The asymptotically optimal acceptance rate is 0.574. We show that, as a function of dimension n, the complexity of the algorithm is O(n1/3), which compares favourably with the O(n) complexity of random walk Metropolis algorithms. We illustrate this comparison with some example simulations.},
 author = {Gareth O. Roberts and Jeffrey S. Rosenthal},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {1},
 pages = {255--268},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Optimal Scaling of Discrete Approximations to Langevin Diffusions},
 urldate = {2024-04-22},
 volume = {60},
 year = {1998}
}

@article{Roberts-Rosenthal2016,
 ISSN = {00219002},
 URL = {http://www.jstor.org/stable/43860930},
 abstract = {We connect known results about diffusion limits of Markov chain Monte Carlo (MCMC) algorithms to the computer science notion of algorithm complexity. Our main result states that any weak limit of a Markov process implies a corresponding complexity bound (in an appropriate metric). We then combine this result with previously-known MCMC diffusion limit results to prove that under appropriate assumptions, the random-walk Metropolis algorithm in d dimensions takes O(d) iterations to converge to stationarity, while the Metropolis-adjusted Langevin algorithm takes O(d⅓) iterations to converge to stationarity.},
 author = {Gareth O. Roberts and Jeffrey S. Rosenthal},
 journal = {Journal of Applied Probability},
 number = {2},
 pages = {410--420},
 publisher = {Applied Probability Trust},
 title = {{Complexity Bounds for Markov Chain Monte Carlo Algorithms via Diffusion Limits}},
 urldate = {2024-06-05},
 volume = {53},
 year = {2016}
}


@article{Besag1994,
    author          = {Julian E. Besag},
    year            = {1994},
    title           = {{Comments on ‘Representations of Knowledge in Complex Systems’ by U. Grenander and M. I. Miller}},
    journal         = {Journal of the Royal Statistical Society. Series B (Methodological)},
    volume          = {56},
    number          = {4},
    pages           = {591-592},
    url             = {https://www.jstor.org/stable/2346184}
}

@misc{Corenflos-Finke2024,
      title={Particle-MALA and Particle-mGRAD: Gradient-based MCMC methods for high-dimensional state-space models}, 
      author={Adrien Corenflos and Axel Finke},
      year={2024},
      eprint={2401.14868},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url          = {https://arxiv.org/abs/2401.14868},
}


@article{Neal1994,
	abstract = {The probability of accepting a candidate move in the hybrid Monte Carlo algorithm can be increased by considering a transition to be between windows of several states at the beginning and end of the trajectory, with a particular state within the selected window then being chosen according to the Boltzmann probabilities. The detailed balance condition used to justify the algorithm still holds with this procedure, provided the start state is randomly positioned within its window. The new procedure is shown empirically to significantly improve the acceptance rate for a test system of uncoupled oscillators. It also allows expectations to be estimated using data from all states in the windows, rather than just states that are accepted.},
	author = {Radford M. Neal},
	doi = {10.1006/jcph.1994.1054},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	number = {1},
	pages = {194-203},
	title = {An Improved Acceptance Procedure for the Hybrid Monte Carlo Algorithm},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999184710540},
	volume = {111},
	year = {1994},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0021999184710540},
	bdsk-url-2 = {https://doi.org/10.1006/jcph.1994.1054}}

@article{Sheu1991,
 ISSN = {00911798},
 URL = {http://www.jstor.org/stable/2244362},
 abstract = {In this paper we study the transition density Pt(x, y) of a nondegenerate diffusion process by using the stochastic control method invented by Fleming and the idea of stochastic parallel translation. We obtain a two-sided estimate for Pt(x, y) as well as some bounds for the derivatives of log Pt(x, y).},
 author = {Shuenn-Jyi Sheu},
 journal = {The Annals of Probability},
 number = {2},
 pages = {538--561},
 publisher = {Institute of Mathematical Statistics},
 title = {Some Estimates of the Transition Density of a Nondegenerate Diffusion Markov Process},
 urldate = {2024-04-23},
 volume = {19},
 year = {1991}
}

@article{Bortz+1975,
title = {A new algorithm for Monte Carlo simulation of Ising spin systems},
journal = {Journal of Computational Physics},
volume = {17},
number = {1},
pages = {10-18},
year = {1975},
issn = {0021-9991},
doi = {https://doi.org/10.1016/0021-9991(75)90060-1},
url = {https://www.sciencedirect.com/science/article/pii/0021999175900601},
author = {A.B. Bortz and M.H. Kalos and J.L. Lebowitz},
abstract = {We describe a new algorithm for Monte Carlo simulation of Ising spin systems and present results of a study comparing the speed of the new technique to that of a standard technique applied to a square lattice of 6400 spins evolving via single spin flips. We find that at temperatures T < Tc, the critical temperature, the new technique is faster than the standard technique, being ten times faster at T = 0.588 Tc. We expect that the new technique will be especially valuable in Monte Carlo simulation of the time evolution of binary alloy systems. The new algorithm is essentially a reorganization of the standard algorithm. It accounts for the a priori probability of changing spins before, rather than after, choosing the spin or spins to change.}
}
@book{Shiryaev2016,
    author         = {Albert N. Shiryaev},
    year           = {2016},
    title          = {Probability-1},
    series         = {Graduate Texts in Mathematics},
    volume         = {95},
    edition        = {3},
    url            = {https://link.springer.com/book/10.1007/978-0-387-72206-1},
    publisher      = {Springer New York}
}

@article{Terada-Toyoizumi2024,
	abstract = {Cortical neurons exhibit highly variable responses over trials and time. Theoretical works posit that this variability arises potentially from chaotic network dynamics of recurrently connected neurons. Here, we demonstrate that chaotic neural dynamics, formed through synaptic learning, allow networks to perform sensory cue integration in a sampling-based implementation. We show that the emergent chaotic dynamics provide neural substrates for generating samples not only of a static variable but also of a dynamical trajectory, where generic recurrent networks acquire these abilities with a biologically plausible learning rule through trial and error. Furthermore, the networks generalize their experience in the stimulus-evoked samples to the inference without partial or all sensory information, which suggests a computational role of spontaneous activity as a representation of the priors as well as a tractable biological computation for marginal distributions. These findings suggest that chaotic neural dynamics may serve for the brain function as a Bayesian generative model.},
	author = {Yu Terada and Taro Toyoizumi},
	doi = {10.1073/pnas.2312992121},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.2312992121},
	journal = {Proceedings of the National Academy of Sciences},
	number = {18},
	pages = {e2312992121},
	title = {Chaotic neural dynamics facilitate probabilistic computations through sampling},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.2312992121},
	volume = {121},
	year = {2024},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.2312992121},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.2312992121}}


@article{Berkes+2011,
	abstract = {Internal models of the environment optimize as the brain develops. The brain maintains internal models of its environment to interpret sensory inputs and to prepare actions. Although behavioral studies have demonstrated that these internal models are optimally adapted to the statistics of the environment, the neural underpinning of this adaptation is unknown. Using a Bayesian model of sensory cortical processing, we related stimulus-evoked and spontaneous neural activities to inferences and prior expectations in an internal model and predicted that they should match if the model is statistically optimal. To test this prediction, we analyzed visual cortical activity of awake ferrets during development. Similarity between spontaneous and evoked activities increased with age and was specific to responses evoked by natural scenes. This demonstrates the progressive adaptation of internal models to the statistics of natural stimuli at the neural level.},
	author = {Pietro Berkes and Gerg{\H o} Orb{\'a}n and M{\'a}t{\'e} Lengyel and J{\'o}zsef Fiser},
	doi = {10.1126/science.1195870},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.1195870},
	journal = {Science},
	number = {6013},
	pages = {83-87},
	title = {Spontaneous Cortical Activity Reveals Hallmarks of an Optimal Internal Model of the Environment},
	url = {https://www.science.org/doi/abs/10.1126/science.1195870},
	volume = {331},
	year = {2011},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.1195870},
	bdsk-url-2 = {https://doi.org/10.1126/science.1195870}}

@book{Rogers-Williams2000,
    author         = {L. C. G. Rogers and David Williams},
    year           = {2000},
    title          = {Diffusions, Markov Processes, and Martingales. Volume I: Foundatinos.},
    series         = {Cambridge Mathematical Library},
    volume         = {},
    edition        = {2},
    url            = {https://www.cambridge.org/core/books/diffusions-markov-processes-and-martingales/188B6A2BAABAF735E61796C3CD18114B},
    publisher      = {Cambridge University Press}
}

@article{Stroock-Varadhan1969,
	author = {Stroock, Daniel W. and Varadhan, S. R. S.},
	doi = {https://doi.org/10.1002/cpa.3160220304},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.3160220304},
	journal = {Communications on Pure and Applied Mathematics},
	number = {3},
	pages = {345-400},
	title = {Diffusion processes with continuous coefficients, I},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160220304},
	volume = {22},
	year = {1969},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160220304},
	bdsk-url-2 = {https://doi.org/10.1002/cpa.3160220304}}

@article{Harris1955,
author = {T. E. Harris},
title = {{On chains of infinite order}},
volume = {5},
journal = {Pacific Journal of Mathematics},
number = {S1},
publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
pages = {707 -- 724},
year = {1955},
}

@book{Petersen1983,
    author         = {Karl E. Petersen},
    year           = {1983},
    title          = {Ergodic Theory},
    series         = {Cambridge Studies in Advanced Mathematics},
    volume         = {2},
    edition        = {},
    url            = {https://doi.org/10.1017/CBO9780511608728},
    publisher      = {Cambridge University Press}
}

@article{Bierkens-Roberts2017,
 ISSN = {10505164, 21688737},
 URL = {http://www.jstor.org/stable/44249153},
 abstract = {In Turitsyn, Chertkov and Vucelja [Phys. D 240 (2011) 410-414] a nonreversible Markov Chain Monte Carlo (MCMC) method on an augmented state space was introduced, here referred to as Lifted Metropolis-Hastings (LMH). A scaling limit of the magnetization process in the Curie-Weiss model is derived for LMH, as well as for Metropolis–Hastings (MH). The required jump rate in the high (supercritical) temperature regime equals n½ for LMH, which should be compared to n for MH. At the critical temperature, the required jump rate equals n¾ for LMH and n3/2 for MH, in agreement with experimental results of Turitsyn, Chertkov and Vucelja (2011). The scaling limit of LMH turns out to be a nonreversible piecewise deterministic exponentially ergodic "zig-zag" Markov process.},
 author = {Joris Bierkens and Gareth Roberts},
 journal = {The Annals of Applied Probability},
 number = {2},
 pages = {846--882},
 publisher = {Institute of Mathematical Statistics},
 title = {{A Piecewise Deterministic Scaling Limit of Lifted Metropolis-Hastings in the Curie-Weiss Model}},
 urldate = {2024-05-17},
 volume = {27},
 year = {2017}
}
@article{Roberts+1997,
 ISSN = {10505164},
 URL = {http://www.jstor.org/stable/2245134},
 abstract = {This paper considers the problem of scaling the proposal distribution of a multidimensional random walk Metropolis algorithm in order to maximize the efficiency of the algorithm. The main result is a weak convergence result as the dimension of a sequence of target densities, n, converges to ∞. When the proposal variance is appropriately scaled according to n, the sequence of stochastic processes formed by the first component of each Markov chain converges to the appropriate limiting Langevin diffusion process. The limiting diffusion approximation admits a straightforward efficiency maximization problem, and the resulting asymptotically optimal policy is related to the asymptotic acceptance rate of proposed moves for the algorithm. The asymptotically optimal acceptance rate is 0.234 under quite general conditions. The main result is proved in the case where the target density has a symmetric product form. Extensions of the result are discussed.},
 author = {G. O. Roberts and A. Gelman and W. R. Gilks},
 journal = {The Annals of Applied Probability},
 number = {1},
 pages = {110--120},
 publisher = {Institute of Mathematical Statistics},
 title = {{Weak Convergence and Optimal Scaling of Random Walk Metropolis Algorithms}},
 urldate = {2024-05-21},
 volume = {7},
 year = {1997}
}
@incollection{Gelman+1996,
    author = {Gelman , A and Roberts, G O and Gilks, W R},
    isbn = {9780198523567},
    title = "{Efficient Metropolis Jumping Rules}",
    booktitle = "{Bayesian Statistics 5: Proceedings of the Fifth Valencia International Meeting}",
    publisher = {Oxford University Press},
    year = {1996},
    month = {05},
    abstract = "{The algorithm of Metropolis et al. (1953) and its generalizations have been increasingly popular in computational physics and, more recently, statistics, for sampling from intractable multivariate distributions. Much recent research has been devoted to increasing the efficiency of simulation algorithms by altering the jumping rules for Metropolis-like algorithms. We study a very specific question: What are the most efficient symmetric jumping kernels for simulating a normal target distribution using the Metropolis algorithmã We provide a general theoretical result as the dimension of a class of canonical problems goes to ∞ and numerical approximations and simulations for low-dimensional Gaussian target distributions that show that the limiting results provide extremely accurate approximations in six and higher dimensions.}",
    doi = {10.1093/oso/9780198523567.003.0038},
    url = {https://doi.org/10.1093/oso/9780198523567.003.0038},
    eprint = {https://academic.oup.com/book/0/chapter/422210114/chapter-pdf/52447340/isbn-9780198523567-book-part-38.pdf},
}



@article{Tierney1994,
	author = {Luke Tierney},
	doi = {10.1214/aos/1176325750},
	journal = {The Annals of Statistics},
	keywords = {62-04, Gibbs sampler, Metropolis-Hastings algorithm, Monte Carlo, variance reduction},
	number = {4},
	pages = {1701 -- 1728},
	publisher = {Institute of Mathematical Statistics},
	title = {{Markov Chains for Exploring Posterior Distributions}},
	url = {https://doi.org/10.1214/aos/1176325750},
	volume = {22},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1176325750}}
@article{Vasdekis-Roberts2022, title={A note on the polynomial ergodicity of the one-dimensional Zig-Zag process}, volume={59}, DOI={10.1017/jpr.2021.97}, number={3}, journal={Journal of Applied Probability}, author={Vasdekis, Giorgos and Roberts, Gareth O.}, year={2022}, pages={895-903}}

@book{樺島祥介2002,
    author         = {樺島祥介},
    year           = {2002},
    title          = {学習と情報の平均場理論},
    series         = {岩波講座 物理の世界 物理と情報},
    volume         = {2},
    edition        = {},
    url            = {https://www.iwanami.co.jp/book/b476277.html},
    publisher      = {岩波書店}
}

@book{田崎晴明2008II,
    author         = {田崎晴明},
    title          = {統計力学II},
    year           = {2008},
    month          = {12},
    publisher      = {培風館},
    series         = {新物理学シリーズ},
    volume         = {38},
    edition        = {},
    howpublished   = {}
}

@article{Glauber1963,
    author = {Glauber, Roy J.},
    title = "{Time-Dependent Statistics of the Ising Model}",
    journal = {Journal of Mathematical Physics},
    volume = {4},
    number = {2},
    pages = {294-307},
    year = {1963},
    month = {02},
    abstract = "{The individual spins of the Ising model are assumed to interact with an external agency (e.g., a heat reservoir) which causes them to change their states randomly with time. Coupling between the spins is introduced through the assumption that the transition probabilities for any one spin depend on the values of the neighboring spins. This dependence is determined, in part, by the detailed balancing condition obeyed by the equilibrium state of the model. The Markoff process which describes the spin functions is analyzed in detail for the case of a closed N‐member chain. The expectation values of the individual spins and of the products of pairs of spins, each of the pair evaluated at a different time, are found explicitly. The influence of a uniform, time‐varying magnetic field upon the model is discussed, and the frequency‐dependent magnetic susceptibility is found in the weak‐field limit. Some fluctuation‐dissipation theorems are derived which relate the susceptibility to the Fourier transform of the time‐dependent correlation function of the magnetization at equilibrium.}",
    issn = {0022-2488},
    url = {https://doi.org/10.1063/1.1703954},
    eprint = {https://pubs.aip.org/aip/jmp/article-pdf/4/2/294/19156949/294\_1\_online.pdf},
}

@inproceedings{Yang2019,
	author = {Yang, Greg},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Wide Feedforward or Recurrent Neural Networks of Any Architecture are Gaussian Processes},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5e69fda38cda2060819766569fd93aa5-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5e69fda38cda2060819766569fd93aa5-Paper.pdf}}

@article{Kennedy-Pendleton1991,
	abstract = {We present the results of an analytic study of the Hybrid Monte Carlo algorithm for free field theory. We calculate the acceptance rate and autocorrelation function as a function of lattice volume, integration step size, and (average) trajectory length. We show that the dynamical critical exponent z can be tuned to unity by a judicious choice of average trajectory length.},
	author = {A.D. Kennedy and Brian Pendleton},
	doi = {https://doi.org/10.1016/0920-5632(91)90893-J},
	issn = {0920-5632},
	journal = {Nuclear Physics B - Proceedings Supplements},
	pages = {118-121},
	title = {Acceptances and autocorrelations in hybrid Monte Carlo},
	url = {https://www.sciencedirect.com/science/article/pii/092056329190893J},
	volume = {20},
	year = {1991},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/092056329190893J},
	bdsk-url-2 = {https://doi.org/10.1016/0920-5632(91)90893-J}}
@Article{Mountain-Thriumalai1994,
  author={Mountain, Raymond D. and Thirumalai, D.},
  title={{Quantative measure of efficiency of Monte Carlo simulations}},
  journal={Physica A: Statistical Mechanics and its Applications},
  year=1994,
  volume={210},
  number={3},
  pages={453-460},
  month={},
  keywords={},
  doi={10.1016/0378-4371(94)9009},
  abstract={An easily applied, physically motivated algorithm for determining the efficiency of Monte Carlo simulations is introduced. The theoretical basis for the algorithm is developed. As an illustration we apply the method to the Lennard-Jones liquid near the triple point. We show that an acceptance ratio of 0.2 is twice as efficient for the purpose of generating a satisfactory sample as is an acceptance ratio of 0.5. There is a strong correlation between the efficiency measure and the diffusion rate of liquid particles during the simulation. We argue that the optimal value of the acceptance ratio is calculable from short Monte Carlo simulations. The method is very general and is applicable to Monte Carlo simulations involving arbitrary potentials.},
  url={https://ideas.repec.org/a/eee/phsmap/v210y1994i3p453-460.html}
}

@techreport{Neal1993,
    author      = {Radford M. Neal},
    institution = {Department of Computer Science, University of Toronto},
    title       = {{Probabilistic Inference using Markov Chain Monte Carlo Methods}},
    year        = {1993},
    url         = {https://glizen.com/radfordneal/review.abstract.html},
}

@article{Bhattacharya1978,
	author = {R. N. Bhattacharya},
	doi = {10.1214/aop/1176995476},
	journal = {The Annals of Probability},
	keywords = {$L$-harmonic functions, Invariant measures, strong Markov property},
	number = {4},
	pages = {541 -- 553},
	publisher = {Institute of Mathematical Statistics},
	title = {{Criteria for Recurrence and Existence of Invariant Measures for Multidimensional Diffusions}},
	url = {https://doi.org/10.1214/aop/1176995476},
	volume = {6},
	year = {1978},
	bdsk-url-1 = {https://doi.org/10.1214/aop/1176995476}}

@book{Ikeda-Watanabe1981,
    author         = {Nobuyuki Ikeda and Shinzo Watanabe},
    year           = {1981},
    title          = {Stochastic Differential Equations and Diffusion Processes},
    series         = {North-Holland Mathematical Library},
    volume         = {24},
    edition        = {},
    url            = {https://doi.org/10.1016/S0924-6509(08)70217-4},
    publisher      = {Elsevier}
}
@article{Meyn-Tweedie1993,
 ISSN = {00018678},
 URL = {http://www.jstor.org/stable/1427521},
 abstract = {In this paper we extend the results of Meyn and Tweedie (1992b) from discrete-time parameter to continuous-parameter Markovian processes Φ evolving on a topological space. We consider a number of stability concepts for such processes in terms of the topology of the space, and prove connections between these and standard probabilistic recurrence concepts. We show that these structural results hold for a major class of processes (processes with continuous components) in a manner analogous to discrete-time results, and that complex operations research models such as storage models with state-dependent release rules, or diffusion models such as those with hypoelliptic generators, have this property. Also analogous to discrete time, 'petite sets', which are known to provide test sets for stability, are here also shown to provide conditions for continuous components to exist. New ergodic theorems for processes with irreducible and countably reducible skeleton chains are derived, and we show that when these conditions do not hold, then the process may be decomposed into an uncountable orbit of skeleton chains.},
 author = {Sean P. Meyn and R. L. Tweedie},
 journal = {Advances in Applied Probability},
 number = {3},
 pages = {487--517},
 publisher = {Applied Probability Trust},
 title = {Stability of Markovian Processes II: Continuous-Time Processes and Sampled Chains},
 urldate = {2024-06-04},
 volume = {25},
 year = {1993}
}

@book{Meyn-Tweedie2009,
    author         = {Sean Meyn and Richard L. Tweedie},
    year           = {2009},
    title          = {Markov Chains and Stochastic Stability},
    series         = {Cambridge Mathematical Library},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1017/CBO9780511626630},
    publisher      = {Cambridge University Press}
}

@book{Wickham2019,
    author         = {Hadley Wickham},
    year           = {2019},
    title          = {Advanced R},
    series         = {The R Series},
    volume         = {},
    edition        = {2},
    url            = {https://adv-r.hadley.nz/index.html},
    publisher      = {Chapman \& Hall}
}
@article{Burkner2017,
	abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to fit - among others - linear, robust linear, binomial, Poisson, survival, ordinal, zero-inflated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user defined covariance structures, censored data, as well as meta-analytic standard errors. Prior specifications are flexible and explicitly encourage users to apply prior distributions that actually reflect their beliefs. In addition, model fit can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.},
	author = {B{\"u}rkner, Paul-Christian},
	doi = {10.18637/jss.v080.i01},
	journal = {Journal of Statistical Software},
	number = {1},
	pages = {1--28},
	title = {brms: An R Package for Bayesian Multilevel Models Using Stan},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v080i01},
	volume = {80},
	year = {2017},
	bdsk-url-1 = {https://www.jstatsoft.org/index.php/jss/article/view/v080i01},
	bdsk-url-2 = {https://doi.org/10.18637/jss.v080.i01}}
@article{Burkner2018,
  author = {Paul-Christian Bürkner},
  title = {{Advanced Bayesian Multilevel Modeling with the R Package brms}},
  year = {2018},
  journal = {{The R Journal}},
  doi = {10.32614/RJ-2018-017},
  url = {https://doi.org/10.32614/RJ-2018-017},
  pages = {395--411},
  volume = {10},
  number = {1}
}

@article{Burkner2021,
	abstract = {&amp;lt;p&amp;gt;Item response theory (IRT) is widely applied in the human sciences to model persons' responses on a set of items measuring one or more latent constructs. While several R packages have been developed that implement IRT models, they tend to be restricted to respective pre-specified classes of models. Further, most implementations are frequentist while the availability of Bayesian methods remains comparably limited. I demonstrate how to use the R package brms together with the probabilistic programming language Stan to specify and fit a wide range of Bayesian IRT models using flexible and intuitive multilevel formula syntax. Further, item and person parameters can be related in both a linear or non-linear manner. Various distributions for categorical, ordinal, and continuous responses are supported. Users may even define their own custom response distribution for use in the presented framework. Common IRT model classes that can be specified natively in the presented framework include 1PL and 2PL logistic models optionally also containing guessing parameters, graded response and partial credit ordinal models, as well as drift diffusion models of response times coupled with binary decisions. Posterior distributions of item and person parameters can be conveniently extracted and postprocessed. Model fit can be evaluated and compared using Bayes factors and efficient cross-validation procedures.&amp;lt;/p&amp;gt;},
	author = {B{\"u}rkner, Paul-Christian},
	doi = {10.18637/jss.v100.i05},
	journal = {Journal of Statistical Software},
	number = {5},
	pages = {1--54},
	title = {Bayesian Item Response Modeling in R with brms and Stan},
	url = {https://www.jstatsoft.org/index.php/jss/article/view/v100i05},
	volume = {100},
	year = {2021},
	bdsk-url-1 = {https://www.jstatsoft.org/index.php/jss/article/view/v100i05},
	bdsk-url-2 = {https://doi.org/10.18637/jss.v100.i05}}
@article{Aalen1978,
    author          = {Odd Aalen},
    year            = {1978},
    title           = {Nonparametric Inference for a Family of Counting Processes},
    journal         = {The Annals of Statistics},
    volume          = {6},
    number          = {4},
    pages           = {701-726},
    url             = {https://www.jstor.org/stable/2958850}
}

@book{Andersen+1993,
    author         = {Per Kragh Andersen and Ørnulf Borgan and Richard D. Gill and Niels Keiding},
    year           = {1993},
    title          = {Statistical Models Based on Counting Processes},
    series         = {Springer Series in Statistics},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-1-4612-4348-9},
    publisher      = {Springer New York}
}


@article{Cox1955,
	abstract = {SUMMARY The paper deals with a number of problems of statistical analysis connected with events occurring haphazardly in space or time. The topics discussed include: tests of randomness, components of variance, the correlation between events of different types, and a modification of the snap-round method used in operational research.},
	author = {Cox, D. R.},
	doi = {https://doi.org/10.1111/j.2517-6161.1955.tb00188.x},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1955.tb00188.x},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	number = {2},
	pages = {129-157},
	title = {Some Statistical Methods Connected with Series of Events},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1955.tb00188.x},
	volume = {17},
	year = {1955},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1955.tb00188.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2517-6161.1955.tb00188.x}}


@book{Mitov-Omey2014,
    author         = {Kosto V. Mitov and Edward Omey},
    year           = {2014},
    title          = {Renewal Processes},
    series         = {Springer Briefs in Statistics},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-3-319-05855-9},
    publisher      = {Springer Cham}
}
@book{Resnick2002,
    author         = {Sidney I. Resnick},
    year           = {2002},
    title          = {Adventures in Stochastic Processes},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-1-4612-0387-2},
    publisher      = {Birkhäuser Boston}
}

@article{Thall-Vail1990,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2532086},
 abstract = {A family of covariance models for longitudinal counts with predictive covariates is presented. These models account for overdispersion, heteroscedasticity, and dependence among repeated observations. The approach is a quasi-likelihood regression similar to the formulation given by Liang and Zeger (1986, Biometrika 73, 13-22). Generalized estimating equations for both the covariate parameters and the variance-covariance parameters are presented. Large-sample properties of the parameter estimates are derived. The proposed methods are illustrated by an analysis of epileptic seizure count data arising from a study of progabide as an adjuvant therapy for partial seizures.},
 author = {Peter F. Thall and Stephen C. Vail},
 journal = {Biometrics},
 number = {3},
 pages = {657--671},
 publisher = {[Wiley, International Biometric Society]},
 title = {Some Covariance Models for Longitudinal Count Data with Overdispersion},
 urldate = {2024-06-11},
 volume = {46},
 year = {1990}
}

@article{Vehtari+2021,
	author = {Aki Vehtari and Andrew Gelman and Daniel Simpson and Bob Carpenter and Paul-Christian B{\"u}rkner},
	doi = {10.1214/20-BA1221},
	journal = {Bayesian Analysis},
	number = {2},
	pages = {667 -- 718},
	publisher = {International Society for Bayesian Analysis},
	title = {{Rank-Normalization, Folding, and Localization: An Improved $\widehat{R}$ for Assessing Convergence of MCMC (with Discussion)}},
	url = {https://doi.org/10.1214/20-BA1221},
	volume = {16},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/20-BA1221}}

@article{Bafumi-Gelman2007,
    author          = {Joseph Bafumi and Andrew Gelman},
    year            = {2007},
    title           = {{Fitting Multilevel Models When Predictors and Group Effects Correlate}},
    journal         = {SSRN},
    volume          = {},
    number          = {},
    pages           = {},
    url             = {https://dx.doi.org/10.2139/ssrn.1010095}
}

@inproceedings{Kincaid2005,
    author          = {Charles D. Kincaid},
    year            = {2005},
    title           = {{Guidelines for Selecting the Covariance Structure in Mixed Model Analysis}},
    booktitle       = {SAS User Group International},
    volume          = {30},
    pages           = {198-30},
    url             = {https://support.sas.com/resources/papers/proceedings/proceedings/sugi30/198-30.pdf}
}


@article{Chung+2015,
	abstract = { When fitting hierarchical regression models, maximum likelihood (ML) estimation has computational (and, for some users, philosophical) advantages compared to full Bayesian inference, but when the number of groups is small, estimates of the covariance matrix (Σ) of group-level varying coefficients are often degenerate. One can do better, even from a purely point estimation perspective, by using a prior distribution or penalty function. In this article, we use Bayes modal estimation to obtain positive definite covariance matrix estimates. We recommend a class of Wishart (not inverse-Wishart) priors for Σ with a default choice of hyperparameters, that is, the degrees of freedom are set equal to the number of varying coefficients plus 2, and the scale matrix is the identity matrix multiplied by a value that is large relative to the scale of the problem. This prior is equivalent to independent gamma priors for the eigenvalues of Σ with shape parameter 1.5 and rate parameter close to 0. It is also equivalent to independent gamma priors for the variances with the same hyperparameters multiplied by a function of the correlation coefficients. With this default prior, the posterior mode for Σ is always strictly positive definite. Furthermore, the resulting uncertainty for the fixed coefficients is less underestimated than under classical ML or restricted maximum likelihood estimation. We also suggest an extension of our method that can be used when stronger prior information is available for some of the variances or correlations. },
	author = {Yeojin Chung and Andrew Gelman and Sophia Rabe-Hesketh and Jingchen Liu and Vincent Dorie},
	doi = {10.3102/1076998615570945},
	eprint = {https://doi.org/10.3102/1076998615570945},
	journal = {Journal of Educational and Behavioral Statistics},
	number = {2},
	pages = {136-157},
	title = {Weakly Informative Prior for Point Estimation of Covariance Matrices in Hierarchical Models},
	url = {https://doi.org/10.3102/1076998615570945},
	volume = {40},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.3102/1076998615570945}}


@article{Chung+2013,
	abstract = {Group-level variance estimates of zero often arise when fitting multilevel or hierarchical linear models, especially when the number of groups is small. For situations where zero variances are implausible a priori, we propose a maximum penalized likelihood approach to avoid such boundary estimates. This approach is equivalent to estimating variance parameters by their posterior mode, given a weakly informative prior distribution. By choosing the penalty from the log-gamma family with shape parameter greater than 1, we ensure that the estimated variance will be positive. We suggest a default log-gamma(2,λ) penalty with λ→0, which ensures that the maximum penalized likelihood estimate is approximately one standard error from zero when the maximum likelihood estimate is zero, thus remaining consistent with the data while being nondegenerate. We also show that the maximum penalized likelihood estimator with this default penalty is a good approximation to the posterior median obtained under a noninformative prior.},
	author = {Chung, Yeojin and Rabe-Hesketh, Sophia and Dorie, Vincent and Gelman, Andrew and Liu, Jingchen},
	date = {2013/10/01},
	date-added = {2024-06-12 15:07:41 +0900},
	date-modified = {2024-06-12 15:07:41 +0900},
	doi = {10.1007/s11336-013-9328-2},
	id = {Chung2013},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {685--709},
	title = {A Nondegenerate Penalized Likelihood Estimator for Variance Parameters in Multilevel Models},
	url = {https://doi.org/10.1007/s11336-013-9328-2},
	volume = {78},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s11336-013-9328-2}}

@article{Harville1977,
	author = {David A. Harville},
	doi = {10.1080/01621459.1977.10480998},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1977.10480998},
	journal = {Journal of the American Statistical Association},
	number = {358},
	pages = {320--338},
	publisher = {Taylor \& Francis},
	title = {Maximum Likelihood Approaches to Variance Component Estimation and to Related Problems},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10480998},
	volume = {72},
	year = {1977},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1977.10480998},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1977.10480998}}
@article{Laird-Ware1982,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2529876},
 abstract = {Models for the analysis of longitudinal data must recognize the relationship between serial observations on the same unit. Multivariate models with general covariance structure are often difficult to apply to highly unbalanced data, whereas two-stage random-effects models can be used easily. In two-stage models, the probability distributions for the response vectors of different individuals belong to a single family, but some random-effects parameters vary across individuals, with a distribution specified at the second stage. A general family of models is discussed, which includes both growth models and repeated-measures models as special cases. A unified approach to fitting these models, based on a combination of empirical Bayes and maximum likelihood estimation of model parameters and using the EM algorithm, is discussed. Two examples are taken from a current epidemiological study of the health effects of air pollution.},
 author = {Nan M. Laird and James H. Ware},
 journal = {Biometrics},
 number = {4},
 pages = {963--974},
 publisher = {[Wiley, International Biometric Society]},
 title = {Random-Effects Models for Longitudinal Data},
 urldate = {2024-06-12},
 volume = {38},
 year = {1982}
}

@article{Leppik+1987,
	abstract = {The results of a multicenter, double-blind, placebo-controlled clinical trial of the efficacy and safety of progabide (PGB) in the treatment of partial seizures are presented. This study was performed with a number of rigorous controls not usually present in clinical trials. These included uniform co-medication in which all patients received only phenytoin and carbamazepine; concentrations of these two drugs were maintained within narrow, predefined concentration ranges. There was no statistically significant difference between PGB and placebo in seizure frequency and seizure duration for most of the analyses performed. One patient was withdrawn from the study because of hepatotoxicity. PGB was associated with a significant inhibition of phenytoin but not carbamazepine clearance. The results of this study indicate that PGB was not a potent antiepileptic drug in this population of persons with intractable epilepsy.},
	author = {I. E. Leppik and F. E. Dreifuss and R. Porter and T. Bowman and N. Santilli and M. Jacobs and C. Crosby and J. Cloyd and J. Stackman and N. Graves and T. Sutula and T. Welty and J. Vickery and R. Brundage and J. Gates and R. J. Gumnit and A. Gutierrez},
	doi = {10.1212/WNL.37.6.963},
	eprint = {https://www.neurology.org/doi/pdf/10.1212/WNL.37.6.963},
	journal = {Neurology},
	number = {6},
	pages = {963-963},
	title = {A controlled study of progabide in partial seizures},
	url = {https://www.neurology.org/doi/abs/10.1212/WNL.37.6.963},
	volume = {37},
	year = {1987},
	bdsk-url-1 = {https://www.neurology.org/doi/abs/10.1212/WNL.37.6.963},
	bdsk-url-2 = {https://doi.org/10.1212/WNL.37.6.963}}
@article{Liang-Zeger1986,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2336267},
 abstract = {This paper proposes an extension of generalized linear models to the analysis of longitudinal data. We introduce a class of estimating equations that give consistent estimates of the regression parameters and of their variance under mild assumptions about the time dependence. The estimating equations are derived without specifying the joint distribution of a subject's observations yet they reduce to the score equations for multivariate Gaussian outcomes. Asymptotic theory is presented for the general class of estimators. Specific cases in which we assume independence, m-dependence and exchangeable correlation structures from each subject are discussed. Efficiency of the proposed estimators in two simple situations is considered. The approach is closely related to quasi-likelihood.},
 author = {Kung-Yee Liang and Scott L. Zeger},
 journal = {Biometrika},
 number = {1},
 pages = {13--22},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Longitudinal Data Analysis Using Generalized Linear Models},
 urldate = {2024-06-12},
 volume = {73},
 year = {1986}
}

@article{Hubbard+2010,
	author = {Hubbard, Alan E. and Ahern, Jennifer and Fleischer, Nancy L. and Laan, Mark Van der and Lippman, Sheri A. and Jewell, Nicholas and Bruckner, Tim and Satariano, William A.},
	date-added = {2024-06-12 20:16:33 +0900},
	date-modified = {2024-06-12 20:16:33 +0900},
	id = {00001648-201007000-00007},
	isbn = {1044-3983},
	journal = {Epidemiology},
	n2 = {Abstract:  Two modeling approaches are commonly used to estimate the associations between neighborhood characteristics and individual-level health outcomes in multilevel studies (subjects within neighborhoods). Random effects models (or mixed models) use maximum likelihood estimation. Population average models typically use a generalized estimating equation (GEE) approach. These methods are used in place of basic regression approaches because the health of residents in the same neighborhood may be correlated, thus violating independence assumptions made by traditional regression procedures. This violation is particularly relevant to estimates of the variability of estimates. Though the literature appears to favor the mixed-model approach, little theoretical guidance has been offered to justify this choice. In this paper, we review the assumptions behind the estimates and inference provided by these 2 approaches. We propose a perspective that treats regression models for what they are in most circumstances: reasonable approximations of some true underlying relationship. We argue in general that mixed models involve unverifiable assumptions on the data-generating distribution, which lead to potentially misleading estimates and biased inference. We conclude that the estimation-equation approach of population average models provides a more useful approximation of the truth.},
	number = {4},
	title = {To GEE or Not to GEE: Comparing Population Average and Mixed Models for Estimating the Associations Between Neighborhood Risk Factors and Health},
	url = {https://journals.lww.com/epidem/fulltext/2010/07000/to_gee_or_not_to_gee__comparing_population_average.7.aspx},
	volume = {21},
	year = {2010},
	bdsk-url-1 = {https://journals.lww.com/epidem/fulltext/2010/07000/to_gee_or_not_to_gee__comparing_population_average.7.aspx}}
@article{Gardiner+2009,
author = {Gardiner, Joseph C. and Luo, Zhehui and Roman, Lee Anne},
title = {Fixed effects, random effects and GEE: What are the differences?},
journal = {Statistics in Medicine},
volume = {28},
number = {2},
pages = {221-239},
keywords = {linear mixed model, generalized linear mixed model, random effects, fixed effects, robust variance, conditional maximum likelihood, Hausman test, CES-D},
doi = {https://doi.org/10.1002/sim.3478},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.3478},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.3478},
abstract = {Abstract For analyses of longitudinal repeated-measures data, statistical methods include the random effects model, fixed effects model and the method of generalized estimating equations. We examine the assumptions that underlie these approaches to assessing covariate effects on the mean of a continuous, dichotomous or count outcome. Access to statistical software to implement these models has led to widespread application in numerous disciplines. However, careful consideration should be paid to their critical assumptions to ascertain which model might be appropriate in a given setting. To illustrate similarities and differences that might exist in empirical results, we use a study that assessed depressive symptoms in low-income pregnant women using a structured instrument with up to five assessments that spanned the pre-natal and post-natal periods. Understanding the conceptual differences between the methods is important in their proper application even though empirically they might not differ substantively. The choice of model in specific applications would depend on the relevant questions being addressed, which in turn informs the type of design and data collection that would be relevant. Copyright © 2008 John Wiley \& Sons, Ltd.},
year = {2009}
}
@article{Vaida-Blanchard2005,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/20441193},
 abstract = {This paper focuses on the Akaike information criterion, AIC, for linear mixed-effects models in the analysis of clustered data. We make the distinction between questions regarding the population and questions regarding the particular clusters in the data. We show that the AIC in current use is not appropriate for the focus on clusters, and we propose instead the conditional Akaike information and its corresponding criterion, the conditional AIC, cAIC. The penalty term in cAIC is related to the effective degrees of freedom ρ for a linear mixed model proposed by Hodges & Sargent (2001); ρ reflects an intermediate level of complexity between a fixed-effects model with no cluster effect and a corresponding model with fixed cluster effects. The cAIC is defined for both maximum likelihood and residual maximum likelihood estimation. A pharmacokinetics data application is used to illuminate the distinction between the two inference settings, and to illustrate the use of the conditional AIC in model selection.},
 author = {Florin Vaida and Suzette Blanchard},
 journal = {Biometrika},
 number = {2},
 pages = {351--370},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Conditional Akaike Information for Mixed-Effects Models},
 urldate = {2024-06-12},
 volume = {92},
 year = {2005}
}
@article{Bulmer1974,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2529621},
 abstract = {An extension of MacArthur's "broken stick" model is proposed to explain why species abundances should be lognormally distributed. A method of fitting the compound Poisson lognormal distribution by maximum likelihood is described; a computer program is available for performing the calculations. It is shown how the information theory measure of species diversity can be estimated from the parameters of the fitted distribution.},
 author = {M. G. Bulmer},
 journal = {Biometrics},
 number = {1},
 pages = {101--110},
 publisher = {[Wiley, International Biometric Society]},
 title = {On Fitting the Poisson Lognormal Distribution to Species-Abundance Data},
 urldate = {2024-06-16},
 volume = {30},
 year = {1974}
}

@book{Iacus-Yoshida2018,
    author         = {Stefano M. Iacus and Nakahiro Yoshida},
    year           = {2018},
    title          = {Simulation and Inference for Stochastic Processes with YUIMA: A Comprehensive R Framework for SDEs and Other Stochastic Processes},
    series         = {Use R!},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-3-319-55569-0},
    publisher      = {Springer Cham}
}

@book{Abraham+1988,
    author         = {Ralph Abraham and Jerrold E. Marsden and Tudor Ratiu},
    year           = {1988},
    title          = {Manifolds, Tensor Analysis, and Applications},
    series         = {Applied Mathematical Sciences},
    volume         = {75},
    edition        = {2},
    url            = {https://doi.org/10.1007/978-1-4612-1029-0},
    publisher      = {Springer New York}
}@article{Faulkner-Livingstone2024,
author = {Michael F. Faulkner and Samuel Livingstone},
title = {{Sampling Algorithms in Statistical Physics: A Guide for Statistics and Machine Learning}},
volume = {39},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {137 -- 164},
keywords = {event chain Monte Carlo, Glauber dynamics, hard-disk model, hybrid Monte Carlo, Ising model, Langevin dynamics, Markov chain Monte Carlo, Metropolis, molecular dynamics, molecular simulation, Potts model, sampling algorithms, statistical physics, XY model},
year = {2024},
doi = {10.1214/23-STS893},
URL = {https://doi.org/10.1214/23-STS893}
}


@article{Guidotti2022,
 title={calculus: High-Dimensional Numerical and Symbolic Calculus in R},
 volume={104},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v104i05},
 doi={10.18637/jss.v104.i05},
 abstract={&amp;lt;p&amp;gt;The R package calculus implements C++-optimized functions for numerical and symbolic calculus, such as the Einstein summing convention, fast computation of the LeviCivita symbol and generalized Kronecker delta, Taylor series expansion, multivariate Hermite polynomials, high-order derivatives, ordinary differential equations, differential operators and numerical integration in arbitrary orthogonal coordinate systems. The library applies numerical methods when working with functions, or symbolic programming when working with characters or expressions. The package handles multivariate numerical calculus in arbitrary dimensions and coordinates. It implements the symbolic counterpart of the numerical methods whenever possible, without depending on external computer algebra systems. Except for Rcpp, the package has no strict dependencies in order to provide a stable self-contained toolbox that invites re-use.&amp;lt;/p&amp;gt;},
 number={5},
 journal={Journal of Statistical Software},
 author={Guidotti, Emanuele},
 year={2022},
 pages={1–37}
}
@misc{Storopoli2021,
  author = {Storopoli, Jose},
  title = {Bayesian Statistics with Julia and Turing},
  url = {https://storopoli.io/Bayesian-Julia},
  year = {2021}
}

@unpublished{Krzakala-Zdeborova2024,
    author = {Florent Krazakala and Lenka Zdeborová},
    year   = {2024},
    title  = {Statistical Physics Methods in Optimization and Machine Learning},
    url    = {https://sphinxteam.github.io/EPFLDoctoralLecture2021/Notes.pdf}
}

@techreport{Stein1972,
    author      = {Charles Stein},
    institution = {Stanford University},
    title       = {{A Bound for the Error in the Normal Approximation to the Distribution of a Sum of Dependent Random Variables}},
    year        = {1972},
    url         = {https://purl.stanford.edu/jc818sv7483},
}

@article{西森1980,
doi = {10.1088/0022-3719/13/21/012},
url = {https://dx.doi.org/10.1088/0022-3719/13/21/012},
year = {1980},
month = {jul},
publisher = {},
volume = {13},
number = {21},
pages = {4071},
author = {H Nishimori},
title = {Exact results and critical properties of the Ising model with competing interactions},
journal = {Journal of Physics C: Solid State Physics},
abstract = {The random Ising model with competing interactions is investigated on the basis of the gauge-invariant formulation of the problem. Exact results for the internal energy, specific heat and gauge-invariant correlation function are derived. The critical exponent alpha is shown to be negative at the phase boundary of the paramagnetic and ferromagnetic phases if the latter exists at fairly low concentration of antiferromagnetic bonds.}
}
@article{Derrida1980,
  title = {Random-Energy Model: Limit of a Family of Disordered Models},
  author = {Derrida, B.},
  journal = {Phys. Rev. Lett.},
  volume = {45},
  issue = {2},
  pages = {79--82},
  numpages = {0},
  year = {1980},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.45.79},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.45.79}
}
@ARTICLE{Guo+2005,
  author={Dongning Guo and Shamai, S. and Verdu, S.},
  journal={IEEE Transactions on Information Theory}, 
  title={Mutual information and minimum mean-square error in Gaussian channels}, 
  year={2005},
  volume={51},
  number={4},
  pages={1261-1282},
  keywords={Mutual information;Gaussian channels;Signal to noise ratio;Additive noise;Gaussian noise;Network address translation;Statistics;Power filters;Filtering;Smoothing methods;Gaussian channel;minimum mean-square error (MMSE);mutual information;nonlinear filtering;optimal estimation;smoothing;Wiener process},
  doi={10.1109/TIT.2005.844072}}

@article{樺島祥介2003,
    author          = {樺島祥介},
    year            = {2003},
    title           = {コトの物理学：誤り訂正符号を例として},
    journal         = {日本物理学会誌},
    volume          = {58},
    number          = {4},
    pages           = {239-246},
    url             = {https://doi.org/10.11316/butsuri1946.58.239}
}
@article{樺島-杉浦2008,
    author          = {樺島祥介 and 杉浦正康},
    year            = {2008},
    title           = {コトの物理学},
    journal         = {物性研究},
    volume          = {91},
    number          = {1},
    pages           = {1-33},
    url             = {http://hdl.handle.net/2433/142666}
}
@article{Sourlas1989,
	abstract = {DURING the transmission of information, errors may occur because of the presence of noise, such as thermal noise in electronic signals or interference with other sources of radiation. One wants to recover the information with the minimum error possible. In theory this is possible by increasing the power of the emitter source. But as the cost is proportional to the energy fed into the channel, it costs less to code the message before sending it, thus including redundant 'coding' bits, and to decode at the end. Coding theory provides rigorous bounds on the cost-effectiveness of any code. The explicit codes proposed so far for practical applications do not saturate these bounds; that is, they do not achieve optimal cost-efficiency. Here we show that theoretical models of magnetically disordered materials (spin glasses) provide a new class of error-correction codes. Their cost performance can be calculated using the methods of statistical mechanics, and is found to be excellent. These models can, under certain circumstances, constitute the first known codes to saturate Shannon's well-known cost-performance bounds.},
	author = {Sourlas, Nicolas},
	date = {1989/06/01},
	date-added = {2024-06-23 19:34:20 +0900},
	date-modified = {2024-06-23 19:34:20 +0900},
	doi = {10.1038/339693a0},
	id = {Sourlas1989},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6227},
	pages = {693--695},
	title = {Spin-glass models as error-correcting codes},
	url = {https://doi.org/10.1038/339693a0},
	volume = {339},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1038/339693a0}}
@book{西森秀稔1999,
    author         = {西森秀稔},
    year           = {1999},
    month          = {11},
    title          = {スピングラス理論と情報統計力学},
    series         = {新物理学選書},
    volume         = {},
    edition        = {},
    url            = {https://www.iwanami.co.jp/book/b259758.html},
    publisher      = {岩波書店}
}
@book{西森秀稔2005,
    author         = {西森秀稔},
    title          = {相転移・臨界現象の統計物理学},
    year           = {2005},
    month          = {11},
    publisher      = {培風館},
    series         = {新物理学シリーズ},
    volume         = {35},
    url             = {https://www.kyoritsu-pub.co.jp/book/b10003637.html},
}

@book{横尾英俊2004,
    author         = {横尾英俊},
    year           = {2004},
    title          = {情報理論の基礎},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.kyoritsu-pub.co.jp/book/b10010336.html},
    publisher      = {共立出版}
}

@phdthesis{Gallager1960,
    author      = {Robert G. Gallager},
    school      = {MIT},
    title       = {{Low Density Parity Check Codes}},
    year        = {1960},
    url         = {http://hdl.handle.net/1721.1/11804},
}

@unpublished{Krzakala+2015,
    author = {Florent Krzakala and Lenka Zdeborova and Maria Chiara Angelini and Francesco Caltagirone},
    year   = {2015},
    title  = {Statistical Physics of Inference and Bayesian Estimation},
    url    = {https://indico.ictp.it/event/a14244/material/10/0.pdf},
    note   = {Lecture Notes in Spring College on the Physics of Complex Systems. Trieste, Italy}
}


@article{Zdeborova-Krzakala2016,
	author = {Lenka Zdeborov{\'a} and Florent Krzakala},
	doi = {10.1080/00018732.2016.1211393},
	eprint = {https://doi.org/10.1080/00018732.2016.1211393},
	journal = {Advances in Physics},
	number = {5},
	pages = {453--552},
	publisher = {Taylor \& Francis},
	title = {Statistical physics of inference: thresholds and algorithms},
	url = {https://doi.org/10.1080/00018732.2016.1211393},
	volume = {65},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1080/00018732.2016.1211393}}
@article{Donoho-Johnstone1994,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2337118},
 abstract = {With ideal spatial adaptation, an oracle furnishes information about how best to adapt a spatially variable estimator, whether piecewise constant, piecewise polynomial, variable knot spline, or variable bandwidth kernel, to the unknown function. Estimation with the aid of an oracle offers dramatic advantages over traditional linear estimation by nonadaptive kernels; however, it is a priori unclear whether such performance can be obtained by a procedure relying on the data alone. We describe a new principle for spatially-adaptive estimation: selective wavelet reconstruction. We show that variable-knot spline fits and piecewise-polynomial fits, when equipped with an oracle to select the knots, are not dramatically more powerful than selective wavelet reconstruction with an oracle. We develop a practical spatially adaptive method, RiskShrink, which works by shrinkage of empirical wavelet coefficients. RiskShrink used in connection with sample rotation. Inclusion probabilities of any order can be written explicitly in closed form. Second-order inclusion probabilities πij satisfy the condition $0 < \pi_{ij} < \pi_{i}\pi_j$, which guarantees Yates & Grundy's variance estimator to be unbiased, definable for all samples and always nonnegative for any sample size.},
 author = {David L. Donoho and Iain M. Johnstone},
 journal = {Biometrika},
 number = {3},
 pages = {425--455},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Ideal Spatial Adaptation by Wavelet Shrinkage},
 urldate = {2024-06-24},
 volume = {81},
 year = {1994}
}
@article{Jaynes1957,
  title = {Information Theory and Statistical Mechanics},
  author = {Jaynes, E. T.},
  journal = {Phys. Rev.},
  volume = {106},
  issue = {4},
  pages = {620--630},
  numpages = {0},
  year = {1957},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.106.620},
  url = {https://link.aps.org/doi/10.1103/PhysRev.106.620}
}
@book{Nishimori2001,
    author         = {Hidetoshi Nishimori},
    year           = {2001},
    title          = {Statistical Physics of Spin Glasses and Information Processing: An Introduction},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1093/acprof:oso/9780198509417.001.0001},
    publisher      = {Oxford University Press}
}

@article{Bryngelson-Wolynes1987,
	abstract = {The theory of spin glasses was used to study a simple model of protein folding. The phase diagram of the model was calculated, and the results of dynamics calculations are briefly reported. The relation of these results to folding experiments, the relation of these hypotheses to previous protein folding theories, and the implication of these hypotheses for protein folding prediction schemes are discussed.},
	author = {J D Bryngelson and P G Wolynes},
	doi = {10.1073/pnas.84.21.7524},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.84.21.7524},
	journal = {Proceedings of the National Academy of Sciences},
	number = {21},
	pages = {7524-7528},
	title = {Spin glasses and the statistical mechanics of protein folding.},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.84.21.7524},
	volume = {84},
	year = {1987},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.84.21.7524},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.84.21.7524}}

@article{Jerrum1992,
	author = {Jerrum, Mark},
	journal = {Random Structures \& Algorithms},
	number = {4},
	pages = {347-359},
	title = {Large Cliques Elude the Metropolis Process},
	volume = {3},
	year = {1992}}
@INPROCEEDINGS{Jerrum-Sorkin1993,
  author={Jerrum, M. and Sorkin, G.B.},
  booktitle={Proceedings of 1993 IEEE 34th Annual Foundations of Computer Science}, 
  title={Simulated annealing for graph bisection}, 
  year={1993},
  volume={},
  number={},
  pages={94-103},
  keywords={Simulated annealing;Energy states;Computational modeling;Computer science;Temperature distribution;Stochastic processes;Polynomials;Costs;Temperature control;Power engineering and energy},
  doi={10.1109/SFCS.1993.366878}}
@INPROCEEDINGS{Achlioptas-Coja-Oghlan2008,
  author={Achlioptas, Dimitris and Coja-Oghlan, Amin},
  booktitle={2008 49th Annual IEEE Symposium on Foundations of Computer Science}, 
  title={Algorithmic Barriers from Phase Transitions}, 
  year={2008},
  volume={},
  number={},
  pages={793-802},
  keywords={Polynomials;Upper bound;Computer science;Phase estimation;Resilience;Geometry;Error correction codes;Physics;Injuries;Moment methods;Phase Transitions;Random Constraint Satisfaction Problems;Algorithms},
  doi={10.1109/FOCS.2008.11}}

@article{Bethe1935,
	abstract = { In a recent paper, Bragg and Williams have pointed out that the arrangement of the atoms in an alloy depends in a striking way on the temperature. At high temperatures, the atoms are distributed practically at random among the lattice points of the crystal, but at low temperatures a superlattice may be formed such that the atoms of one kind are arranged in a regular lattice of their own and the atoms of the other kind occupy the remaining ``sites'' in the crystal. The transition from the ordered to the disordered state occurs in a fairly small temperature range, and is accompanied by a large specific heat, an increase in electric resistance, etc. The mathematical method employed by Bragg and Williams is similar to that used in Weiss's theory of ferromagnetism. Both involve the assumption that the ``force'' tending to produce order at a given point is uniquely determined by the average state of order throughout the crystal. Actually it will depend on the configuration of the atoms in the immediate neighbourhood of the point under consideration. The order of the crystal as a whole determines this configuration only on the average. In the present paper, the effect of fluctuations in configuration, which was neglected by Bragg and Williams, will be taken into account. },
	author = {Bethe, H. A. and Bragg, William Lawrence},
	doi = {10.1098/rspa.1935.0122},
	eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.1935.0122},
	journal = {Proceedings of the Royal Society of London. Series A - Mathematical and Physical Sciences},
	number = {871},
	pages = {552-575},
	title = {Statistical theory of superlattices},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1935.0122},
	volume = {150},
	year = {1935},
	bdsk-url-1 = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1935.0122},
	bdsk-url-2 = {https://doi.org/10.1098/rspa.1935.0122}}


@article{Peierls+1936,
	abstract = { Recently, Bragg and Williams have given an explanation of the variation of the degree of order in an alloy with temperature. According to this explanation it is essential to consider the ordering as a co-operative phenomenon characteristic for large assemblies of atoms. The force, V, tending to produce order, is, according to this view, a function of the degree of order, because in order to remove an atom from a ``right'' to a ``wrong'' position one requires more energy the greater the number of ``right'' atoms in the neighbourhood, i. e., the greater the degree of order. One is thus led to consider two functions: V (s), describing the dependence of ordering force on order, and s (V), the order produced by a given ordering force, at given temperature. If both these functions are known, they implicitly define the degree of order as a function of the temperature. },
	author = {Peierls, R. and Bragg, William Lawrence},
	doi = {10.1098/rspa.1936.0047},
	eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rspa.1936.0047},
	journal = {Proceedings of the Royal Society of London. Series A - Mathematical and Physical Sciences},
	number = {881},
	pages = {207-222},
	title = {Statistical theory of superlattices with unequal concentrations of the components},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1936.0047},
	volume = {154},
	year = {1936},
	bdsk-url-1 = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1936.0047},
	bdsk-url-2 = {https://doi.org/10.1098/rspa.1936.0047}}
@article{Plefka1982,
doi = {10.1088/0305-4470/15/6/035},
url = {https://dx.doi.org/10.1088/0305-4470/15/6/035},
year = {1982},
month = {jun},
publisher = {},
volume = {15},
number = {6},
pages = {1971},
author = {T Plefka},
title = {Convergence condition of the TAP equation for the infinite-ranged Ising spin glass model},
journal = {Journal of Physics A: Mathematical and General},
abstract = {It is shown that the power expansion of the Gibbs potential of the SK model up to second order in the exchange couplings leads to the TAP equation. This result remains valid for the general (including a ferromagnetic exchange) SK model. Theorems of power expansions and resolvent techniques are employed to solve the convergence problem. The convergence condition is presented for the whole temperature range and for general distributions of the local magnetisations.}
}
@article{Georges-Yedidia1991,
doi = {10.1088/0305-4470/24/9/024},
url = {https://dx.doi.org/10.1088/0305-4470/24/9/024},
year = {1991},
month = {may},
publisher = {},
volume = {24},
number = {9},
pages = {2173},
author = {A Georges and  J S Yedidia},
title = {How to expand around mean-field theory using high-temperature expansions},
journal = {Journal of Physics A: Mathematical and General},
abstract = {High-temperature expansions performed at a fixed-order parameter provide a simple and systematic way to derive and correct mean-field theories for statistical mechanical models. For models like spin glasses which have general couplings between spins, the authors show that these expansions generate the Thouless-Anderson-Palmer equations at low order. They explicitly calculate the corrections to TAP theory for these models. For ferromagnetic models, they show that their expansions can easily be converted into 1/d expansions around mean-field theory, where d is the number of spatial dimensions. Only a small finite number of graphs need to be calculated to generate each order in 1/d for thermodynamic quantities like free energy or magnetization. Unlike previous 1/d expansions, the expansions are valid in the low-temperature phases of the models considered. They consider alternative ways to expand around mean-field theory besides 1/d expansions. In contrast to the 1/d expansion for the critical temperature, which is presumably asymptotic, these schemes can be used to devise convergent expansions for the critical temperature. They also appear to give convergent series for thermodynamic quantities and critical exponents. They test the schemes using the spherical model, where their properties can be studied using exact expressions.}
}
@article{Kikuchi1951,
  title = {A Theory of Cooperative Phenomena},
  author = {Kikuchi, Ryoichi},
  journal = {Phys. Rev.},
  volume = {81},
  issue = {6},
  pages = {988--1003},
  numpages = {0},
  year = {1951},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.81.988},
  url = {https://link.aps.org/doi/10.1103/PhysRev.81.988}
}
@ARTICLE{Gallager1962,
  author={Gallager, R.},
  journal={IRE Transactions on Information Theory}, 
  title={Low-density parity-check codes}, 
  year={1962},
  volume={8},
  number={1},
  pages={21-28},
  keywords={Parity check codes;Maximum likelihood decoding;Equations;Channel capacity;Information theory;Error probability;Linear approximation;Data communication;Error correction codes;Communication systems},
  doi={10.1109/TIT.1962.1057683}}
@proceedings{Pearl1982,
    editor          = {},
    author          = {Judea Pearl},
    published       = {},
    series          = {Proceedings of the AAAI Conference on Artificial Intelligence},
    title           = {{Reverend Bayes on Inference Engines: A Distributed Hierarchical Approach}},
    volume          = {2},
    year            = {1982},
    pages           = {133},
    url             = {https://aaai.org/papers/00133-aaai82-032-reverend-bayes-on-inference-engines-a-distributed-hierarchical-approach/},
}
@inbook{Yedidia+2003,
author = {Yedidia, Jonathan S. and Freeman, William T. and Weiss, Yair},
title = {Understanding belief propagation and its generalizations},
year = {2003},
isbn = {1558608117},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {"Inference" problems arise in statistical physics, computer vision, error-correcting coding theory, and AI. We explain the principles behind the belief propagation (BP) algorithm, which is an efficient way to solve inference problems based on passing local messages. We develop a unified approach, with examples, notation, and graphical models borrowed from the relevant disciplines.We explain the close connection between the BP algorithm and the Bethe approximation of statistical physics. In particular, we show that BP can only converge to a fixed point that is also a stationary point of the Bethe approximation to the free energy. This result helps explaining the successes of the BP algorithm and enables connections to be made with variational approaches to approximate inference.The connection of BP with the Bethe approximation also suggests a way to construct new message-passing algorithms based on improvements to Bethe's approximation introduced Kikuchi and others. The new generalized belief propagation (GBP) algorithms are significantly more accurate than ordinary BP for some problems. We illustrate how to construct GBP algorithms with a detailed example.},
booktitle = {Exploring Artificial Intelligence in the New Millennium},
pages = {239–269},
numpages = {31}
}
@ARTICLE{Abbe+2014,
  author={Abbe, Emmanuel and Bandeira, Afonso S. and Bracher, Annina and Singer, Amit},
  journal={IEEE Transactions on Network Science and Engineering}, 
  title={Decoding Binary Node Labels from Censored Edge Measurements: Phase Transition and Efficient Recovery}, 
  year={2014},
  volume={1},
  number={1},
  pages={10-22},
  keywords={Noise measurement;Clustering;Decoding;Graph theory;Image edge detection;Inverse problems;Synchronization;information theory;Synchronization problem;Information theoretic bounds;Erdos-Renyi graphs;Semidefinite relaxations;graphbased codes;Synchronization problem;Information theoretic bounds;Stochastic block model;Semidefinite relaxations;graph-based codes},
  doi={10.1109/TNSE.2014.2368716}}

@article{Mattis1976,
	abstract = {It is shown that if the bonds connecting spins on a lattice are separable functions of random variables, the thermodynamic and magnetic parameters may be obtained using the known properties of a spin system with non-random bonds.},
	author = {D.C. Mattis},
	doi = {https://doi.org/10.1016/0375-9601(76)90396-0},
	issn = {0375-9601},
	journal = {Physics Letters A},
	number = {5},
	pages = {421-422},
	title = {Solvable spin systems with random interactions},
	url = {https://www.sciencedirect.com/science/article/pii/0375960176903960},
	volume = {56},
	year = {1976},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0375960176903960},
	bdsk-url-2 = {https://doi.org/10.1016/0375-9601(76)90396-0}}
@article{Nishimori1980,
doi = {10.1088/0022-3719/13/21/012},
url = {https://dx.doi.org/10.1088/0022-3719/13/21/012},
year = {1980},
month = {jul},
publisher = {},
volume = {13},
number = {21},
pages = {4071},
author = {H Nishimori},
title = {Exact results and critical properties of the Ising model with competing interactions},
journal = {Journal of Physics C: Solid State Physics},
abstract = {The random Ising model with competing interactions is investigated on the basis of the gauge-invariant formulation of the problem. Exact results for the internal energy, specific heat and gauge-invariant correlation function are derived. The critical exponent alpha is shown to be negative at the phase boundary of the paramagnetic and ferromagnetic phases if the latter exists at fairly low concentration of antiferromagnetic bonds.}
}

@article{Mezard+2002,
	abstract = {We study the satisfiability of random Boolean expressions built from many clauses with K variables per clause (K-satisfiability). Expressions with a ratio α of clauses to variables less than a threshold αc are almost always satisfiable, whereas those with a ratio above this threshold are almost always unsatisfiable. We show the existence of an intermediate phase below αc, where the proliferation of metastable states is responsible for the onset of complexity in search algorithms. We introduce a class of optimization algorithms that can deal with these metastable states; one such algorithm has been tested successfully on the largest existing benchmark of K-satisfiability.},
	author = {M. M{\'e}zard and G. Parisi and R. Zecchina},
	doi = {10.1126/science.1073287},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.1073287},
	journal = {Science},
	number = {5582},
	pages = {812-815},
	title = {Analytic and Algorithmic Solution of Random Satisfiability Problems},
	url = {https://www.science.org/doi/abs/10.1126/science.1073287},
	volume = {297},
	year = {2002},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.1073287},
	bdsk-url-2 = {https://doi.org/10.1126/science.1073287}}

@article{Kabashima2005,
	abstract = { A scheme to provide various mean-field-type approximation algorithms is presented by employing the Bethe free energy formalism to a family of replicated systems in conjunction with analytical continuation with respect to the number of replicas. In the scheme, survey propagation (SP), which is an efficient algorithm developed recently for analyzing the microscopic properties of glassy states for a fixed sample of disordered systems, can be reproduced by assuming the simplest replica symmetry on stationary points of the replicated Bethe free energy. Belief propagation and generalized SP can also be offered in the identical framework under assumptions of the highest and broken replica symmetries, respectively. },
	author = {Kabashima ,Yoshiyuki},
	doi = {10.1143/JPSJ.74.2133},
	eprint = {https://doi.org/10.1143/JPSJ.74.2133},
	journal = {Journal of the Physical Society of Japan},
	number = {8},
	pages = {2133-2136},
	title = {Replicated Bethe Free Energy: A Variational Principle behind Survey Propagation},
	url = {https://doi.org/10.1143/JPSJ.74.2133},
	volume = {74},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1143/JPSJ.74.2133}}

@book{Talagrand2011,
    author         = {Michel Talagrand},
    year           = {2011},
    title          = {{Mean Field Models for Spin Glasses. Volume I: Basic Examples}},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-3-642-15202-3},
    publisher      = {Springer Berlin, Heidelberg}
}
@article{Kosterlitz-Thouless1973,
doi = {10.1088/0022-3719/6/7/010},
url = {https://dx.doi.org/10.1088/0022-3719/6/7/010},
year = {1973},
month = {apr},
publisher = {},
volume = {6},
number = {7},
pages = {1181},
author = {J M Kosterlitz and  D J Thouless},
title = {Ordering, metastability and phase transitions in two-dimensional systems},
journal = {Journal of Physics C: Solid State Physics},
abstract = {A new definition of order called topological order is proposed for two-dimensional systems in which no long-range order of the conventional type exists. The possibility of a phase transition characterized by a change in the response of the system to an external perturbation is discussed in the context of a mean field type of approximation. The critical behaviour found in this model displays very weak singularities. The application of these ideas to the xy model of magnetism, the solid-liquid transition, and the neutral superfluid are discussed. This type of phase transition cannot occur in a superconductor nor in a Heisenberg ferromagnet.}
}
@article{Thouless1986,
  title = {{Spin-Glass on a Bethe Lattice}},
  author = {Thouless, D. J.},
  journal = {Phys. Rev. Lett.},
  volume = {56},
  issue = {10},
  pages = {1082--1085},
  numpages = {0},
  year = {1986},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.56.1082},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.56.1082}
}

@article{Onsager1944,
  title = {Crystal Statistics. I. A Two-Dimensional Model with an Order-Disorder Transition},
  author = {Onsager, Lars},
  journal = {Phys. Rev.},
  volume = {65},
  issue = {3-4},
  pages = {117--149},
  numpages = {0},
  year = {1944},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.65.117},
  url = {https://link.aps.org/doi/10.1103/PhysRev.65.117}
}
@inproceedings{Husimi1954,
    author          = {K. Husimi},
    year            = {1954},
    title           = {Statistical Mechanics of Condensation},
    booktitle       = {Proceedings of the International Conference of Theoretical Physics. Science Council of Japan},
    editor          = {Hideki Yukawa},
    volume          = {},
    pages           = {},
    url             = {}
}
@article{Temperley1954,
doi = {10.1088/0370-1298/67/3/306},
url = {https://dx.doi.org/10.1088/0370-1298/67/3/306},
year = {1954},
month = {mar},
publisher = {},
volume = {67},
number = {3},
pages = {233},
author = {H N V Temperley},
title = {The Mayer Theory of Condensation Tested Against a Simple Model of the Imperfect Gas},
journal = {Proceedings of the Physical Society. Section A},
abstract = {A very simple model of an imperfect gas, analogous to the Weiss model of a ferromagnetic, is used to check various conflicting conclusions about the relationship between the condensation of an imperfect gas and the divergence of Mayer's virial series. For this model, the divergence of the virial series has no physical significance, and the interpretations of this divergence suggested by Mayer and by Born and Green are both incorrect. The model also turns out to be one for which care is necessary in going to the limit of a very large assembly.}
}
@article{Kac-Thompson1966,
 ISSN = {00278424},
 URL = {http://www.jstor.org/stable/57450},
 author = {Mark Kac and Colin J. Thompson},
 journal = {Proceedings of the National Academy of Sciences of the United States of America},
 number = {4},
 pages = {676--683},
 publisher = {National Academy of Sciences},
 title = {On the Mathematical Mechanism of Phase Transition},
 urldate = {2024-05-30},
 volume = {55},
 year = {1966}
}
@article{Potts1952, title={Some generalized order-disorder transformations}, volume={48}, DOI={10.1017/S0305004100027419}, number={1}, journal={Mathematical Proceedings of the Cambridge Philosophical Society}, author={Potts, R. B.}, year={1952}, pages={106–109}}


@article{Storath+2015,
	abstract = {We propose a new algorithmic approach to the non-smooth and non-convex Potts problem (also called piecewise-constant Mumford--Shah problem) for inverse imaging problems. We derive a suitable splitting into specific subproblems that can all be solved efficiently. Our method does not require a priori knowledge on the gray levels nor on the number of segments of the reconstruction. Further, it avoids anisotropic artifacts such as geometric staircasing. We demonstrate the suitability of our method for joint image reconstruction and segmentation. We focus on Radon data, where we in particular consider limited data situations. For instance, our method is able to recover all segments of the Shepp--Logan phantom from seven angular views only. We illustrate the practical applicability on a real positron emission tomography dataset. As further applications, we consider spherical Radon data as well as blurred data.},
	author = {Martin Storath and Andreas Weinmann and J{\"u}rgen Frikel and Michael Unser},
	doi = {10.1088/0266-5611/31/2/025003},
	journal = {Inverse Problems},
	month = {jan},
	number = {2},
	pages = {025003},
	publisher = {IOP Publishing},
	title = {Joint image reconstruction and segmentation using the Potts model},
	url = {https://dx.doi.org/10.1088/0266-5611/31/2/025003},
	volume = {31},
	year = {2015},
	bdsk-url-1 = {https://dx.doi.org/10.1088/0266-5611/31/2/025003}}
@article{Engel+2013,
  title = {Hard-disk equation of state: First-order liquid-hexatic transition in two dimensions with three simulation methods},
  author = {Engel, Michael and Anderson, Joshua A. and Glotzer, Sharon C. and Isobe, Masaharu and Bernard, Etienne P. and Krauth, Werner},
  journal = {Phys. Rev. E},
  volume = {87},
  issue = {4},
  pages = {042134},
  numpages = {8},
  year = {2013},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.87.042134},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.87.042134}
}
@article{Thorneywork2017,
  title = {Two-Dimensional Melting of Colloidal Hard Spheres},
  author = {Thorneywork, Alice L. and Abbott, Joshua L. and Aarts, Dirk G. A. L. and Dullens, Roel P. A.},
  journal = {Phys. Rev. Lett.},
  volume = {118},
  issue = {15},
  pages = {158001},
  numpages = {5},
  year = {2017},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.118.158001},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.118.158001}
}
@article{Wei+2011,
    author = {Wei, Dongshan and Song, Yang and Wang, Feng},
    title = "{A simple molecular mechanics potential for μm scale graphene simulations from the adaptive force matching method}",
    journal = {The Journal of Chemical Physics},
    volume = {134},
    number = {18},
    pages = {184704},
    year = {2011},
    month = {05},
    abstract = "{A simple molecular mechanics force field for graphene (PPBE-G) was created by force matching the density functional theory Perdew-Burke-Ernzerhof forces using the adaptive force matching method recently developed in our group. The PPBE-G potential was found to provide significantly more accurate forces than other existing force fields. Several properties of graphene, such as Young's modulus, bending rigidity, and thermal conductivity, have been studied with our potential. The calculated properties are in good agreement with corresponding density functional theory and experimental values. The thermal conductivity calculated with reverse non-equilibrium molecular dynamics depends sensitively on graphene size thus requiring the simulation of large sheets for convergence. Since the PPBE-G potential only contains simple additive energy expressions, it is very computationally efficient and is capable of modeling large graphene sheets in the μm length scale.}",
    issn = {0021-9606},
    doi = {10.1063/1.3589163},
    url = {https://doi.org/10.1063/1.3589163},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/doi/10.1063/1.3589163/14062575/184704\_1\_online.pdf},
}
@article{Wood-Parker1957,
    author = {Wood, W. W. and Parker, F. R.},
    title = "{Monte Carlo Equation of State of Molecules Interacting with the Lennard‐Jones Potential. I. A Supercritical Isotherm at about Twice the Critical Temperature}",
    journal = {The Journal of Chemical Physics},
    volume = {27},
    number = {3},
    pages = {720-733},
    year = {1957},
    month = {09},
    abstract = "{Values obtained by Monte Carlo calculations are reported for the compressibility factor, excess internal energy, excess constant‐volume heat capacity, and the radial distribution function of Lennard‐Jones (12,6) molecules at the reduced temperature kT/ε*=2.74, and at thirteen volumes between v/v*=0.75 and 7.5. (v is the molar volume; v*=2—½N0r*3; N0 is Avogadro's number; ε* is the depth, and r* the radius of the Lennard‐Jones potential well.) The results are compared with the experimental observations of Michels (∼150–2000 atmos) and Bridgman (∼2000–15 000 atmos) on argon at 55°C, using Michels' second virial coefficient values for the potential parameters. Close agreement with Michels is found, but significant disagreement with Bridgman. The Monte Carlo calculations display the fluid‐solid transition; the transition pressure and the volume and enthalpy increments are not precisely determined. The Lennard‐Jones‐Devonshire cell theory gives results which disagree throughout the fluid phase, but agree on the solid branch of the isotherm. Limited comparisons with the Kirkwood‐Born‐Green results indicate that the superposition approximation yields useful results at least up to v/v*=2.5.}",
    issn = {0021-9606},
    doi = {10.1063/1.1743822},
    url = {https://doi.org/10.1063/1.1743822},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/27/3/720/18812357/720\_1\_online.pdf},
}
@article{Alder-Wainwright1962,
  title = {Phase Transition in Elastic Disks},
  author = {Alder, B. J. and Wainwright, T. E.},
  journal = {Phys. Rev.},
  volume = {127},
  issue = {2},
  pages = {359--361},
  numpages = {0},
  year = {1962},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.127.359},
  url = {https://link.aps.org/doi/10.1103/PhysRev.127.359}
}

@article{Simanyi2003,
	abstract = {We consider the system of N (≥2) hard disks of masses m1,...,mNand radius r in the flat unit torus 𝕋2. We prove the ergodicity (actually, the B-mixing property) of such systems for almost every selection (m1,...,mN;r) of the outer geometric parameters.},
	author = {Sim{\'a}nyi, N{\'a}ndor},
	date = {2003/10/01},
	date-added = {2024-06-29 12:33:25 +0900},
	date-modified = {2024-06-29 12:33:25 +0900},
	doi = {10.1007/s00222-003-0304-9},
	id = {Sim{\'a}nyi2003},
	isbn = {1432-1297},
	journal = {Inventiones mathematicae},
	number = {1},
	pages = {123--178},
	title = {Proof of the Boltzmann-Sinai ergodic hypothesis for typical hard disk systems},
	url = {https://doi.org/10.1007/s00222-003-0304-9},
	volume = {154},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1007/s00222-003-0304-9}}

@book{Tabachnikov2005,
    author         = {Serge Tabachnikov},
    year           = {2005},
    title          = {Geometry and Billiards},
    series         = {Student Mathematical Library},
    volume         = {30},
    edition        = {},
    url            = {https://bookstore.ams.org/view?ProductCode=STML/30},
    publisher      = {American Mathematical Society}
}
@article{Bou-Rabee_Sanz-Serna2018, title={Geometric integrators and the Hamiltonian Monte Carlo method}, volume={27}, DOI={10.1017/S0962492917000101}, journal={Acta Numerica}, author={Bou-Rabee, Nawaf and Sanz-Serna, J. M.}, year={2018}, pages={113–206}}

@article{Verlet1967,
  title = {Computer "Experiments" on Classical Fluids. I. Thermodynamical Properties of Lennard-Jones Molecules},
  author = {Verlet, Loup},
  journal = {Phys. Rev.},
  volume = {159},
  issue = {1},
  pages = {98--103},
  numpages = {0},
  year = {1967},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.159.98},
  url = {https://link.aps.org/doi/10.1103/PhysRev.159.98}
}

@article{Stormer1907,
    author          = {Carl Störmer},
    year            = {1907},
    title           = {Sur les trajectoires des corpuscules électrisés dans l’espace. Applications à l’aurore boréale et aux perturbations magnétiques},
    journal         = {Radium (Paris)},
    volume          = {4},
    number          = {1},
    pages           = {2-5},
    doi             = {10.1051/ra-dium:01907004010201},
}

@book{Leimkuhler-Matthews2015,
    author         = {Ben Leimkuhler and Charles Matthews},
    year           = {2015},
    title          = {Molecular Dynamics: With Deterministic and Stochastic Numerical Methods},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-3-319-16375-8},
    publisher      = {Springer Cham}
}

@book{Pavliotis2014,
    author         = {Grigorios A. Pavliotis},
    year           = {2014},
    title          = {Stochastic Processes and Applications: Diffusion Processes, the Fokker-Planck and Langevin Equations},
    series         = {Texts in Applied Mathematics},
    volume         = {60},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-1-4939-1323-7},
    publisher      = {Springer New York}
}
@book{Leimkuhler-Reich2005, place={Cambridge}, series={Cambridge Monographs on Applied and Computational Mathematics}, title={Simulating Hamiltonian Dynamics}, publisher={Cambridge University Press}, author={Leimkuhler, Benedict and Reich, Sebastian}, year={2005}, collection={Cambridge Monographs on Applied and Computational Mathematics}}

@article{Eberle+2019,
	author = {Andreas Eberle and Arnaud Guillin and Raphael Zimmer},
	doi = {10.1214/18-AOP1299},
	journal = {The Annals of Probability},
	keywords = {Convergence to equilibrium, hypocoercivity, kinetic Fokker--Planck equation, Langevin diffusion, Lyapunov functions, quantitative bounds, Reflection coupling, stochastic Hamiltonian dynamics, Wasserstein distance},
	number = {4},
	pages = {1982 -- 2010},
	publisher = {Institute of Mathematical Statistics},
	title = {{Couplings and quantitative contraction rates for Langevin dynamics}},
	url = {https://doi.org/10.1214/18-AOP1299},
	volume = {47},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1214/18-AOP1299}}


@article{Dalalyan-Riou-Durand2020,
	author = {Arnak S. Dalalyan and Lionel Riou-Durand},
	doi = {10.3150/19-BEJ1178},
	journal = {Bernoulli},
	keywords = {Hamiltonian Monte Carlo, kinetic Langevin, Langevin algorithm, Markov chain Monte Carlo, mixing rate},
	number = {3},
	pages = {1956 -- 1988},
	publisher = {Bernoulli Society for Mathematical Statistics and Probability},
	title = {{On sampling from a log-concave density using kinetic Langevin diffusions}},
	url = {https://doi.org/10.3150/19-BEJ1178},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.3150/19-BEJ1178}}
@misc{Stoltz2021,
      title={Computational statistical physics and hypocoercivity}, 
      author={Gabriel Stoltz},
      year={2021},
      eprint={2112.08221},
      archivePrefix={arXiv},
      primaryClass={math.AP},
      url={https://arxiv.org/abs/2112.08221}, 
}
@article{Rossky+1978,
    author = {Rossky, P. J. and Doll, J. D. and Friedman, H. L.},
    title = "{Brownian dynamics as smart Monte Carlo simulation}",
    journal = {The Journal of Chemical Physics},
    volume = {69},
    number = {10},
    pages = {4628-4633},
    year = {1978},
    month = {11},
    abstract = "{A new Monte Carlo simulation procedure is developed which is expected to produce more rapid convergence than the standard Metropolis method. The trial particle moves are chosen in accord with a Brownian dynamics algorithm rather than at random. For two model systems, a string of point masses joined by harmonic springs and a cluster of charged soft spheres, the new procedure is compared to the standard one and shown to manifest a more rapid convergence rate for some important energetic and structural properties.}",
    issn = {0021-9606},
    doi = {10.1063/1.436415},
    url = {https://doi.org/10.1063/1.436415},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/69/10/4628/18914248/4628\_1\_online.pdf},
}
@article{Li+2022-HD,
    author = {Li, Botao and Nishikawa, Yoshihiko and Höllmer, Philipp and Carillo, Louis and Maggs, A. C. and Krauth, Werner},
    title = "{Hard-disk pressure computations—a historic perspective}",
    journal = {The Journal of Chemical Physics},
    volume = {157},
    number = {23},
    pages = {234111},
    year = {2022},
    month = {12},
    abstract = "{We discuss pressure computations for the hard-disk model performed since 1953 and compare them to the results that we obtain with a powerful event-chain Monte Carlo and a massively parallel Metropolis algorithm. Like other simple models in the sciences, such as the Drosophila model of biology, the hard-disk model has needed monumental efforts to be understood. In particular, we argue that the difficulty of estimating the pressure has not been fully realized in the decades-long controversy over the hard-disk phase-transition scenario. We present the physics of the hard-disk model, the definition of the pressure and its unbiased estimators, several of which are new. We further treat different sampling algorithms and crucial criteria for bounding mixing times in the absence of analytical predictions. Our definite results for the pressure, for up to one million disks, may serve as benchmarks for future sampling algorithms. A synopsis of hard-disk pressure data as well as different versions of the sampling algorithms and pressure estimators are made available in an open-source repository.}",
    issn = {0021-9606},
    doi = {10.1063/5.0126437},
    url = {https://doi.org/10.1063/5.0126437},
    eprint = {https://pubs.aip.org/aip/jcp/article-pdf/doi/10.1063/5.0126437/16715104/234111\_1\_online.pdf},
}
@book{Sato2013,
    author         = {Keniti Sato},
    year           = {2013},
    title          = {Lévy Processes and Infinitely Divisible Distributions},
    series         = {Cambridge Studies in Advanced Mathematics},
    volume         = {},
    edition        = {2},
    url            = {},
    publisher      = {Cambridge University Press}
}

@book{Last_Penrose2017, place={Cambridge}, series={Institute of Mathematical Statistics Textbooks}, title={Lectures on the Poisson Process}, publisher={Cambridge University Press}, author={Last, Günter and Penrose, Mathew}, year={2017}, collection={Institute of Mathematical Statistics Textbooks}}

@article{Kingman2006,
    author          = {J. F. C. Kingman},
    year            = {2006},
    title           = {{Poisson Processes Revisited}},
    journal         = {Probability and Mathematical Statistics},
    volume          = {26},
    number          = {1},
    pages           = {77-95},
    url             = {http://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-66de0dee-771a-4cf4-bc02-57f689661a36}
}

@book{Kingman1992,
    author         = {J. F. C. Kingman},
    year           = {1992},
    title          = {Poisson Processes},
    series         = {Oxford Studies in Probability},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Clarendon Press}
}

@article{Madan+1998,
    author = {Madan, Dilip B. and Carr, Peter P. and Chang, Eric C.},
    title = "{The Variance Gamma Process and Option Pricing}",
    journal = {Review of Finance},
    volume = {2},
    number = {1},
    pages = {79-105},
    year = {1998},
    month = {04},
    abstract = "{A three parameter stochastic process, termed the variance gamma process, that generalizes Brownian motion is developed as a model for the dynamics of log stock prices. Theprocess is obtained by evaluating Brownian motion with drift at a random time given by a gamma process. The two additional parameters are the drift of the Brownian motion and the volatility of the time change. These additional parameters provide control over the skewness and kurtosis of the return distribution. Closed forms are obtained for the return density and the prices of European options.The statistical and risk neutral densities are estimated for data on the S\\&amp;P500 Index and the prices of options on this Index. It is observed that the statistical density is symmetric with some kurtosis, while the risk neutral density is negatively skewed with a larger kurtosis. The additional parameters also correct for pricing biases of the Black Scholes model that is a parametric special case of the option pricing model developed here.}",
    issn = {1572-3097},
    doi = {10.1023/A:1009703431535},
    url = {https://doi.org/10.1023/A:1009703431535},
    eprint = {https://academic.oup.com/rof/article-pdf/2/1/79/26315662/2-1-79.pdf},
}

@techreport{Todorovic-Yevjevich1969,
    institution = {Colorado State University},
    author          = {P. Todorovic and V. Yevjevich},
    year            = {1969},
    title           = {Stochastic Process of Precipitation},
    journal         = {Hydrology Papers},
}

@article{西村克己-江藤剛治1981,
    author          = {西村克己 and 江藤剛治},
    year            = {1981},
    title           = {Marked point Processモデルによる降水量時系列の解析},
    journal         = {水理講演会論文集},
    volume          = {25},
    number          = {},
    pages           = {191-196},
    url             = {https://doi.org/10.2208/prohe1975.25.191}
}

@ARTICLE{Ogata1981,
  author={Ogata, Y.},
  journal={IEEE Transactions on Information Theory}, 
  title={On Lewis' simulation method for point processes}, 
  year={1981},
  volume={27},
  number={1},
  pages={23-31},
  keywords={},
  doi={10.1109/TIT.1981.1056305}}
@article{Campbell1909,
  added-at = {2008-07-07T11:54:00.000+0200},
  author = {Campbell, N.},
  biburl = {https://www.bibsonomy.org/bibtex/2c6cd14bf56c439b248b0a2126f243abe/srd27},
  date = {1909},
  interhash = {e5c8dace5d02dca967de594892943460},
  intrahash = {c6cd14bf56c439b248b0a2126f243abe},
  journal = {Proc Cambr. Phil. Soc},
  keywords = {imported},
  pages = {pp. 117-136},
  timestamp = {2008-07-07T11:54:00.000+0200},
  title = {The study of discontinuous phenomena},
  volume = {vol. 15},
  year = 1909
}

@unpublished{Eberle2012,
    author = {Andreas Eberle},
    year   = {2012},
    title  = {Stochastic Analysis},
    url    = {https://wt.iam.uni-bonn.de/fileadmin/WT/Inhalt/people/Andreas_Eberle/StoAn1112/StochasticAnalysisFinal.pdf}
}


@article{Ito1941,
	author = {Kiyosi Ito},
	doi = {10.4099/jjm1924.18.0_261},
	journal = {Japanese journal of mathematics :transactions and abstracts},
	pages = {261-301},
	title = {On stochastic processes (I)},
	volume = {18},
	year = {1941},
	bdsk-url-1 = {https://doi.org/10.4099/jjm1924.18.0_261}}

@book{Arteaga-Sato2019,
    author         = {Alfonso Rocha-Arteaga and Keniti Sato},
    year           = {2019},
    title          = {Topics in Infinitely Divisible Distributions and Lévy Processes, Revised Edition},
    series         = {Springer Briefs in Probability and Mathematical Statistics},
    volume         = {},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-030-22700-5},
    publisher      = {Springer Cham}
}

@book{Bottcher+2013,
    author         = {Björn Böttcher and René Schilling and Jian Wang},
    year           = {2013},
    title          = {Lévy Matters III: Lévy-Type Processes: Construction, Approximation and Sample Path Properties},
    series         = {Lecture Notes in Mathematics},
    volume         = {2099},
    edition        = {},
    url            = {https://link.springer.com/book/10.1007/978-3-319-02684-8},
    publisher      = {Springer Cham}
}
@book{Osswald2012,
    author         = {Horst Osswald},
    year           = {2012},
    title          = {Malliavin Calculus for Lévy Processes and Infinite-Dimentional Brownian Motion},
    series         = {Cambridge Tracts in Mathematics},
    volume         = {191},
    edition        = {},
    url            = {https://www.cambridge.org/core/books/malliavin-calculus-for-levy-processes-and-infinitedimensional-brownian-motion/860213B6477038E51E829940593AB15D},
    publisher      = {Cambridge University Press}
}
@book{佐藤健一1990,
    author         = {佐藤健一},
    year           = {1990},
    month          = {12},
    title          = {加法過程},
    series         = {紀伊国屋数学叢書},
    volume         = {33},
    edition        = {},
    url            = {},
    publisher      = {紀伊国屋書店}
}
@article{佐藤健一2011,
    author          = {佐藤健一},
    year            = {2011},
    title           = {レヴィ過程による確率積分と無限分解可能分布},
    journal         = {数学},
    volume          = {63},
    number          = {2},
    pages           = {161-181},
    url             = {https://www.jstage.jst.go.jp/article/sugaku/63/2/63_0632161/_article/-char/ja}
}

@article{Khinchin-Levy1936,
    author          = {A. Ya. Khinchin and Paul Lévy},
    year            = {1936},
    title           = {Sur les lois stables},
    journal         = {Comptes Rendus de l'Académie des Sciences},
    volume          = {202},
    number          = {},
    pages           = {374-376},
    url             = {}
}

@book{Moran1959,
    author         = {P. A. P. Moran},
    year           = {1959},
    title          = {The Theory of Storage},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {London: Methuen and Co. Led. New York: Wiley and Sons, Inc.}
}

@unpublished{AlmostSure2011,
    author = {George Lowther},
    year   = {2011},
    title  = {Properties of Lévy Processes},
    url    = {https://almostsuremath.com/2011/02/25/properties-of-levy-processes/}
}

@article{Karamata1933,
author = {Karamata, J.},
journal = {Bulletin de la Société Mathématique de France},
keywords = {Analysis},
language = {fre},
pages = {55-62},
publisher = {Société mathématique de France},
title = {Sur un mode de croissance régulière. Théorèmes fondamentaux},
url = {http://eudml.org/doc/86621},
volume = {61},
year = {1933},
}

@book{Ibragimov-Linnik1971,
    author         = {I. A. Ibragimov and Yu V. Linnik},
    year           = {1971},
    title          = {Independent and Stationary Sequences of Random Variables},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {Groningen, Wolters-Noordhoff}
}

@book{Embrechts-Maejima2002,
    author         = {Paul Embrechts and Makoto Maejima},
    year           = {2002},
    title          = {Selfsimilar Processes},
    series         = {Princeton Series in Applied Mathematics},
    volume         = {},
    edition        = {},
    url            = {https://www.jstor.org/stable/j.ctt7t1hk},
    publisher      = {Princeton University Press}
}


@book{Davis1993,
    author         = {M. H. A. Davis},
    year           = {1993},
    title          = {Markov Models and Optimization},
    series         = {Monographs on Statistics and Applied Probability},
    volume         = {49},
    edition        = {},
    url            = {https://doi.org/10.1201/9780203748039},
    publisher      = {Chapman \& Hall},
}
@phdthesis{Vasdekis2021,
           title = {On zig-zag extensions and related ergodicity properties},
           month = {February},
            note = {Unpublished},
          school = {University of Warwick},
            year = {2021},
             url = {http://webcat.warwick.ac.uk/record=b3714913},
        abstract = {In this thesis, we study the Zig-Zag process, which was recently proposed as an MCMC algorithm. We propose two extensions for this process and we prove geometric  ergodicity results for both of them. The first extension, allows the process  to move in more directions than just parallel to f},
          author = {Vasdekis, Georgios}
}

@article{Vasdekis-Roberts2023,
	author = {G. Vasdekis and G. O. Roberts},
	doi = {10.1214/23-AAP1930},
	journal = {The Annals of Applied Probability},
	keywords = {central limit theorem, exponential ergodicity, Markov chain Monte Carlo, Piecewise deterministic Markov process},
	number = {6A},
	pages = {4693 -- 4746},
	publisher = {Institute of Mathematical Statistics},
	title = {{Speed up Zig-Zag}},
	url = {https://doi.org/10.1214/23-AAP1930},
	volume = {33},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1214/23-AAP1930}}
@article{Goldstein1951,
    author = {Goldstein, S.},
    title = "{On Diffusion by Discontinuous Movements, and on the Telegraph Equation}",
    journal = {The Quarterly Journal of Mechanics and Applied Mathematics},
    volume = {4},
    number = {2},
    pages = {129-156},
    year = {1951},
    month = {01},
    abstract = "{At time t=0 a large number of non-interacting particles start from an origin and move with a uniform velocity υ along a straight line for an interval of time τ. To begin with, half move in each direction. Thereafter, and at the end of each successive interval of time τ, each particle starts a new partial path; it still moves with speed υ, and there is a probability p that it will continue to move in the same direction as in its previous path, and a probability q (= 1−p) that the direction of its velocity will be reversed, so the directions in any two consecutive intervals are correlated with a correlation coefficient c=p−q. The partial correlations for non-consecutive intervals are zero. The difference equation is found for the fraction γ(n,ν) of the number of particles at a distance y=νυτ from the origin after a time t=nτ; it is shown how γ(n,ν) may be computed; asymptotic formulae for large n are found, both for a fixed value p/q and for a fixed value of nq/p. The limiting density distribution (and the limiting characteristic function) are found when n→∞, τ→0, with nτ=t, νυτ=y, and in the limiting operation c=1−τ/A, with A constant, so that c→1, and the speed υ is kept constant; the limiting form of the difference equation is the telegraph equation, with υ2=(LC)−1, A=L/R, where L, C, and R are the self-inductance, capacitance, and resistance per unit length; and the limiting density distribution is the solution of this equation for an instantaneous source. If p+q≠1, and there is, at the end of each interval of time τ, a non-zero probability, 1−p−q, that a particle will escape from the system, then the same limiting operation, with T/(1−p−q)=G, G constant, leads to the telegraph equation with leakage, the leakage resistance being G/C. The solution of the telegraph equation is further considered (and in particular is given in terms of Lommel's function of two variables) when one end of a long cable is held at a constant potential for t≥0.}",
    issn = {0033-5614},
    doi = {10.1093/qjmam/4.2.129},
    url = {https://doi.org/10.1093/qjmam/4.2.129},
    eprint = {https://academic.oup.com/qjmam/article-pdf/4/2/129/5301574/4-2-129.pdf},
}

@article{Andrieu-Livingstone2021,
	author = {Christophe Andrieu and Samuel Livingstone},
	doi = {10.1214/20-AOS2008},
	journal = {The Annals of Statistics},
	keywords = {Markov chain Monte Carlo, Peskun ordering, Piecewise deterministic Markov processes},
	number = {4},
	pages = {1958 -- 1981},
	publisher = {Institute of Mathematical Statistics},
	title = {{Peskun--Tierney ordering for Markovian Monte Carlo: Beyond the reversible scenario}},
	url = {https://doi.org/10.1214/20-AOS2008},
	volume = {49},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/20-AOS2008}}
@article{Sen+2020,
    author = {Sen, Deborshee and Sachs, Matthias and Lu, Jianfeng and Dunson, David B},
    title = "{Efficient posterior sampling for high-dimensional imbalanced logistic regression}",
    journal = {Biometrika},
    volume = {107},
    number = {4},
    pages = {1005-1012},
    year = {2020},
    month = {06},
    abstract = "{Classification with high-dimensional data is of widespread interest and often involves dealing with imbalanced data. Bayesian classification approaches are hampered by the fact that current Markov chain Monte Carlo algorithms for posterior computation become inefficient as the number \\$p\\$ of predictors or the number \\$n\\$ of subjects to classify gets large, because of the increasing computational time per step and worsening mixing rates. One strategy is to employ a gradient-based sampler to improve mixing while using data subsamples to reduce the per-step computational complexity. However, the usual subsampling breaks down when applied to imbalanced data. Instead, we generalize piecewise-deterministic Markov chain Monte Carlo algorithms to include importance-weighted and mini-batch subsampling. These maintain the correct stationary distribution with arbitrarily small subsamples and substantially outperform current competitors. We provide theoretical support for the proposed approach and demonstrate its performance gains in simulated data examples and an application to cancer data.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asaa035},
    url = {https://doi.org/10.1093/biomet/asaa035},
    eprint = {https://academic.oup.com/biomet/article-pdf/107/4/1005/34865861/asaa035.pdf},
}

@InProceedings{Pakman+2017,
  title = 	 {Stochastic Bouncy Particle Sampler},
  author =       {Ari Pakman and Dar Gilboa and David Carlson and Liam Paninski},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {2741--2750},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/pakman17a/pakman17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/pakman17a.html},
  abstract = 	 {We introduce a stochastic version of the non-reversible, rejection-free Bouncy Particle Sampler (BPS), a Markov process whose sample trajectories are piecewise linear, to efficiently sample Bayesian posteriors in big datasets. We prove that in the BPS no bias is introduced by noisy evaluations of the log-likelihood gradient. On the other hand, we argue that efficiency considerations favor a small, controllable bias, in exchange for faster mixing. We introduce a simple method that controls this trade-off. We illustrate these ideas in several examples which outperform previous approaches.}
}

@article{Bierkens+2023,
	abstract = {We construct a new class of efficient Monte Carlo methods based on continuous-time piecewise deterministic Markov processes (PDMPs) suitable for inference in high dimensional sparse models, i.e. models for which there is prior knowledge that many coordinates are likely to be exactly 0. This is achieved with the fairly simple idea of endowing existing PDMP samplers with ``sticky''coordinate axes, coordinate planes etc. Upon hitting those subspaces, an event is triggered during which the process sticks to the subspace, this way spending some time in a sub-model. This results in non-reversible jumps between different (sub-)models. While we show that PDMP samplers in general can be made sticky, we mainly focus on the Zig-Zag sampler. Compared to the Gibbs sampler for variable selection, we heuristically derive favourable dependence of the Sticky Zig-Zag sampler on dimension and data size. The computational efficiency of the Sticky Zig-Zag sampler is further established through numerical experiments where both the sample size and the dimension of the parameter space are large.},
	author = {Bierkens, Joris and Grazzi, Sebastiano and Meulen, Frank van der and Schauer, Moritz},
	date = {2022/11/28},
	date-added = {2024-07-03 12:51:59 +0900},
	date-modified = {2024-07-03 12:51:59 +0900},
	doi = {10.1007/s11222-022-10180-5},
	id = {Bierkens2022},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {1},
	pages = {8},
	title = {{Sticky PDMP Samplers for Sparse and Local Inference Problems}},
	url = {https://doi.org/10.1007/s11222-022-10180-5},
	volume = {33},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-022-10180-5}}

@unpublished{Grazzi2020,
    author = {Sebastiano Grazzi},
    year   = {2020},
    title  = {Piecewise Deterministic Monte Carlo},
    url    = {https://diamhomes.ewi.tudelft.nl/~jorisbierkens/pdmps.html}
}

@inproceedings{Ge+2018,
  author    = {Hong Ge and
               Kai Xu and
               Zoubin Ghahramani},
  title     = {Turing: a language for flexible probabilistic inference},
  booktitle = {International Conference on Artificial Intelligence and Statistics,
               {AISTATS} 2018, 9-11 April 2018, Playa Blanca, Lanzarote, Canary Islands,
               Spain},
  pages     = {1682--1690},
  year      = {2018},
  url       = {http://proceedings.mlr.press/v84/ge18b.html},
  biburl    = {https://dblp.org/rec/bib/conf/aistats/GeXG18},
}
@misc{Veltz2015,
      title={A new twist for the simulation of hybrid systems using the true jump method}, 
      author={Romain Veltz},
      year={2015},
      eprint={1504.06873},
      archivePrefix={arXiv},
      primaryClass={math.NA},
      url={https://arxiv.org/abs/1504.06873}, 
}

@unpublished{Lawler2019,
    author = {Gregory F. Lawler},
    year   = {2019},
    title  = {Notes on the Bessel Process},
    url    = {https://www.math.uchicago.edu/~lawler/bessel18new.pdf}
}


@article{Kohatsu-Higa2022,
	abstract = {We consider a real-valued diffusion process with a linear jump term driven by a Poisson point process and we assume that the jump amplitudes have a centered density with finite moments. We show upper and lower estimates for the density of the solution in the case that the jump amplitudes follow a Gaussian or Laplacian law. The proof of the lower bound uses a general expression for the density of the solution in terms of the convolution of the density of the continuous part and the jump amplitude density. The upper bound uses an upper tail estimate in terms of the jump amplitude distribution and techniques of the Malliavin calculus in order to bound the density by the tails of the solution. We also extend the lower bounds to the multidimensional case.},
	author = {Arturo Kohatsu-Higa and Eulalia Nualart and Ngoc Khue Tran},
	doi = {https://doi.org/10.1016/j.amc.2021.126814},
	issn = {0096-3003},
	journal = {Applied Mathematics and Computation},
	keywords = {Density estimates, Jump diffusion process, Malliavin calculus},
	pages = {126814},
	title = {Density estimates for jump diffusion processes},
	url = {https://www.sciencedirect.com/science/article/pii/S0096300321008973},
	volume = {420},
	year = {2022},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0096300321008973},
	bdsk-url-2 = {https://doi.org/10.1016/j.amc.2021.126814}}

@inproceedings{Kohatsu-Higa2003,
	abstract = {In this article we interpret heuristically the conditions of the definition of a uniformly elliptic random variable on Wiener space which allow to obtain Aronson type estimates for the density of this random variable. As an example we apply this concept to uniformly elliptic non-homogeneous diffusions.},
	address = {Basel},
	author = {Kohatsu-Higa, Arturo},
	booktitle = {Stochastic Inequalities and Applications},
	editor = {Gin{\'e}, Evariste and Houdr{\'e}, Christian and Nualart, David},
	isbn = {978-3-0348-8069-5},
	pages = {323--338},
	publisher = {Birkh{\"a}user Basel},
	title = {Lower Bounds for Densities of Uniformly Elliptic Non-homogeneous Diffusions},
	year = {2003},
    url             = {https://doi.org/10.1007/978-3-0348-8069-5_18},
}

@article{Taniguchi1985,
	author = {Setsuo Taniguchi},
	journal = {Osaka Journal of Mathematics},
	number = {2},
	pages = {307 -- 320},
	publisher = {Osaka University and Osaka Metropolitan University, Departments of Mathematics},
	title = {{Applications of Malliavin's calculus to time-dependent systems of heat equations}},
	volume = {22},
	year = {1985},
    url             = {https://projecteuclid.org/journals/osaka-journal-of-mathematics/volume-22/issue-2/Applications-of-Malliavins-calculus-to-time-dependent-systems-of-heat/ojm/1200778261.full},
}
@article{Li+2017,
    author = {Li, Cheng and Srivastava, Sanvesh and Dunson, David B.},
    title = "{Simple, scalable and accurate posterior interval estimation}",
    journal = {Biometrika},
    volume = {104},
    number = {3},
    pages = {665-680},
    year = {2017},
    month = {06},
    abstract = "{Standard posterior sampling algorithms, such as Markov chain Monte Carlo procedures, face major challenges in scaling up to massive datasets. We propose a simple and general posterior interval estimation algorithm to rapidly and accurately estimate quantiles of the posterior distributions for one-dimensional functionals. Our algorithm runs Markov chain Monte Carlo in parallel for subsets of the data, and then averages quantiles estimated from each subset. We provide strong theoretical guarantees and show that the credible intervals from our algorithm asymptotically approximate those from the full posterior in the leading parametric order. Our algorithm has a better balance of accuracy and efficiency than its competitors across a variety of simulations and a real-data example.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asx033},
    url = {https://doi.org/10.1093/biomet/asx033},
    eprint = {https://academic.oup.com/biomet/article-pdf/104/3/665/19563079/asx033.pdf},
}

@InProceedings{Srivastava+2015,
  title = 	 {{WASP: Scalable Bayes via barycenters of subset posteriors}},
  author = 	 {Srivastava, Sanvesh and Cevher, Volkan and Dinh, Quoc and Dunson, David},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {912--920},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/srivastava15.pdf},
  url = 	 {https://proceedings.mlr.press/v38/srivastava15.html},
  abstract = 	 {The promise of Bayesian methods for big data sets has not fully been realized due to the lack of scalable computational algorithms. For massive data, it is necessary to store and process subsets on different machines in a distributed manner. We propose a simple, general, and highly efficient approach, which first runs a posterior sampling algorithm in parallel on different machines for subsets of a large data set. To combine these subset posteriors, we calculate the Wasserstein barycenter via a highly efficient linear program. The resulting estimate for the Wasserstein posterior (WASP) has an atomic form, facilitating straightforward estimation of posterior summaries of functionals of interest. The WASP approach allows posterior sampling algorithms for smaller data sets to be trivially scaled to huge data. We provide theoretical justification in terms of posterior consistency and algorithm efficiency.  Examples are provided in complex settings including Gaussian process regression and nonparametric Bayes mixture models.}
}
@inproceedings{Welling-Teh2011,
author = {Welling, Max and Teh, Yee Whye},
title = {Bayesian learning via stochastic gradient langevin dynamics},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {681–688},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11},
url             = {https://dl.acm.org/doi/10.5555/3104482.3104568},
}

@article{Jarner-Tweedie2003,
 ISSN = {13507265},
 URL = {http://www.jstor.org/stable/3318785},
 abstract = {We give necessary conditions for geometric and polynomial convergence rates of random-walk-type Markov chains to stationarity in terms of existence of exponential and polynomial moments of the invariant distribution and the Markov transition kernel. These results complement the use of Foster-Lyapunov drift conditions for establishing geometric and polynomial ergodicity. For polynomially ergodic Markov chains, the results allow us to derive exact rates of convergence and exact relations between the moments of the invariant distribution and the Markov transition kernel. In an application to Markov chain Monte Carlo we derive tight rates of convergence for symmetric random walk Metropolis algorithms and Langevin algorithms with polynomial target densities.},
 author = {Søren F. Jarner and Richard L. Tweedie},
 journal = {Bernoulli},
 number = {4},
 pages = {559--578},
 publisher = {International Statistical Institute (ISI) and Bernoulli Society for Mathematical Statistics and Probability},
 title = {Necessary Conditions for Geometric and Polynomial Ergodicity of Random-Walk-Type Markov Chains},
 urldate = {2024-07-10},
 volume = {9},
 year = {2003}
}

@article{Polson+2013,
	author = {Nicholas G. Polson and James G. Scott and Jesse Windle},
	doi = {10.1080/01621459.2013.829001},
	eprint = {https://doi.org/10.1080/01621459.2013.829001},
	journal = {Journal of the American Statistical Association},
	number = {504},
	pages = {1339--1349},
	publisher = {Taylor \& Francis},
	title = {Bayesian Inference for Logistic Models Using P{\'o}lya--Gamma Latent Variables},
	url = {https://doi.org/10.1080/01621459.2013.829001},
	volume = {108},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2013.829001}}

@article{Albert-Chib1993,
	author = {James H. Albert and Siddhartha Chib},
	doi = {10.1080/01621459.1993.10476321},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1993.10476321},
	journal = {Journal of the American Statistical Association},
	number = {422},
	pages = {669--679},
	publisher = {Taylor \& Francis},
	title = {Bayesian Analysis of Binary and Polychotomous Response Data},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476321},
	volume = {88},
	year = {1993},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476321},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1993.10476321}}

@article{Johndrow+2019,
	author = {James E. Johndrow and Aaron Smith and Natesh Pillai and David B. Dunson},
	doi = {10.1080/01621459.2018.1505626},
	eprint = {https://doi.org/10.1080/01621459.2018.1505626},
	journal = {Journal of the American Statistical Association},
	number = {527},
	pages = {1394--1403},
	publisher = {Taylor \& Francis},
	title = {MCMC for Imbalanced Categorical Data},
	url = {https://doi.org/10.1080/01621459.2018.1505626},
	volume = {114},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2018.1505626}}
@article{Owen2007,
  author  = {Art B. Owen},
  title   = {Infinitely Imbalanced Logistic Regression},
  journal = {Journal of Machine Learning Research},
  year    = {2007},
  volume  = {8},
  number  = {27},
  pages   = {761--773},
  url     = {http://jmlr.org/papers/v8/owen07a.html}
}

@article{Bierkens+2021,
	author = {Joris Bierkens and Pierre Nyquist and Mikola C. Schlottke},
	doi = {10.1214/21-AAP1663},
	journal = {The Annals of Applied Probability},
	keywords = {empirical measure, large deviations, Piecewise deterministic Markov process, zig-zag process},
	number = {6},
	pages = {2811 -- 2843},
	publisher = {Institute of Mathematical Statistics},
	title = {{Large Deviations for the Empirical Measure of the Zig-Zag Process}},
	url = {https://doi.org/10.1214/21-AAP1663},
	volume = {31},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/21-AAP1663}}
@misc{Fearnhead+2024,
      title={Stochastic Gradient Piecewise Deterministic Monte Carlo Samplers}, 
      author={Paul Fearnhead and Sebastiano Grazzi and Chris Nemeth and Gareth O. Roberts},
      year={2024},
      eprint={2406.19051},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2406.19051}, 
}
@article{Johnson1970,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239239},
 author = {Richard A. Johnson},
 journal = {The Annals of Mathematical Statistics},
 number = {3},
 pages = {851--864},
 publisher = {Institute of Mathematical Statistics},
 title = {Asymptotic Expansions Associated with Posterior Distributions},
 urldate = {2024-07-17},
 volume = {41},
 year = {1970}
}

@article{Sur-Candes2019,
	abstract = {Students in statistics or data science usually learn early on that when the sample size n is large relative to the number of variables p, fitting a logistic model by the method of maximum likelihood produces estimates that are consistent and that there are well-known formulas that quantify the variability of these estimates which are used for the purpose of statistical inference. We are often told that these calculations are approximately valid if we have 5 to 10 observations per unknown parameter. This paper shows that this is far from the case, and consequently, inferences produced by common software packages are often unreliable. Consider a logistic model with independent features in which n and p become increasingly large in a fixed ratio. We prove that (i) the maximum-likelihood estimate (MLE) is biased, (ii) the variability of the MLE is far greater than classically estimated, and (iii) the likelihood-ratio test (LRT) is not distributed as a χ2. The bias of the MLE yields wrong predictions for the probability of a case based on observed values of the covariates. We present a theory, which provides explicit expressions for the asymptotic bias and variance of the MLE and the asymptotic distribution of the LRT. We empirically demonstrate that these results are accurate in finite samples. Our results depend only on a single measure of signal strength, which leads to concrete proposals for obtaining accurate inference in finite samples through the estimate of this measure.},
	author = {Pragya Sur and Emmanuel J. Cand{\`e}s},
	doi = {10.1073/pnas.1810420116},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1810420116},
	journal = {Proceedings of the National Academy of Sciences},
	number = {29},
	pages = {14516-14525},
	title = {{A Modern Maximum-Likelihood Theory for High-Dimensional Logistic Regression}},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1810420116},
	volume = {116},
	year = {2019},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1810420116},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1810420116}}
@ARTICLE{Candes-Tao2006,
  author={Candes, Emmanuel J. and Tao, Terence},
  journal={IEEE Transactions on Information Theory}, 
  title={Near-Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?}, 
  year={2006},
  volume={52},
  number={12},
  pages={5406-5425},
  keywords={Encoding;Vectors;Image reconstruction;Mathematics;Digital images;Image coding;Measurement standards;Linear programming;Geometry;Concrete;Concentration of measure;convex optimization;duality in optimization;linear programming;random matrices;random projections;signal recovery;singular values of random matrices;sparsity;trigonometric expansions;uncertainty principle},
  doi={10.1109/TIT.2006.885507}}
@inproceedings{池田思朗2012,
 address = {京都大学},
 author = {池田思朗},
 booktitle = {第55回自動制御連合講演会講演論文集},
 month = {11},
 pages = {1043--1046},
 title = {スパースモデリングとベイズ統計},
 year = {2012},
 yomi = {Ikeda, Shiro}
}
@article{池田思朗+2004,
 author = {池田思朗 and 田中利幸 and 甘利俊一},
 journal = {統計数理},
 month = {12},
 number = {2},
 pages = {393--405},
 title = {情報幾何学に基づく確率伝搬法の解析},
 volume = {52},
 year = {2004},
 yomi = {Ikeda, Shiro}
}




@article{Sachs+2023,
	author = {Matthias Sachs and Deborshee Sen and Jianfeng Lu and David Dunson},
	doi = {10.1214/22-BA1319},
	journal = {Bayesian Analysis},
	keywords = {Gibbs sampler, Markov chain Monte Carlo, non-reversible, Piecewise deterministic Markov process, sub-sampling},
	number = {3},
	pages = {909 -- 927},
	publisher = {International Society for Bayesian Analysis},
	title = {{Posterior Computation with the Gibbs Zig-Zag Sampler}},
	url = {https://doi.org/10.1214/22-BA1319},
	volume = {18},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1214/22-BA1319}}

@article{Quiroz+2019,
	author = {Matias Quiroz and Robert Kohn and Mattias Villani and Minh-Ngoc Tran},
	doi = {10.1080/01621459.2018.1448827},
	eprint = {https://doi.org/10.1080/01621459.2018.1448827},
	journal = {Journal of the American Statistical Association},
	number = {526},
	pages = {831--843},
	publisher = {Taylor \& Francis},
	title = {Speeding Up MCMC by Efficient Data Subsampling},
	url = {https://doi.org/10.1080/01621459.2018.1448827},
	volume = {114},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2018.1448827}}

@article{Dang+2019,
  author  = {Khue-Dung Dang and Matias Quiroz and Robert Kohn and Minh-Ngoc Tran and Mattias Villani},
  title   = {Hamiltonian Monte Carlo with Energy Conserving Subsampling},
  journal = {Journal of Machine Learning Research},
  year    = {2019},
  volume  = {20},
  number  = {100},
  pages   = {1-31},
  url     = {http://jmlr.org/papers/v20/17-452.html}
}

@inproceedings{Zhang+2020,
	author = {Zhang, Ruqi and Cooper, A. Feder and De Sa, Christopher M},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {19500--19510},
	publisher = {Curran Associates, Inc.},
	title = {Asymptotically Optimal Exact Minibatch Metropolis-Hastings},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e2a7555f7cabd6e31aef45cb8cda4999-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/e2a7555f7cabd6e31aef45cb8cda4999-Paper.pdf}}
@article{Scott+2016,
title	= {Bayes and Big Data: The Consensus Monte Carlo Algorithm},
author	= {Steven L. Scott and Alexander W. Blocker and Fernando V. Bonassi and Hugh A. Chipman and Edward I. George and Robert E. McCulloch},year	= {2016},URL	= {http://www.tandfonline.com/doi/full/10.1080/17509653.2016.1142191},journal	= {International Journal of Management Science and Engineering Management},pages	= {78-88},volume	= {11}}


@article{Corbella+2022,
	abstract = {Novel Monte Carlo methods to generate samples from a target distribution, such as a posterior from a Bayesian analysis, have rapidly expanded in the past decade. Algorithms based on Piecewise Deterministic Markov Processes (PDMPs), non-reversible continuous-time processes, are developing into their own research branch, thanks their important properties (e.g., super-efficiency). Nevertheless, practice has not caught up with the theory in this field, and the use of PDMPs to solve applied problems is not widespread. This might be due, firstly, to several implementational challenges that PDMP-based samplers present with and, secondly, to the lack of papers that showcase the methods and implementations in applied settings. Here, we address both these issues using one of the most promising PDMPs, the Zig-Zag sampler, as an archetypal example. After an explanation of the key elements of the Zig-Zag sampler, its implementation challenges are exposed and addressed. Specifically, the formulation of an algorithm that draws samples from a target distribution of interest is provided. Notably, the only requirement of the algorithm is a closed-form differentiable function to evaluate the log-target density of interest, and, unlike previous implementations, no further information on the target is needed. The performance of the algorithm is evaluated against canonical Hamiltonian Monte Carlo, and it is proven to be competitive, in simulation and real-data settings. Lastly, we demonstrate that the super-efficiency property, i.e. the ability to draw one independent sample at a lesser cost than evaluating the likelihood of all the data, can be obtained in practice.},
	author = {Corbella, Alice and Spencer, Simon E. F. and Roberts, Gareth O.},
	date = {2022/11/09},
	date-added = {2024-07-24 21:14:56 +0900},
	date-modified = {2024-07-24 21:14:56 +0900},
	doi = {10.1007/s11222-022-10142-x},
	id = {Corbella2022},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {6},
	pages = {107},
	title = {{Automatic Zig-Zag Sampling in Practice}},
	url = {https://doi.org/10.1007/s11222-022-10142-x},
	volume = {32},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-022-10142-x}}

@article{Sutton-Fearnhead2023,
	author = {Matthew Sutton and Paul Fearnhead},
	doi = {10.1080/10618600.2023.2203735},
	eprint = {https://doi.org/10.1080/10618600.2023.2203735},
	journal = {Journal of Computational and Graphical Statistics},
	number = {4},
	pages = {1425--1435},
	publisher = {Taylor \& Francis},
	title = {{Concave-Convex PDMP-based Sampling}},
	url = {https://doi.org/10.1080/10618600.2023.2203735},
	volume = {32},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1080/10618600.2023.2203735}}

@article{Pagani+2023,
	abstract = {Markov chain Monte Carlo (MCMC) is a key algorithm in computational statistics, and as datasets grow larger and models grow more complex, many popular MCMC algorithms become too computationally expensive to be practical. Recent progress has been made on this problem through development of MCMC algorithms based on Piecewise Deterministic Markov Processes (PDMPs), irreversible processes which can be engineered to converge at a rate which is independent of the size of the dataset. While there has understandably been a surge of theoretical studies following these results, PDMPs have so far only been implemented for models where certain gradients can be bounded in closed form, which is not possible in many relevant statistical problems. Furthermore, there has been substantionally less focus on practical implementation, or the efficiency of PDMP dynamics in exploring challenging densities. Focusing on the Zig-Zag process, we present the Numerical Zig-Zag (NuZZ) algorithm, which is applicable to general statistical models without the need for bounds on the gradient of the log posterior. This allows us to perform numerical experiments on: (i) how the Zig-Zag dynamics behaves on some test problems with common challenging features; and (ii) how the error between the target and sampled distributions evolves as a function of computational effort for different MCMC algorithms including NuZZ. Moreover, due to the specifics of the NuZZ algorithms, we are able to give an explicit bound on the Wasserstein distance between the exact posterior and its numerically perturbed counterpart in terms of the user-specified numerical tolerances of NuZZ.},
	author = {Pagani, Filippo and Chevallier, Augustin and Power, Sam and House, Thomas and Cotter, Simon},
	date = {2024/01/05},
	date-added = {2024-07-24 21:17:39 +0900},
	date-modified = {2024-07-24 21:17:39 +0900},
	doi = {10.1007/s11222-023-10363-8},
	id = {Pagani2024},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {1},
	pages = {61},
	title = {NuZZ: Numerical Zig-Zag for general models},
	url = {https://doi.org/10.1007/s11222-023-10363-8},
	volume = {34},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-023-10363-8}}

@article{Bierkens-Roberts-Zigg2019,
	author = {Joris Bierkens and Gareth O. Roberts and Pierre-Andr{\'e} Zitt},
	doi = {10.1214/18-AAP1453},
	journal = {The Annals of Applied Probability},
	keywords = {central limit theorem, ergodicity, exponential ergodicity, irreducibility, Piecewise deterministic Markov process},
	number = {4},
	pages = {2266 -- 2301},
	publisher = {Institute of Mathematical Statistics},
	title = {{Ergodicity of the zigzag process}},
	url = {https://doi.org/10.1214/18-AAP1453},
	volume = {29},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1214/18-AAP1453}}
@misc{Carbone2024,
      title={Hitchhiker's guide on Energy-Based Models: a comprehensive review on the relation with other generative models, sampling and statistical physics}, 
      author={Davide Carbone},
      year={2024},
      eprint={2406.13661},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.13661}, 
}

@article{Winn-Bishop2005,
  author  = {John Winn and Christopher M. Bishop},
  title   = {Variational Message Passing},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {23},
  pages   = {661--694},
  url     = {http://jmlr.org/papers/v6/winn05a.html}
}

@article{Fortunato2010,
	abstract = {The modern science of networks has brought significant advances to our understanding of complex systems. One of the most relevant features of graphs representing real systems is community structure, or clustering, i.e. the organization of vertices in clusters, with many edges joining vertices of the same cluster and comparatively few edges joining vertices of different clusters. Such clusters, or communities, can be considered as fairly independent compartments of a graph, playing a similar role like, e.g., the tissues or the organs in the human body. Detecting communities is of great importance in sociology, biology and computer science, disciplines where systems are often represented as graphs. This problem is very hard and not yet satisfactorily solved, despite the huge effort of a large interdisciplinary community of scientists working on it over the past few years. We will attempt a thorough exposition of the topic, from the definition of the main elements of the problem, to the presentation of most methods developed, with a special focus on techniques designed by statistical physicists, from the discussion of crucial issues like the significance of clustering and how methods should be tested and compared against each other, to the description of applications to real networks.},
	author = {Santo Fortunato},
	doi = {https://doi.org/10.1016/j.physrep.2009.11.002},
	issn = {0370-1573},
	journal = {Physics Reports},
	keywords = {Graphs, Clusters, Statistical physics},
	number = {3},
	pages = {75-174},
	title = {Community detection in graphs},
	url = {https://www.sciencedirect.com/science/article/pii/S0370157309002841},
	volume = {486},
	year = {2010},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0370157309002841},
	bdsk-url-2 = {https://doi.org/10.1016/j.physrep.2009.11.002}}

@article{Girvan-Newman2002,
	abstract = {A number of recent studies have focused on the statistical properties of networked systems such as social networks and the Worldwide Web. Researchers have concentrated particularly on a few properties that seem to be common to many networks: the small-world property, power-law degree distributions, and network transitivity. In this article, we highlight another property that is found in many networks, the property of community structure, in which network nodes are joined together in tightly knit groups, between which there are only looser connections. We propose a method for detecting such communities, built around the idea of using centrality indices to find community boundaries. We test our method on computer-generated and real-world graphs whose community structure is already known and find that the method detects this known structure with high sensitivity and reliability. We also apply the method to two networks whose community structure is not well known---a collaboration network and a food web---and find that it detects significant and informative community divisions in both cases.},
	author = {M. Girvan and M. E. J. Newman},
	doi = {10.1073/pnas.122653799},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.122653799},
	journal = {Proceedings of the National Academy of Sciences},
	number = {12},
	pages = {7821-7826},
	title = {Community structure in social and biological networks},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.122653799},
	volume = {99},
	year = {2002},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.122653799},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.122653799}}

@article{村田剛志2009,
	author = {村田剛志},
	doi = {10.3156/jsoft.21.4_500},
	journal = {知能と情報},
	number = {4},
	pages = {500-508},
	title = {ネットワークからのコミュニティ抽出},
	volume = {21},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.3156/jsoft.21.4_500}}
@article{Newman2004,
  title = {Fast algorithm for detecting community structure in networks},
  author = {Newman, M. E. J.},
  journal = {Phys. Rev. E},
  volume = {69},
  issue = {6},
  pages = {066133},
  numpages = {5},
  year = {2004},
  month = {Jun},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.69.066133},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.066133}
}
@article{Clauset+2004,
  title = {Finding community structure in very large networks},
  author = {Clauset, Aaron and Newman, M. E. J. and Moore, Cristopher},
  journal = {Phys. Rev. E},
  volume = {70},
  issue = {6},
  pages = {066111},
  numpages = {6},
  year = {2004},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.70.066111},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.70.066111}
}
@article{Guimera+2004,
  title = {Modularity from fluctuations in random graphs and complex networks},
  author = {Guimer\`a, Roger and Sales-Pardo, Marta and Amaral, Lu\'{\i}s A. Nunes},
  journal = {Phys. Rev. E},
  volume = {70},
  issue = {2},
  pages = {025101},
  numpages = {4},
  year = {2004},
  month = {Aug},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.70.025101},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.70.025101}
}
@article{Reichardt-Bornholdt2006,
  title = {Statistical mechanics of community detection},
  author = {Reichardt, J\"org and Bornholdt, Stefan},
  journal = {Phys. Rev. E},
  volume = {74},
  issue = {1},
  pages = {016110},
  numpages = {14},
  year = {2006},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.74.016110},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.74.016110}
}
@ARTICLE{Dpnath-Hoffman1973,
  author={Donath, W. E. and Hoffman, A. J.},
  journal={IBM Journal of Research and Development}, 
  title={Lower Bounds for the Partitioning of Graphs}, 
  year={1973},
  volume={17},
  number={5},
  pages={420-425},
  keywords={},
  doi={10.1147/rd.175.0420}}
@article{Donetti-Munoz2004,
doi = {10.1088/1742-5468/2004/10/P10012},
url = {https://dx.doi.org/10.1088/1742-5468/2004/10/P10012},
year = {2004},
month = {oct},
publisher = {},
volume = {2004},
number = {10},
pages = {P10012},
author = {Luca Donetti and  Miguel A Muñoz},
title = {Detecting network communities: a new systematic and efficient algorithm},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
abstract = {An efficient and relatively fast algorithm for the detection of communities in complex networks is introduced. The method exploits spectral properties of the graph Laplacian matrix combined with hierarchical clustering techniques, and includes a procedure for maximizing the ‘modularity’ of the output. Its performance is compared with that of other existing methods, as applied to different well-known instances of complex networks with a community structure, both computer generated and from the real world. Our results are, in all the cases tested, at least as good as the best ones obtained with any other methods, and faster in most of the cases than methods providing similar quality results. This converts the algorithm into a valuable computational tool for detecting and analysing communities and modular structures in complex networks.}
}
@article{Hastings2006,
  title = {Community detection as an inference problem},
  author = {Hastings, M. B.},
  journal = {Phys. Rev. E},
  volume = {74},
  issue = {3},
  pages = {035102},
  numpages = {4},
  year = {2006},
  month = {Sep},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.74.035102},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.74.035102}
}

@article{Newman-Leicht2007,
	abstract = {Networks are widely used in the biological, physical, and social sciences as a concise mathematical representation of the topology of systems of interacting components. Understanding the structure of these networks is one of the outstanding challenges in the study of complex systems. Here we describe a general technique for detecting structural features in large-scale network data that works by dividing the nodes of a network into classes such that the members of each class have similar patterns of connection to other nodes. Using the machinery of probabilistic mixture models and the expectation--maximization algorithm, we show that it is possible to detect, without prior knowledge of what we are looking for, a very broad range of types of structure in networks. We give a number of examples demonstrating how the method can be used to shed light on the properties of real-world networks, including social and information networks.},
	author = {M. E. J. Newman and E. A. Leicht},
	doi = {10.1073/pnas.0610537104},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0610537104},
	journal = {Proceedings of the National Academy of Sciences},
	number = {23},
	pages = {9564-9569},
	title = {Mixture models and exploratory analysis in networks},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0610537104},
	volume = {104},
	year = {2007},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0610537104},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0610537104}}

@article{Snijders-Nowicki1997,
	abstract = {blockmodeling for graphs is proposed. The model assumes that the vertices of the graph are partitioned into two unknown blocks and that the probability of an edge between two vertices depends only on the blocks to which they belong. Statistical procedures are derived for estimating the probabilities of edges and for predicting the block structure from observations of the edge pattern only. ML estimators can be computed using the EM algorithm, but this strategy is practical only for small graphs. A Bayesian estimator, based on the Gibbs sampling, is proposed. This estimator is practical also for large graphs. When ML estimators are used, the block structure can be predicted based on predictive likelihood. When Gibbs sampling is used, the block structure can be predicted from posterior predictive probabilities. A side result is that when the number of vertices tends to infinity while the probabilities remain constant, the block structure can be recovered correctly with probability tending to 1.},
	author = {Snijders, Tom A. B. and Nowicki, Krzysztof},
	date = {1997/01/01},
	date-added = {2024-07-26 20:42:31 +0900},
	date-modified = {2024-07-26 20:42:31 +0900},
	doi = {10.1007/s003579900004},
	id = {Snijders1997},
	isbn = {1432-1343},
	journal = {Journal of Classification},
	number = {1},
	pages = {75--100},
	title = {Estimation and Prediction for Stochastic Blockmodels for Graphs with Latent Block Structure},
	url = {https://doi.org/10.1007/s003579900004},
	volume = {14},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1007/s003579900004}}
@article{Decelle2011,
  title = {Asymptotic analysis of the stochastic block model for modular networks and its algorithmic applications},
  author = {Decelle, Aurelien and Krzakala, Florent and Moore, Cristopher and Zdeborov\'a, Lenka},
  journal = {Phys. Rev. E},
  volume = {84},
  issue = {6},
  pages = {066106},
  numpages = {19},
  year = {2011},
  month = {Dec},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.84.066106},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.84.066106}
}

@article{Lustig+2007,
	abstract = {Abstract The sparsity which is implicit in MR images is exploited to significantly undersample k-space. Some MR images such as angiograms are already sparse in the pixel representation; other, more complicated images have a sparse representation in some transform domain--for example, in terms of spatial finite-differences or their wavelet coefficients. According to the recently developed mathematical theory of compressed-sensing, images with a sparse representation can be recovered from randomly undersampled k-space data, provided an appropriate nonlinear recovery scheme is used. Intuitively, artifacts due to random undersampling add as noise-like interference. In the sparse transform domain the significant coefficients stand out above the interference. A nonlinear thresholding scheme can recover the sparse coefficients, effectively recovering the image itself. In this article, practical incoherent undersampling schemes are developed and analyzed by means of their aliasing interference. Incoherence is introduced by pseudo-random variable-density undersampling of phase-encodes. The reconstruction is performed by minimizing the ℓ1 norm of a transformed image, subject to data fidelity constraints. Examples demonstrate improved spatial resolution and accelerated acquisition for multislice fast spin-echo brain imaging and 3D contrast enhanced angiography. Magn Reson Med, 2007. {\copyright} 2007 Wiley-Liss, Inc.},
	author = {Lustig, Michael and Donoho, David and Pauly, John M.},
	doi = {https://doi.org/10.1002/mrm.21391},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.21391},
	journal = {Magnetic Resonance in Medicine},
	keywords = {compressed sensing, compressive sampling, random sampling, rapid MRI, sparsity, sparse reconstruction, nonlinear reconstruction},
	number = {6},
	pages = {1182-1195},
	title = {Sparse MRI: The application of compressed sensing for rapid MR imaging},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.21391},
	volume = {58},
	year = {2007},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/mrm.21391},
	bdsk-url-2 = {https://doi.org/10.1002/mrm.21391}}

@book{Pinkus1985,
    author         = {Allan Pinkus},
    year           = {1985},
    title          = {{$n$-Width in Approximation Theory}},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://doi.org/10.1007/978-3-642-69894-1},
    publisher      = {Springer Berlin, Heidelberg}
}

@article{Chen+1998,
	abstract = { The time-frequency and time-scale communities have recently developed a large number of overcomplete waveform dictionaries --- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the method of frames (MOF), Matching pursuit (MP), and, for special dictionaries, the best orthogonal basis (BOB).Basis Pursuit (BP) is a principle for decomposing a signal into an "optimal" superposition of dictionary elements, where optimal means having the smallest l1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP, and BOB, including better sparsity and superresolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation denoising, and multiscale edge denoising.BP in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver. },
	author = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
	doi = {10.1137/S1064827596304010},
	eprint = {https://doi.org/10.1137/S1064827596304010},
	journal = {SIAM Journal on Scientific Computing},
	number = {1},
	pages = {33-61},
	title = {Atomic Decomposition by Basis Pursuit},
	url = {https://doi.org/10.1137/S1064827596304010},
	volume = {20},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1137/S1064827596304010}}
@ARTICLE{Donoho2016,
  author={Donoho, D.L.},
  journal={IEEE Transactions on Information Theory}, 
  title={Compressed sensing}, 
  year={2006},
  volume={52},
  number={4},
  pages={1289-1306},
  keywords={Compressed sensing;Image reconstruction;Pixel;Vectors;Digital images;Image coding;Transform coding;Size measurement;Signal processing;Data mining;Adaptive sampling;almost-spherical sections of Banach spaces;Basis Pursuit;eigenvalues of random matrices;Gel'fand;information-based complexity;integrated sensing and processing;minimum;optimal recovery;Quotient-of-a-Subspace theorem;sparse solution of linear equations},
  doi={10.1109/TIT.2006.871582}}
@ARTICLE{Maleki-Donoho2010,
  author={Maleki, Arian and Donoho, David L.},
  journal={IEEE Journal of Selected Topics in Signal Processing}, 
  title={Optimally Tuned Iterative Reconstruction Algorithms for Compressed Sensing}, 
  year={2010},
  volume={4},
  number={2},
  pages={330-341},
  keywords={Reconstruction algorithms;Compressed sensing;Iterative algorithms;Signal processing algorithms;Proposals;Equations;Pursuit algorithms;Sparse matrices;Linear systems;Distributed computing;Compressed sensing;iterative algorithms;phase transition;random matrices;thresholding},
  doi={10.1109/JSTSP.2009.2039176}}
@ARTICLE{Kschischang+2001,
  author={Kschischang, F.R. and Frey, B.J. and Loeliger, H.-A.},
  journal={IEEE Transactions on Information Theory}, 
  title={Factor graphs and the sum-product algorithm}, 
  year={2001},
  volume={47},
  number={2},
  pages={498-519},
  keywords={Graph theory},
  doi={10.1109/18.910572}}

@article{Donoho+2009,
	abstract = {Compressed sensing aims to undersample certain high-dimensional signals yet accurately reconstruct them by exploiting signal characteristics. Accurate reconstruction is possible when the object to be recovered is sufficiently sparse in a known basis. Currently, the best known sparsity--undersampling tradeoff is achieved when reconstructing by convex optimization, which is expensive in important large-scale applications. Fast iterative thresholding algorithms have been intensively studied as alternatives to convex optimization for large-scale problems. Unfortunately known fast algorithms offer substantially worse sparsity--undersampling tradeoffs than convex optimization. We introduce a simple costless modification to iterative thresholding making the sparsity--undersampling tradeoff of the new algorithms equivalent to that of the corresponding convex optimization procedures. The new iterative-thresholding algorithms are inspired by belief propagation in graphical models. Our empirical measurements of the sparsity--undersampling tradeoff for the new algorithms agree with theoretical calculations. We show that a state evolution formalism correctly derives the true sparsity--undersampling tradeoff. There is a surprising agreement between earlier calculations based on random convex polytopes and this apparently very different theoretical formalism.},
	author = {David L. Donoho and Arian Maleki and Andrea Montanari},
	doi = {10.1073/pnas.0909892106},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0909892106},
	journal = {Proceedings of the National Academy of Sciences},
	number = {45},
	pages = {18914-18919},
	title = {Message-passing algorithms for compressed sensing},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0909892106},
	volume = {106},
	year = {2009},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0909892106},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0909892106}}

@article{Antenucci+2019,
	abstract = {Approximate message passing algorithm enjoyed considerable attention in the last decade. In this paper we introduce a variant of the AMP algorithm that takes into account glassy nature of the system under consideration. We coin this algorithm as the approximate survey propagation (ASP) and derive it for a class of low-rank matrix estimation problems. We derive the state evolution for the ASP algorithm and prove that it reproduces the one-step replica symmetry breaking (1RSB) fixed-point equations, well-known in physics of disordered systems. Our derivation thus gives a concrete algorithmic meaning to the 1RSB equations that is of independent interest. We characterize the performance of ASP in terms of convergence and mean-squared error as a function of the free Parisi parameter s. We conclude that when there is a model mismatch between the true generative model and the inference model, the performance of AMP rapidly degrades both in terms of MSE and of convergence, while for well-chosen values of the Parisi parameter s ASP converges in a larger regime and can reach lower errors. Among other results, our analysis leads us to a striking hypothesis that whenever s (or other parameters) can be set in such a way that the Nishimori condition M  =  Q  &gt;  0 is restored, then the corresponding algorithm is able to reach mean-squared error as low as the Bayes-optimal error obtained when the model and its parameters are known and exactly matched in the inference procedure. The remaining drawback is that we have not found a procedure that would systematically find a value of s leading to such low errors, this is a challenging problem let for future work.},
	author = {Fabrizio Antenucci and Florent Krzakala and Pierfrancesco Urbani and Lenka Zdeborov{\'a}},
	doi = {10.1088/1742-5468/aafa7d},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	month = {feb},
	number = {2},
	pages = {023401},
	publisher = {IOP Publishing and SISSA},
	title = {Approximate survey propagation for statistical inference},
	url = {https://dx.doi.org/10.1088/1742-5468/aafa7d},
	volume = {2019},
	year = {2019},
	bdsk-url-1 = {https://dx.doi.org/10.1088/1742-5468/aafa7d}}
@article{El-Showk+2012,
  title = {Solving the 3D Ising model with the conformal bootstrap},
  author = {El-Showk, Sheer and Paulos, Miguel F. and Poland, David and Rychkov, Slava and Simmons-Duffin, David and Vichi, Alessandro},
  journal = {Phys. Rev. D},
  volume = {86},
  issue = {2},
  pages = {025022},
  numpages = {17},
  year = {2012},
  month = {Jul},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevD.86.025022},
  url = {https://link.aps.org/doi/10.1103/PhysRevD.86.025022}
}

@article{Bierkens-Kamatani-Roberts2022,
	author = {Joris Bierkens and Kengo Kamatani and Gareth O. Roberts},
	doi = {10.1214/21-AAP1762},
	journal = {The Annals of Applied Probability},
	keywords = {exponential ergodicity, Gaussian process, Markov chain Monte Carlo, Piecewise deterministic Markov processes, weak convergence},
	number = {5},
	pages = {3361 -- 3407},
	publisher = {Institute of Mathematical Statistics},
	title = {{High-dimensional scaling limits of piecewise deterministic sampling algorithms}},
	url = {https://doi.org/10.1214/21-AAP1762},
	volume = {32},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1214/21-AAP1762}}

@article{Deligiannidis+2021,
	author = {George Deligiannidis and Daniel Paulin and Alexandre Bouchard-C{\^o}t{\'e} and Arnaud Doucet},
	doi = {10.1214/20-AAP1659},
	journal = {The Annals of Applied Probability},
	keywords = {Bouncy particle sampler, coupling, hypocoercivity, Randomized Hamiltonian Monte Carlo, weak convergence},
	number = {6},
	pages = {2612 -- 2662},
	publisher = {Institute of Mathematical Statistics},
	title = {{Randomized Hamiltonian Monte Carlo as scaling limit of the bouncy particle sampler and dimension-free convergence rates}},
	url = {https://doi.org/10.1214/20-AAP1659},
	volume = {31},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/20-AAP1659}}

@article{Andrieu+2021,
	author = {Christophe Andrieu and Alain Durmus and Nikolas N{\"u}sken and Julien Roussel},
	doi = {10.1214/20-AAP1653},
	journal = {The Annals of Applied Probability},
	keywords = {geometric convergence, Hypoellipticity, PDMCMC},
	number = {5},
	pages = {2478 -- 2517},
	publisher = {Institute of Mathematical Statistics},
	title = {{Hypocoercivity of piecewise deterministic Markov process-Monte Carlo}},
	url = {https://doi.org/10.1214/20-AAP1653},
	volume = {31},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/20-AAP1653}}
@article{Fabrizio+2019,
  title = {Glassy Nature of the Hard Phase in Inference Problems},
  author = {Antenucci, Fabrizio and Franz, Silvio and Urbani, Pierfrancesco and Zdeborov\'a, Lenka},
  journal = {Phys. Rev. X},
  volume = {9},
  issue = {1},
  pages = {011020},
  numpages = {10},
  year = {2019},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevX.9.011020},
  url = {https://link.aps.org/doi/10.1103/PhysRevX.9.011020}
}

@book{ラトゥール1999,
    author         = {ブルーノ・ラトゥール},
    year           = {1999},
    title          = {科学が作られているとき――人類学的考察},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {産業図書},
    note           = {川崎勝・高田紀代志訳},
}

@phdthesis{本武陽一2017,
    author      = {本武陽一},
    school      = {東京大学},
    title       = {高次元データセットに潜む幾何構造と深層学習 : その解析と大自由度力学系への応用},
    year        = {2017},
    url         = {https://repository.dl.itc.u-tokyo.ac.jp/records/48134},
}

@book{ConvexityAndConcentration2017,
    author         = {},
    editor         = {Eric Carlen and Mokshay Madiman and Elisabeth M. Werner},
    year           = {2017},
    title          = {Convexity and Concentration},
    series         = {The IMA Volumes in Mathematics and its Applications},
    volume         = {161},
    edition        = {},
    url            = {https://doi.org/10.1007/978-1-4939-7005-6},
    publisher      = {Springer New York}
}

@misc{Zhu+2024,
      title={Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond}, 
      author={Zheng Zhu and Xiaofeng Wang and Wangbo Zhao and Chen Min and Nianchen Deng and Min Dou and Yuqi Wang and Botian Shi and Kai Wang and Chi Zhang and Yang You and Zhaoxiang Zhang and Dawei Zhao and Liang Xiao and Jian Zhao and Jiwen Lu and Guan Huang},
      year={2024},
      eprint={2405.03520},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2405.03520}, 
}

@article{Kersten+2004,
	abstract = {We perceive the shapes and material properties of objects quickly and reliably despite the complexity and objective ambiguities of natural images. Typical images are highly complex because they consist of many objects embedded in background clutter. Moreover, the image features of an object are extremely variable and ambiguous owing to the effects of projection, occlusion, background clutter, and illumination. The very success of everyday vision implies neural mechanisms, yet to be understood, that discount irrelevant information and organize ambiguous or noisy local image features into objects and surfaces. Recent work in Bayesian theories of visual perception has shown how complexity may be managed and ambiguity resolved through the task-dependent, probabilistic integration of prior object knowledge with image features.},
	author = {Kersten, Daniel and Mamassian, Pascal and Yuille, Alan},
	doi = {https://doi.org/10.1146/annurev.psych.55.090902.142005},
	issn = {1545-2085},
	journal = {Annual Review of Psychology},
	keywords = {vision},
	number = {Volume 55, 2004},
	pages = {271-304},
	publisher = {Annual Reviews},
	title = {Object Perception as Bayesian Inference},
	type = {Journal Article},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev.psych.55.090902.142005},
	volume = {55},
	year = {2004},
	bdsk-url-1 = {https://www.annualreviews.org/content/journals/10.1146/annurev.psych.55.090902.142005},
	bdsk-url-2 = {https://doi.org/10.1146/annurev.psych.55.090902.142005}}

@article{Yuille-Kersten2006,
	abstract = {We argue that the study of human vision should be aimed at determining how humans perform natural tasks with natural images. Attempts to understand the phenomenology of vision from artificial stimuli, although worthwhile as a starting point, can lead to faulty generalizations about visual systems, because of the enormous complexity of natural images. Dealing with this complexity is daunting, but Bayesian inference on structured probability distributions offers the ability to design theories of vision that can deal with the complexity of natural images, and that use `analysis by synthesis' strategies with intriguing similarities to the brain. We examine these strategies using recent examples from computer vision, and outline some important imlications for cognitive science.},
	author = {Alan Yuille and Daniel Kersten},
	doi = {https://doi.org/10.1016/j.tics.2006.05.002},
	issn = {1364-6613},
	journal = {Trends in Cognitive Sciences},
	note = {Special issue: Probabilistic models of cognition},
	number = {7},
	pages = {301-308},
	title = {{Vision as Bayesian Inference: Analysis by Synthesis?}},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661306001264},
	volume = {10},
	year = {2006},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1364661306001264},
	bdsk-url-2 = {https://doi.org/10.1016/j.tics.2006.05.002}}
@inproceedings{Chen+2017,
title={{Variational Lossy Autoencoder}},
author={Xi Chen and Diederik P. Kingma and Tim Salimans and Yan Duan and Prafulla Dhariwal and John Schulman and Ilya Sutskever and Pieter Abbeel},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=BysvGP5ee}
}
@article{Lawrence2005,
  author  = {Neil Lawrence},
  title   = {Probabilistic Non-linear Principal Component Analysis with Gaussian Process Latent Variable Models},
  journal = {Journal of Machine Learning Research},
  year    = {2005},
  volume  = {6},
  number  = {60},
  pages   = {1783--1816},
  url     = {http://jmlr.org/papers/v6/lawrence05a.html}
}

@article{Bourlard-Kamp1988,
	abstract = {The multilayer perceptron, when working in auto-association mode, is sometimes considered as an interesting candidate to perform data compression or dimensionality reduction of the feature space in information processing applications. The present paper shows that, for auto-association, the nonlinearities of the hidden units are useless and that the optimal parameter values can be derived directly by purely linear techniques relying on singular value decomposition and low rank matrix approximation, similar in spirit to the well-known Karhunen-Lo{\`e}ve transform. This approach appears thus as an efficient alternative to the general error back-propagation algorithm commonly used for training multilayer perceptrons. Moreover, it also gives a clear interpretation of the r{\^o}le of the different parameters.},
	author = {Bourlard, H. and Kamp, Y.},
	date = {1988/09/01},
	date-added = {2024-07-29 12:12:02 +0900},
	date-modified = {2024-07-29 12:12:02 +0900},
	doi = {10.1007/BF00332918},
	id = {Bourlard1988},
	isbn = {1432-0770},
	journal = {Biological Cybernetics},
	number = {4},
	pages = {291--294},
	title = {Auto-association by multilayer perceptrons and singular value decomposition},
	url = {https://doi.org/10.1007/BF00332918},
	volume = {59},
	year = {1988},
	bdsk-url-1 = {https://doi.org/10.1007/BF00332918}}
@inproceedings{Vincent+2008,
author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
title = {Extracting and composing robust features with denoising autoencoders},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390294},
doi = {10.1145/1390156.1390294},
abstract = {Previous work has shown that the difficulties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classification benchmark suite.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1096–1103},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}
@InProceedings{He+2022,
    author    = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll\'ar, Piotr and Girshick, Ross},
    title     = {Masked Autoencoders Are Scalable Vision Learners},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {16000-16009}
}
@inproceedings{Gershman-Goodman2014,
    author          = {Samuel Gershman and Noah Goodman},
    year            = {2014},
    title           = {{Amortized Inference in Probabilistic Reasoning}},
    booktitle       = {Proceedings of the Annual Meating of the Cognitive Science Society},
    volume          = {36},
    pages           = {},
    url             = {https://escholarship.org/uc/item/34j1h7k5}
}
@inproceedings{Bowman+2016,
    title = "Generating Sentences from a Continuous Space",
    author = "Bowman, Samuel R.  and
      Vilnis, Luke  and
      Vinyals, Oriol  and
      Dai, Andrew  and
      Jozefowicz, Rafal  and
      Bengio, Samy",
    editor = "Riezler, Stefan  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 20th {SIGNLL} Conference on Computational Natural Language Learning",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/K16-1002",
    doi = "10.18653/v1/K16-1002",
    pages = "10--21",
}
@inproceedings{Balle+2017,
title={End-to-end Optimized Image Compression},
author={Johannes Ball{\'e} and Valero Laparra and Eero P. Simoncelli},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rJxdQ3jeg}
}
@inproceedings{Yu+2022VIM,
title={Vector-quantized Image Modeling with Improved {VQGAN}},
author={Jiahui Yu and Xin Li and Jing Yu Koh and Han Zhang and Ruoming Pang and James Qin and Alexander Ku and Yuanzhong Xu and Jason Baldridge and Yonghui Wu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=pfNyExj7z2}
}

@inproceedings{Locatello+2019,title	= {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},author	= {Francesco Locatello and Stefan Bauer and Mario Lučić and Gunnar Rätsch and Sylvain Gelly and Bernhard Schölkopf and Olivier Frederic Bachem},year	= {2019},URL	= {http://proceedings.mlr.press/v97/locatello19a.html},note	= {Best Paper Award},booktitle	= {International Conference on Machine Learning}}

@misc{Oord+2019,
      title={Representation Learning with Contrastive Predictive Coding}, 
      author={Aaron van den Oord and Yazhe Li and Oriol Vinyals},
      year={2019},
      eprint={1807.03748},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1807.03748}, 
}


@InProceedings{Hyvarinen+2019,
  title = 	 {Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning},
  author =       {Hyvarinen, Aapo and Sasaki, Hiroaki and Turner, Richard},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {859--868},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/hyvarinen19a/hyvarinen19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/hyvarinen19a.html},
  abstract = 	 {Nonlinear ICA is a fundamental problem for unsupervised representation learning, emphasizing the capacity to recover the underlying latent variables generating the data (i.e., identifiability). Recently, the very first identifiability proofs for nonlinear ICA have been proposed, leveraging the temporal structure of the independent components. Here, we propose a general framework for nonlinear ICA, which, as a special case, can make use of temporal structure. It is based on augmenting the data by an auxiliary variable, such as the time index, the history of the time series, or any other available information. We propose to learn  nonlinear ICA by discriminating between true augmented data, or data in which the auxiliary variable has been randomized.  This enables  the framework to be implemented algorithmically through logistic regression, possibly in a neural network. We provide a comprehensive proof of the identifiability of the model as well as the consistency of our estimation method. The approach not only provides a general theoretical framework combining and generalizing  previously proposed nonlinear ICA models and algorithms, but also brings practical advantages.}
}
@inproceedings{Hyvarinen-Morioka2016,
author = {Hyv\"{a}rinen, Aapo and Morioka, Hiroshi},
title = {Unsupervised feature extraction by time-contrastive learning and nonlinear ICA},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL), finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique — thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3772–3780},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@InProceedings{Khemakhem+2020,
  title = 	 {Variational Autoencoders and Nonlinear ICA: A Unifying Framework},
  author =       {Khemakhem, Ilyes and Kingma, Diederik and Monti, Ricardo and Hyvarinen, Aapo},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2207--2217},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/khemakhem20a/khemakhem20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/khemakhem20a.html},
  abstract = 	 {The framework of variational autoencoders allows us to efficiently learn deep latent-variable models, such that the model’s marginal distribution over observed variables fits the data. Often, we’re interested in going a step further, and want to approximate the true joint distribution over observed and latent variables, including the true prior and posterior distributions over latent variables. This is known to be generally impossible due to unidentifiability of the model. We address this issue by showing that for a broad family of deep latent-variable models, identification of the true joint distribution over observed and latent variables is actually possible up to  very simple transformations, thus achieving a principled and powerful form of disentanglement. Our result requires a factorized prior distribution over the latent variables that is conditioned on an additionally observed variable, such as a class label or almost any other observation. We build on recent developments in nonlinear ICA, which we extend to the case with noisy, undercomplete or discrete observations, integrated in a maximum likelihood framework. The result also trivially contains identifiable flow-based generative models as a special case. }
}

@InProceedings{Hyvarinen-Morioka2017,
  title = 	 {{Nonlinear ICA of Temporally Dependent Stationary Sources}},
  author = 	 {Hyvarinen, Aapo and Morioka, Hiroshi},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {460--469},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/hyvarinen17a/hyvarinen17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/hyvarinen17a.html},
  abstract = 	 {We develop a nonlinear generalization of independent component analysis (ICA) or blind source separation, based on temporal dependencies (e.g. autocorrelations). We introduce a nonlinear generative model where the independent sources are assumed to be temporally dependent, non-Gaussian, and stationary, and we observe arbitrarily nonlinear mixtures of them. We develop a method for estimating the model (i.e. separating the sources) based on logistic regression in a neural network which learns to discriminate between a short temporal window of the data vs. a temporal window of temporally permuted data. We prove that the method estimates the sources for general smooth mixing nonlinearities, assuming the sources have sufficiently strong temporal dependencies, and these dependencies are in a certain way different from dependencies found in Gaussian processes. For Gaussian (and similar) sources, the method estimates the nonlinear part of the mixing. We thus provide the first rigorous and general proof of identifiability of nonlinear ICA for temporally dependent sources, together with a practical method for its estimation.}
}
@ARTICLE{Elias1955,
  author={Elias, P.},
  journal={IRE Transactions on Information Theory}, 
  title={Predictive coding--I}, 
  year={1955},
  volume={1},
  number={1},
  pages={16-24},
  keywords={Transmitters;Predictive coding;Pulse modulation;Entropy;Feeds;Decoding;Error correction codes;Information filtering;Information filters;Bandwidth},
  doi={10.1109/TIT.1955.1055126}}

@article{Rao-Ballard1999,
	abstract = {We describe a model of visual processing in which feedback connections from a higher- to a lower-order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These results suggest that rather than being exclusively feedforward phenomena, nonclassical surround effects in the visual cortex may also result from cortico-cortical feedback as a consequence of the visual system using an efficient hierarchical strategy for encoding natural images.},
	author = {Rao, Rajesh P. N. and Ballard, Dana H.},
	date = {1999/01/01},
	date-added = {2024-07-29 19:49:49 +0900},
	date-modified = {2024-07-29 19:49:49 +0900},
	doi = {10.1038/4580},
	id = {Rao1999},
	isbn = {1546-1726},
	journal = {Nature Neuroscience},
	number = {1},
	pages = {79--87},
	title = {Predictive coding in the visual cortex:  a functional interpretation of some extra-classical receptive-field effects},
	url = {https://doi.org/10.1038/4580},
	volume = {2},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1038/4580}}
@inproceedings{Larochelle+2007,
author = {Larochelle, Hugo and Erhan, Dumitru and Courville, Aaron and Bergstra, James and Bengio, Yoshua},
title = {An empirical evaluation of deep architectures on problems with many factors of variation},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273496.1273556},
doi = {10.1145/1273496.1273556},
abstract = {Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {473–480},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@article{Luxburg2007,
	abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
	author = {von Luxburg, Ulrike},
	date = {2007/12/01},
	date-added = {2024-07-30 10:48:12 +0900},
	date-modified = {2024-07-30 10:48:12 +0900},
	doi = {10.1007/s11222-007-9033-z},
	id = {von Luxburg2007},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {4},
	pages = {395--416},
	title = {A tutorial on spectral clustering},
	url = {https://doi.org/10.1007/s11222-007-9033-z},
	volume = {17},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-007-9033-z}}

@article{Fefferman+2016,
    author          = {Charles Fefferman and Sanjoy Mitter and Hariharan Narayanan},
    year            = {2016},
    title           = {Testing the Manifold Hypothesis},
    journal         = {Journal of the American Mathematical Society},
    volume          = {29},
    number          = {},
    pages           = {983-1049},
    url             = {https://doi.org/10.1090/jams/852}
}

@article{Agrawal+2021,
	author = {Akshay Agrawal and Alnur Ali and Stephen Boyd},
	doi = {10.1561/2200000090},
	issn = {1935-8237},
	journal = {Foundations and Trends{\textregistered} in Machine Learning},
	number = {3},
	pages = {211-378},
	title = {Minimum-Distortion Embedding},
	url = {http://dx.doi.org/10.1561/2200000090},
	volume = {14},
	year = {2021},
	bdsk-url-1 = {http://dx.doi.org/10.1561/2200000090}}

@article{Wilk+2020,
	abstract = {There is an urgent need to better understand the pathophysiology of Coronavirus disease 2019 (COVID-19), the global pandemic caused by SARS-CoV-2, which has infected more than three million people worldwide1. Approximately 20{\%} of patients with COVID-19 develop severe disease and 5{\%} of patients require intensive care2. Severe disease has been associated with changes in peripheral immune activity, including increased levels of pro-inflammatory cytokines3,4 that may be produced by a subset of inflammatory monocytes5,6, lymphopenia7,8 and T cell exhaustion9,10. To elucidate pathways in peripheral immune cells that might lead to immunopathology or protective immunity in severe COVID-19, we applied single-cell RNA sequencing (scRNA-seq) to profile peripheral blood mononuclear cells (PBMCs) from seven patients hospitalized for COVID-19, four of whom had acute respiratory distress syndrome, and six healthy controls. We identify reconfiguration of peripheral immune cell phenotype in COVID-19, including a heterogeneous interferon-stimulated gene signature, HLA class II downregulation and a developing neutrophil population that appears closely related to plasmablasts appearing in patients with acute respiratory failure requiring mechanical ventilation. Importantly, we found that peripheral monocytes and lymphocytes do not express substantial amounts of pro-inflammatory cytokines. Collectively, we provide a cell atlas of the peripheral immune response to severe COVID-19.},
	author = {Wilk, Aaron J. and Rustagi, Arjun and Zhao, Nancy Q. and Roque, Jonasel and Mart{\'\i}nez-Col{\'o}n, Giovanny J. and McKechnie, Julia L. and Ivison, Geoffrey T. and Ranganath, Thanmayi and Vergara, Rosemary and Hollis, Taylor and Simpson, Laura J. and Grant, Philip and Subramanian, Aruna and Rogers, Angela J. and Blish, Catherine A.},
	date = {2020/07/01},
	date-added = {2024-07-30 14:22:55 +0900},
	date-modified = {2024-07-30 14:22:55 +0900},
	doi = {10.1038/s41591-020-0944-y},
	id = {Wilk2020},
	isbn = {1546-170X},
	journal = {Nature Medicine},
	number = {7},
	pages = {1070--1076},
	title = {A single-cell atlas of the peripheral immune response in patients with severe COVID-19},
	url = {https://doi.org/10.1038/s41591-020-0944-y},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1038/s41591-020-0944-y}}

@article{Burges2010,
	author = {Christopher J. C. Burges},
	doi = {10.1561/2200000002},
	issn = {1935-8237},
	journal = {Foundations and Trends{\textregistered} in Machine Learning},
	number = {4},
	pages = {275-365},
	title = {Dimension Reduction: A Guided Tour},
	url = {http://dx.doi.org/10.1561/2200000002},
	volume = {2},
	year = {2010},
	bdsk-url-1 = {http://dx.doi.org/10.1561/2200000002}}

@article{Karhunen-Joutsensalo1995,
	abstract = {We derive and discuss various generalizations of neural PCA (Principal Component Analysis)-type learning algorithms containing nonlinearities using optimization-based approach. Standard PCA arises as an optimal solution to several different information representation problems. We justify that this is essentially due to the fact that the solution is based on the second-order statistics only. If the respective optimization problems are generalized for nonquadratic criteria so that higher-order statistics are taken into account, their solutions will in general be different. The solutions define in a natural way several meaningful extensions of PCA and give a solid foundation for them. In this framework, we study more closely generalizations of the problems of variance maximization and mean-square error minimization. For these problems, we derive gradient-type neural learning algorithms both for symmetric and hierarchic PCA-type networks. As an important special case, the well-known Sanger's generalized Hebbian algorithm (GHA) is shown to emerge from natural optimization problems.},
	author = {Juha Karhunen and Jyrki Joutsensalo},
	doi = {https://doi.org/10.1016/0893-6080(94)00098-7},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Principal components, Optimization, Neural network, Unsupervised learning, Nonlinearity, Robust statistics, Generalized Hebbian algorithm, Oja's rule},
	number = {4},
	pages = {549-562},
	title = {Generalizations of principal component analysis, optimization problems, and neural networks},
	url = {https://www.sciencedirect.com/science/article/pii/0893608094000987},
	volume = {8},
	year = {1995},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0893608094000987},
	bdsk-url-2 = {https://doi.org/10.1016/0893-6080(94)00098-7}}
@ARTICLE{Japkowicz+2000,
  author={Japkowicz, Nathalie and Hanson, Stephen José and Gluck, Mark A.},
  journal={Neural Computation}, 
  title={Nonlinear Autoassociation Is Not Equivalent to PCA}, 
  year={2000},
  volume={12},
  number={3},
  pages={531-545},
  keywords={},
  doi={10.1162/089976600300015691}}
@article{Vincent+2010,
  author  = {Pascal Vincent and Hugo Larochelle and Isabelle Lajoie and Yoshua Bengio and Pierre-Antoine Manzagol},
  title   = {Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion},
  journal = {Journal of Machine Learning Research},
  year    = {2010},
  volume  = {11},
  number  = {110},
  pages   = {3371--3408},
  url     = {http://jmlr.org/papers/v11/vincent10a.html}
}

@InProceedings{Glorot+2011,
  title = 	 {Deep Sparse Rectifier Neural Networks},
  author = 	 {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {315--323},
  year = 	 {2011},
  editor = 	 {Gordon, Geoffrey and Dunson, David and Dudík, Miroslav},
  volume = 	 {15},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Fort Lauderdale, FL, USA},
  month = 	 {11--13 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf},
  url = 	 {https://proceedings.mlr.press/v15/glorot11a.html},
  abstract = 	 {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.}
}

@article{Beyeler+2019,
	abstract = {Supported by recent computational studies, there is increasing evidence that a wide range of neuronal responses can be understood as an emergent property of nonnegative sparse coding (NSC), an efficient population coding scheme based on dimensionality reduction and sparsity constraints. We review evidence that NSC might be employed by sensory areas to efficiently encode external stimulus spaces, by some associative areas to conjunctively represent multiple behaviorally relevant variables, and possibly by the basal ganglia to coordinate movement. In addition, NSC might provide a useful theoretical framework under which to understand the often complex and nonintuitive response properties of neurons in other brain areas. Although NSC might not apply to all brain areas (for example, motor or executive function areas) the success of NSC-based models, especially in sensory areas, warrants further investigation for neural correlates in other regions.},
	author = {Beyeler, Michael AND Rounds, Emily L. AND Carlson, Kristofor D. AND Dutt, Nikil AND Krichmar, Jeffrey L.},
	doi = {10.1371/journal.pcbi.1006908},
	journal = {PLOS Computational Biology},
	month = {06},
	number = {6},
	pages = {1-33},
	publisher = {Public Library of Science},
	title = {Neural correlates of sparse coding and dimensionality reduction},
	url = {https://doi.org/10.1371/journal.pcbi.1006908},
	volume = {15},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pcbi.1006908}}
@inproceedings{Rifai+2011,
author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
title = {Contractive auto-encoders: explicit invariance during feature extraction},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We present in this paper a novel approach for training deterministic auto-encoders. We show that by adding a well chosen penalty term to the classical reconstruction cost function, we can achieve results that equal or surpass those attained by other regularized auto-encoders as well as denoising auto-encoders on a range of datasets. This penalty term corresponds to the Frobenius norm of the Jacobian matrix of the encoder activations with respect to the input. We show that this penalty term results in a localized space contraction which in turn yields robust features on the activation layer. Furthermore, we show how this penalty term is related to both regularized auto-encoders and denoising auto-encoders and how it can be seen as a link between deterministic and non-deterministic auto-encoders. We find empirically that this penalty helps to carve a representation that better captures the local directions of variation dictated by the data, corresponding to a lower-dimensional non-linear manifold, while being more invariant to the vast majority of directions orthogonal to the manifold. Finally, we show that by using the learned features to initialize a MLP, we achieve state of the art classification error on a range of datasets, surpassing other methods of pretraining.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {833–840},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}

@inproceedings{Papamakarios+2017,
	author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Masked Autoregressive Flow for Density Estimation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/6c1da886822c67822bcf3679d04369fa-Paper.pdf}}

@inproceedings{Kingma+2016,
	author = {Diederik P. Kingma and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Improved Variational Inference with Inverse Autoregressive Flow},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/ddeebdeefdb7e7e7a697e1c3e3d8ef54-Paper.pdf}}

@InProceedings{Oord+2018,
  title = 	 {Parallel {W}ave{N}et: Fast High-Fidelity Speech Synthesis},
  author =       {van den Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and van den Driessche, George and Lockhart, Edward and Cobo, Luis and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {3918--3926},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/oord18a/oord18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/oord18a.html},
  abstract = 	 {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today’s massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.}
}
@inproceedings{Jacobsen+2018,
title={i-RevNet: Deep Invertible Networks},
author={Jörn-Henrik Jacobsen and Arnold W.M. Smeulders and Edouard Oyallon},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HJsjkMb0Z},
}

@InProceedings{Behrmann+2019,
  title = 	 {Invertible Residual Networks},
  author =       {Behrmann, Jens and Grathwohl, Will and Chen, Ricky T. Q. and Duvenaud, David and Jacobsen, Joern-Henrik},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {573--582},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/behrmann19a/behrmann19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/behrmann19a.html},
  abstract = 	 {We show that standard ResNet architectures can be made invertible, allowing the same model to be used for classification, density estimation, and generation. Typically, enforcing invertibility requires partitioning dimensions or restricting network architectures. In contrast, our approach only requires adding a simple normalization step during training, already available in standard frameworks. Invertible ResNets define a generative model which can be trained by maximum likelihood on unlabeled data. To compute likelihoods, we introduce a tractable approximation to the Jacobian log-determinant of a residual block. Our empirical evaluation shows that invertible ResNets perform competitively with both state-of-the-art image classifiers and flow-based generative models, something that has not been previously achieved with a single architecture.}
}

@inproceedings{Chen+2019,
	author = {Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David K and Jacobsen, Joern-Henrik},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Residual Flows for Invertible Generative Modeling},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5d0d5594d24f0f955548f0fc0ff83d10-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/5d0d5594d24f0f955548f0fc0ff83d10-Paper.pdf}}

@article{Hutchinson1990,
	annote = {doi: 10.1080/03610919008812866},
	author = {Hutchinson, M. F. },
	date = {1990/01/01},
	date-added = {2024-07-30 16:36:50 +0900},
	date-modified = {2024-07-30 16:36:50 +0900},
	doi = {10.1080/03610919008812866},
	isbn = {0361-0918},
	journal = {Communications in Statistics - Simulation and Computation},
	journal1 = {Communications in Statistics - Simulation and Computation},
	journal2 = {Communications in Statistics - Simulation and Computation},
	month = {01},
	number = {2},
	pages = {433--450},
	publisher = {Taylor \& Francis},
	title = {{A Stochastic Estimator of the Trace of the Influence Matrix for Laplacian Smoothing Splines}},
	type = {doi: 10.1080/03610919008812866},
	url = {https://doi.org/10.1080/03610919008812866},
	volume = {19},
	year = {1990},
	year1 = {1990},
	bdsk-url-1 = {https://doi.org/10.1080/03610919008812866}}

@inbook{Skilling1989,
	abstract = {Often, we need to know some integral property of the eigenvalues {\{}x{\}} of a large N {\texttimes} N symmetric matrix A. For example, determinants det (A) = exp(∑ log (x)) play a role in the classic maximum entropy algorithm [Gull, 1988] . Likewise in physics, the specific heat of a system is a temperature- -dependent sum over the eigenvalues of the Hamiltonian matrix. However, the matrix may be so large that direct O (N3 calculation of all N eigenvalues is prohibited. Indeed, if A is coded as a ``fast'' procedure, then O (N2 operations may also be prohibited.},
	address = {Dordrecht},
	author = {Skilling, John},
	booktitle = {Maximum Entropy and Bayesian Methods: Cambridge, England, 1988},
	doi = {10.1007/978-94-015-7860-8_48},
	editor = {Skilling, J.},
	isbn = {978-94-015-7860-8},
	pages = {455--466},
	publisher = {Springer Netherlands},
	title = {The Eigenvalues of Mega-dimensional Matrices},
	url = {https://doi.org/10.1007/978-94-015-7860-8_48},
	year = {1989},
	bdsk-url-1 = {https://doi.org/10.1007/978-94-015-7860-8_48}}
@inproceedings{vandenBerg+2019,
    title={Sylvester Normalizing Flows for Variational Inference}, 
      author={Rianne van den Berg and Leonard Hasenclever and Jakub M. Tomczak and Max Welling},
      year={2019},
    booktitle       = {Conference on Uncertainty in Artificial Intelligence},
    volume          = {34},
    pages           = {393-402},
    url             = {http://auai.org/uai2018/accepted.php}
}

@article{Tabak-Vanden-Eijnden2010,
	author = {Esteban G. Tabak and Eric Vanden-Eijnden},
	date = {2010/3/1},
	date-added = {2024-07-30 18:00:10 +0900},
	date-modified = {2024-07-30 18:00:10 +0900},
	journal = {Communications in Mathematical Sciences},
	journal1 = {Communications in Mathematical Sciences},
	journal2 = {Communications in Mathematical Sciences},
	month = {3},
	number = {1},
	pages = {217--233 },
	title = {Density estimation by dual ascent of the log-likelihood},
	volume = {8},
	year = {2010},
    url             = {https://doi.org/10.4310/cms.2010.v8.n1.a11},
}

@article{Tabak-Turner2013,
	abstract = {Abstract A new methodology for density estimation is proposed. The methodology, which builds on the one developed by Tabak and Vanden-Eijnden, normalizes the data points through the composition of simple maps. The parameters of each map are determined through the maximization of a local quadratic approximation to the log-likelihood. Various candidates for the elementary maps of each step are proposed; criteria for choosing one includes robustness, computational simplicity, and good behavior in high-dimensional settings. A good choice is that of localized radial expansions, which depend on a single parameter: all the complexity of arbitrary, possibly convoluted probability densities can be built through the composition of such simple maps. {\copyright} 2012 Wiley Periodicals, Inc.},
	author = {Esteban G. Tabak and Turner, Cristina V.},
	doi = {https://doi.org/10.1002/cpa.21423},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.21423},
	journal = {Communications on Pure and Applied Mathematics},
	number = {2},
	pages = {145-164},
	title = {A Family of Nonparametric Density Estimation Algorithms},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
	volume = {66},
	year = {2013},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.21423},
	bdsk-url-2 = {https://doi.org/10.1002/cpa.21423}}

@inproceedings{Chen-Gopinath2000,
	author = {Chen, Scott and Gopinath, Ramesh},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {T. Leen and T. Dietterich and V. Tresp},
	publisher = {MIT Press},
	title = {Gaussianization},
	url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3c947bc2f7ff007b86a9428b74654de5-Paper.pdf},
	volume = {13},
	year = {2000},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2000/file/3c947bc2f7ff007b86a9428b74654de5-Paper.pdf}}
@misc{Rippel-Adams2013,
      title={High-Dimensional Probability Estimation with Deep Density Models}, 
      author={Oren Rippel and Ryan Prescott Adams},
      year={2013},
      eprint={1302.5125},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1302.5125}, 
}
@inproceedings{Dinh+2015,
      title={NICE: Non-linear Independent Components Estimation}, 
      author={Laurent Dinh and David Krueger and Yoshua Bengio},
      year={2015},
    booktitle       = {International Conference on Learning Representations. Poster},
    volume          = {},
    pages           = {},
    url={https://arxiv.org/abs/1410.8516},
}
@inproceedings{Dinh+2017,
title={Density estimation using Real {NVP}},
author={Laurent Dinh and Jascha Sohl-Dickstein and Samy Bengio},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=HkpbnH9lx}
}
@inproceedings{Nalisnick+2019,
      title={Hybrid Models with Deep and Invertible Features}, 
      author={Eric Nalisnick and Akihiro Matsukawa and Yee Whye Teh and Dilan Gorur and Balaji Lakshminarayanan},
      year={2019},
        booktitle = {International Conference on Machine Learning},

      url={https://arxiv.org/abs/1902.02767}, 
}

@inproceedings{Zhang+2020Detection,
	abstract = {Open set recognition requires a classifier to detect samples not belonging to any of the classes in its training set. Existing methods fit a probability distribution to the training samples on their embedding space and detect outliers according to this distribution. The embedding space is often obtained from a discriminative classifier. However, such discriminative representation focuses only on known classes, which may not be critical for distinguishing the unknown classes. We argue that the representation space should be jointly learned from the inlier classifier and the density estimator (served as an outlier detector). We propose the OpenHybrid framework, which is composed of an encoder to encode the input data into a joint embedding space, a classifier to classify samples to inlier classes, and a flow-based density estimator to detect whether a sample belongs to the unknown category. A typical problem of existing flow-based models is that they may assign a higher likelihood to outliers. However, we empirically observe that such an issue does not occur in our experiments when learning a joint representation for discriminative and generative components. Experiments on standard open set benchmarks also reveal that an end-to-end trained OpenHybrid model significantly outperforms state-of-the-art methods and flow-based baselines.},
	address = {Cham},
	author = {Zhang, Hongjie and Li, Ang and Guo, Jie and Guo, Yanwen},
	booktitle = {Computer Vision -- ECCV 2020},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58580-8},
	pages = {102--117},
	publisher = {Springer International Publishing},
	title = {Hybrid Models for Open Set Recognition},
	year = {2020}}

@inproceedings{Charpentier+2020,
	author = {Charpentier, Bertrand and Z\"{u}gner, Daniel and G\"{u}nnemann, Stephan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {1356--1367},
	publisher = {Curran Associates, Inc.},
	title = {Posterior Network: Uncertainty Estimation without OOD Samples via Density-Based Pseudo-Counts},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/0eac690d7059a8de4b48e90f14510391-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/0eac690d7059a8de4b48e90f14510391-Paper.pdf}}
@misc{Prenger+2018,
      title={WaveGlow: A Flow-based Generative Network for Speech Synthesis}, 
      author={Ryan Prenger and Rafael Valle and Bryan Catanzaro},
      year={2018},
      eprint={1811.00002},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/1811.00002}, 
}

@InProceedings{Kim+2019,
  title = 	 {{F}lo{W}ave{N}et : A Generative Flow for Raw Audio},
  author =       {Kim, Sungwon and Lee, Sang-Gil and Song, Jongyoon and Kim, Jaehyeon and Yoon, Sungroh},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3370--3378},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/kim19b/kim19b.pdf},
  url = 	 {https://proceedings.mlr.press/v97/kim19b.html},
  abstract = 	 {Most modern text-to-speech architectures use a WaveNet vocoder for synthesizing high-fidelity waveform audio, but there have been limitations, such as high inference time, in practical applications due to its ancestral sampling scheme. The recently suggested Parallel WaveNet and ClariNet has achieved real-time audio synthesis capability by incorporating inverse autoregressive flow (IAF) for parallel sampling. However, these approaches require a two-stage training pipeline with a well-trained teacher network and can only produce natural sound by using probability distillation along with heavily-engineered auxiliary loss terms. We propose FloWaveNet, a flow-based generative model for raw audio synthesis. FloWaveNet requires only a single-stage training procedure and a single maximum likelihood loss, without any additional auxiliary terms, and it is inherently parallel due to the characteristics of generative flow. The model can efficiently sample raw audio in real-time, with clarity comparable to previous two-stage parallel models. The code and samples for all models, including our FloWaveNet, are available on GitHub.}
}

@inproceedings{Kingma-Dhariwal2018,
	author = {Diederik P. Kingma and Prafulla Dhariwal},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Glow: Generative Flow with Invertible 1x1 Convolutions},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/d139db6a236200b21cc7f752979132d0-Paper.pdf}}
@inproceedings{Kumar+2020,
title={VideoFlow: A Conditional Flow-Based Model for Stochastic Video Generation},
author={Manoj Kumar and Mohammad Babaeizadeh and Dumitru Erhan and Chelsea Finn and Sergey Levine and Laurent Dinh and Durk Kingma},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rJgUfTEYvH}
}

@inproceedings{Tran+2019,
	author = {Tran, Dustin and Vafa, Keyon and Agrawal, Kumar and Dinh, Laurent and Poole, Ben},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Discrete Flows: Invertible Generative Models of Discrete Data},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/e046ede63264b10130007afca077877f-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/e046ede63264b10130007afca077877f-Paper.pdf}}

@InProceedings{Ziegler-Rush2019,
  title = 	 {Latent Normalizing Flows for Discrete Sequences},
  author =       {Ziegler, Zachary and Rush, Alexander},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {7673--7682},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/ziegler19a/ziegler19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/ziegler19a.html},
  abstract = 	 {Normalizing flows are a powerful class of generative models for continuous random variables, showing both strong model flexibility and the potential for non-autoregressive generation. These benefits are also desired when modeling discrete random variables such as text, but directly applying normalizing flows to discrete sequences poses significant additional challenges. We propose a VAE-based generative model which jointly learns a normalizing flow-based distribution in the latent space and a stochastic mapping to an observed discrete space. In this setting, we find that it is crucial for the flow-based distribution to be highly multimodal. To capture this property, we propose several normalizing flow architectures to maximize model flexibility. Experiments consider common discrete sequence tasks of character-level language modeling and polyphonic music generation. Our results indicate that an autoregressive flow-based model can match the performance of a comparable autoregressive baseline, and a non-autoregressive flow-based model can improve generation speed with a penalty to performance.}
}
@misc{Tomczak-Welling2017,
      title={Improving Variational Auto-Encoders using Householder Flow}, 
      author={Jakub M. Tomczak and Max Welling},
      year={2017},
      eprint={1611.09630},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.09630}, 
}
@inproceedings{Louizos-Welling2017,
author = {Louizos, Christos and Welling, Max},
title = {Multiplicative normalizing flows for variational Bayesian neural networks},
year = {2017},
publisher = {JMLR.org},
abstract = {We reinterpret multiplicative noise in neural networks as auxiliary random variables that augment the approximate posterior in a variational setting for Bayesian neural networks. We show that through this interpretation it is both efficient and straightforward to improve the approximation by employing normalizing flows (Rezende \& Mohamed, 2015) while still allowing for local reparametrizations (Kingma et al., 2015) and a tractable lower bound (Ranganath et al., 2015; Maal0e et al., 2016). In experiments we show that with this new approximation we can significantly improve upon classical mean field for Bayesian neural networks on both predictive accuracy as well as predictive uncertainty.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {2218–2227},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}
@article{Muller+2019,
author = {M\"{u}ller, Thomas and Mcwilliams, Brian and Rousselle, Fabrice and Gross, Markus and Nov\'{a}k, Jan},
title = {Neural Importance Sampling},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {5},
issn = {0730-0301},
url = {https://doi.org/10.1145/3341156},
doi = {10.1145/3341156},
abstract = {We propose to use deep neural networks for generating samples in Monte Carlo integration. Our work is based on non-linear independent components estimation (NICE), which we extend in numerous ways to improve performance and enable its application to integration problems. First, we introduce piecewise-polynomial coupling transforms that greatly increase the modeling power of individual coupling layers. Second, we propose to preprocess the inputs of neural networks using one-blob encoding, which stimulates localization of computation and improves inference. Third, we derive a gradient-descent-based optimization for the Kullback-Leibler and the χ2 divergence for the specific application of Monte Carlo integration with unnormalized stochastic estimates of the target distribution. Our approach enables fast and accurate inference and efficient sample generation independently of the dimensionality of the integration domain. We show its benefits on generating natural images and in two applications to light-transport simulation: first, we demonstrate learning of joint path-sampling densities in the primary sample space and importance sampling of multi-dimensional path prefixes thereof. Second, we use our technique to extract conditional directional densities driven by the product of incident illumination and the BSDF in the rendering equation, and we leverage the densities for path guiding. In all applications, our approach yields on-par or higher performance than competing techniques at equal sample count.},
journal = {ACM Trans. Graph.},
month = {oct},
articleno = {145},
numpages = {19},
keywords = {Monte Carlo, deep learning, importance sampling, normalizing flows, path guiding, rendering}
}

@article{Noe+2019,
	abstract = {Molecular dynamics or Monte Carlo methods can be used to sample equilibrium states, but these methods become computationally expensive for complex systems, where the transition from one equilibrium state to another may only occur through rare events. No{\'e} et al. used neural networks and deep learning to generate distributions of independent soft condensed-matter samples at equilibrium (see the Perspective by Tuckerman). Supervised training is used to construct invertible transformations between the coordinates of the complex system of interest and simple Gaussian coordinates of the same dimensionality. Thus, configurations can be sampled in this simpler coordinate system and then transformed back into the complex one using the correct statistical weighting. Science, this issue p. eaaw1147; see also p. 982 By combining deep learning and statistical mechanics, neural networks sample the equilibrium distribution of many-body systems. Computing equilibrium states in condensed-matter many-body systems, such as solvated proteins, is a long-standing challenge. Lacking methods for generating statistically independent equilibrium samples in ``one shot,'' vast computational effort is invested for simulating these systems in small steps, e.g., using molecular dynamics. Combining deep learning and statistical mechanics, we developed Boltzmann generators, which are shown to generate unbiased one-shot equilibrium samples of representative condensed-matter systems and proteins. Boltzmann generators use neural networks to learn a coordinate transformation of the complex configurational equilibrium distribution to a distribution that can be easily sampled. Accurate computation of free-energy differences and discovery of new configurations are demonstrated, providing a statistical mechanics tool that can avoid rare events during sampling without prior knowledge of reaction coordinates.},
	author = {Frank No{\'e} and Simon Olsson and Jonas K{\"o}hler and Hao Wu},
	doi = {10.1126/science.aaw1147},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.aaw1147},
	journal = {Science},
	number = {6457},
	pages = {eaaw1147},
	title = {Boltzmann generators: Sampling equilibrium states of many-body systems with deep learning},
	url = {https://www.science.org/doi/abs/10.1126/science.aaw1147},
	volume = {365},
	year = {2019},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.aaw1147},
	bdsk-url-2 = {https://doi.org/10.1126/science.aaw1147}}

@inproceedings{Hoffmann+2019,
      title={NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural Transport}, 
      author={Matthew Hoffman and Pavel Sountsov and Joshua V. Dillon and Ian Langmore and Dustin Tran and Srinivas Vasudevan},
      year={2019},
    booktitle       = {Symposium on Advances in Approximate Bayesian Inference},
      url={https://arxiv.org/abs/1903.03704}, 
}

@InProceedings{Papamakarios+2019,
  title = 	 {Sequential Neural Likelihood: Fast Likelihood-free Inference with Autoregressive Flows},
  author =       {Papamakarios, George and Sterratt, David and Murray, Iain},
  booktitle = 	 {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages = 	 {837--848},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume = 	 {89},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v89/papamakarios19a/papamakarios19a.pdf},
  url = 	 {https://proceedings.mlr.press/v89/papamakarios19a.html},
  abstract = 	 {We present Sequential Neural Likelihood (SNL), a new method for Bayesian inference in simulator models, where the likelihood is intractable but simulating data from the model is possible. SNL trains an autoregressive flow on simulated data in order to learn a model of the likelihood in the region of high posterior density. A sequential training procedure guides simulations and reduces simulation cost by orders of magnitude. We show that SNL is more robust, more accurate and requires less tuning than related neural-based methods, and we discuss diagnostics for assessing calibration, convergence and goodness-of-fit.}
}

@InProceedings{Greenberg+2019,
  title = 	 {Automatic Posterior Transformation for Likelihood-Free Inference},
  author =       {Greenberg, David and Nonnenmacher, Marcel and Macke, Jakob},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2404--2414},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/greenberg19a/greenberg19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/greenberg19a.html},
  abstract = 	 {How can one perform Bayesian inference on stochastic simulators with intractable likelihoods? A recent approach is to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. However, existing methods are limited to a narrow range of proposal distributions or require importance weighting that can limit performance in practice. Here we present automatic posterior transformation (APT), a new sequential neural posterior estimation method for simulation-based inference. APT can modify the posterior estimate using arbitrary, dynamically updated proposals, and is compatible with powerful flow-based density estimators. It is more flexible, scalable and efficient than previous simulation-based inference techniques. APT can operate directly on high-dimensional time series and image data, opening up new applications for likelihood-free inference.}
}
@article{Hinton2002,
    author = {Hinton, Geoffrey E.},
    title = "{Training Products of Experts by Minimizing Contrastive Divergence}",
    journal = {Neural Computation},
    volume = {14},
    number = {8},
    pages = {1771-1800},
    year = {2002},
    month = {08},
    abstract = "{It is possible to combine multiple latent-variable models of the same data by multiplying their probability distributions together and then renormalizing. This way of combining individual “expert” models makes it hard to generate samples from the combined model but easy to infer the values of the latent variables of each expert, because the combination rule ensures that the latent variables of different experts are conditionally independent when given the data. A product of experts (PoE) is therefore an interesting candidate for a perceptual system in which rapid inference is vital and generation is unnecessary. Training a PoE by maximizing the likelihood of the data is difficult because it is hard even to approximate the derivatives of the renormalization term in the combination rule. Fortunately, a PoE can be trained using a different objective function called “contrastive divergence” whose derivatives with regard to the parameters can be approximated accurately and efficiently. Examples are presented of contrastive divergence learning using several types of expert on several types of data.}",
    issn = {0899-7667},
    doi = {10.1162/089976602760128018},
    url = {https://doi.org/10.1162/089976602760128018},
    eprint = {https://direct.mit.edu/neco/article-pdf/14/8/1771/815447/089976602760128018.pdf},
}
@inproceedings{Tieleman2008,
author = {Tieleman, Tijmen},
title = {Training restricted Boltzmann machines using approximations to the likelihood gradient},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390290},
doi = {10.1145/1390156.1390290},
abstract = {A new algorithm for training Restricted Boltzmann Machines is introduced. The algorithm, named Persistent Contrastive Divergence, is different from the standard Contrastive Divergence algorithms in that it aims to draw samples from almost exactly the model distribution. It is compared to some standard Contrastive Divergence and Pseudo-Likelihood algorithms on the tasks of modeling and classifying various types of data. The Persistent Contrastive Divergence algorithm outperforms the other algorithms, and is equally fast and simple.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {1064–1071},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}
@inproceedings{Tieleman-Hinton2009,
author = {Tieleman, Tijmen and Hinton, Geoffrey},
title = {Using fast weights to improve persistent contrastive divergence},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553506},
doi = {10.1145/1553374.1553506},
abstract = {The most commonly used learning algorithm for restricted Boltzmann machines is contrastive divergence which starts a Markov chain at a data point and runs the chain for only a few iterations to get a cheap, low variance estimate of the sufficient statistics under the model. Tieleman (2008) showed that better learning can be achieved by estimating the model's statistics using a small set of persistent "fantasy particles" that are not reinitialized to data points after each weight update. With sufficiently small weight updates, the fantasy particles represent the equilibrium distribution accurately but to explain why the method works with much larger weight updates it is necessary to consider the interaction between the weight updates and the Markov chain. We show that the weight updates force the Markov chain to mix fast, and using this insight we develop an even faster mixing chain that uses an auxiliary set of "fast weights" to implement a temporary overlay on the energy landscape. The fast weights learn rapidly but also decay rapidly and do not contribute to the normal energy landscape that defines the model.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1033–1040},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{Du-Mordatch2019,
	author = {Du, Yilun and Mordatch, Igor},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {{Implicit Generation and Modeling with Energy Based Models}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/378a063b8fdb1db941e34f4bde584c7d-Paper.pdf}}


@inproceedings{Nijkaml+2019,
	author = {Nijkamp, Erik and Hill, Mitch and Zhu, Song-Chun and Wu, Ying Nian},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Learning Non-Convergent Non-Persistent Short-Run MCMC Toward Energy-Based Model},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/2bc8ae25856bc2a6a1333d1331a3b7a6-Paper.pdf}}
@misc{Song-Kingma2021,
      title={How to Train Your Energy-Based Models}, 
      author={Yang Song and Diederik P. Kingma},
      year={2021},
      eprint={2101.03288},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2101.03288}, 
}

@article{Younes1999,
	annote = {doi: 10.1080/17442509908834179},
	author = {Younes ,Laurent},
	date = {1999/02/01},
	date-added = {2024-07-31 10:40:13 +0900},
	date-modified = {2024-07-31 10:40:13 +0900},
	doi = {10.1080/17442509908834179},
	isbn = {1045-1129},
	journal = {Stochastics and Stochastic Reports},
	journal1 = {Stochastics and Stochastic Reports},
	journal2 = {Stochastics and Stochastic Reports},
	month = {02},
	number = {3-4},
	pages = {177--228},
	publisher = {Taylor \& Francis},
	title = {On the convergence of markovian stochastic algorithms with rapidly decreasing ergodicity rates},
	type = {doi: 10.1080/17442509908834179},
	url = {https://doi.org/10.1080/17442509908834179},
	volume = {65},
	year = {1999},
	year1 = {1999},
	bdsk-url-1 = {https://doi.org/10.1080/17442509908834179}}

@article{Neal1992,
	abstract = {Connectionist learning procedures are presented for ``sigmoid'' and ``noisy-OR'' varieties of probabilistic belief networks. These networks have previously been seen primarily as a means of representing knowledge derived from experts. Here it is shown that the ``Gibbs sampling'' simulation procedure for such networks can support maximum-likelihood learning from empirical data through local gradient ascent. This learning procedure resembles that used for ``Boltzmann machines'', and like it, allows the use of ``hidden'' variables to model correlations between visible variables. Due to the directed nature of the connections in a belief network, however, the ``negative phase'' of Boltzmann machine learning is unnecessary. Experimental results show that, as a result, learning in a sigmoid belief network can be faster than in a Boltzmann machine. These networks have other advantages over Boltzmann machines in pattern classification and decision making applications, are naturally applicable to unsupervised learning problems, and provide a link between work on connectionist learning and work on the representation of expert knowledge.},
	author = {Radford M. Neal},
	doi = {https://doi.org/10.1016/0004-3702(92)90065-6},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	number = {1},
	pages = {71-113},
	title = {Connectionist learning of belief networks},
	url = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	volume = {56},
	year = {1992},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0004370292900656},
	bdsk-url-2 = {https://doi.org/10.1016/0004-3702(92)90065-6}}

@InProceedings{Du+2021,
  title = 	 {Improved Contrastive Divergence Training of Energy-Based Models},
  author =       {Du, Yilun and Li, Shuang and Tenenbaum, Joshua and Mordatch, Igor},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {2837--2848},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/du21b/du21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/du21b.html},
  abstract = 	 {Contrastive divergence is a popular method of training energy-based models, but is known to have difficulties with training stability. We propose an adaptation to improve contrastive divergence training by scrutinizing a gradient term that is difficult to calculate and is often left out for convenience. We show that this gradient term is numerically significant and in practice is important to avoid training instabilities, while being tractable to estimate. We further highlight how data augmentation and multi-scale processing can be used to improve model robustness and generation quality. Finally, we empirically evaluate stability of model architectures and show improved performance on a host of benchmarks and use cases, such as image generation, OOD detection, and compositional generation.}
}
@inproceedings{Zhao+2024,
      title={Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo}, 
      author={Stephen Zhao and Rob Brekelmans and Alireza Makhzani and Roger Grosse},
      year={2024},
	  booksubtitle    = {International Conference on Machine Learning},
      url={https://arxiv.org/abs/2404.17546}, 
}
@article{Teh+2003,
author = {Teh, Yee Whye and Welling, Max and Osindero, Simon and Hinton, Geoffrey E.},
title = {Energy-based models for sparse overcomplete representations},
year = {2003},
issue_date = {12/1/2003},
publisher = {JMLR.org},
volume = {4},
number = {null},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
month = {dec},
pages = {1235–1260},
numpages = {26},
url = {https://www.jmlr.org/papers/v4/teh03a.html}
}


@article{Teh+2006,
	author = {Yee Whye Teh, Michael I Jordan, Matthew J Beal and David M Blei},
	doi = {10.1198/016214506000000302},
	eprint = {https://doi.org/10.1198/016214506000000302},
	journal = {Journal of the American Statistical Association},
	number = {476},
	pages = {1566--1581},
	publisher = {Taylor \& Francis},
	title = {Hierarchical Dirichlet Processes},
	url = {https://doi.org/10.1198/016214506000000302},
	volume = {101},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1198/016214506000000302}}

@inproceedings{Wallach+2009,
author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
title = {Evaluation methods for topic models},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553515},
doi = {10.1145/1553374.1553515},
abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of held-out documents, and propose two alternative methods that are both accurate and efficient.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {1105–1112},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@InProceedings{Xie+2016,
  title = 	 {A Theory of Generative ConvNet},
  author = 	 {Xie, Jianwen and Lu, Yang and Zhu, Song-Chun and Wu, Yingnian},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2635--2644},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/xiec16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/xiec16.html},
  abstract = 	 {We show that a generative random field model, which we call generative ConvNet, can be derived from the commonly used discriminative ConvNet, by assuming a ConvNet for multi-category classification and assuming one of the category is a base category generated by a reference distribution. If we further assume that the non-linearity in the ConvNet is Rectified Linear Unit (ReLU) and the reference distribution is Gaussian white noise, then we obtain a generative ConvNet model that is unique among energy-based models: The model is piecewise Gaussian, and the means of the Gaussian pieces are defined by an auto-encoder, where the filters in the bottom-up encoding become the basis functions in the top-down decoding, and the binary activation variables detected by the filters in the bottom-up convolution process become the coefficients of the basis functions in the top-down deconvolution process. The Langevin dynamics for sampling the generative ConvNet is driven by the reconstruction error of this auto-encoder. The contrastive divergence learning of the generative ConvNet reconstructs the training images by the auto-encoder. The maximum likelihood learning algorithm can synthesize realistic natural image patterns.}
}
@inproceedings{He+2019,
      title={Lagging Inference Networks and Posterior Collapse in Variational Autoencoders}, 
      author={Junxian He and Daniel Spokoyny and Graham Neubig and Taylor Berg-Kirkpatrick},
      year={2019},
      booksubtitle    = {International Conference on Learning Representations},
      url={https://arxiv.org/abs/1901.05534}, 
}
@inproceedings{Hinton-Teh2001,
author = {Hinton, Geoffrey E. and Teh, Yee-Whye},
title = {Discovering multiple constraints that are frequently approximately satisfied},
year = {2001},
isbn = {1558608001},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Some high-dimensional datasets can be modelled by assuming that there are many different linear constraints, each of which is Frequently Approximately Satisfied (FAS) by the data. The probability of a data vector under the model is then proportional to the product of the probabilities of its constraint violations. We describe three methods of learning products of constraints using a heavy-tailed probability distribution for the violations.},
booktitle = {Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence},
pages = {227–234},
numpages = {8},
location = {Seattle, Washington},
series = {UAI'01},
url             = {https://dl.acm.org/doi/abs/10.5555/2074022.2074051},
}
@misc{Finn+2016,
      title={A Connection between Generative Adversarial Networks, Inverse Reinforcement Learning, and Energy-Based Models}, 
      author={Chelsea Finn and Paul Christiano and Pieter Abbeel and Sergey Levine},
      year={2016},
      eprint={1611.03852},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1611.03852}, 
}
@inproceedings{Swersku+2011,
author = {Swersky, Kevin and Ranzato, Marc'Aurelio and Buchman, David and Marlin, Benjamin M. and Freitas, Nandode},
title = {On autoencoders and score matching for energy based models},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We consider estimation methods for the class of continuous-data energy based models (EBMs). Our main result shows that estimating the parameters of an EBM using score matching when the conditional distribution over the visible units is Gaussian corresponds to training a particular form of regularized autoencoder. We show how different Gaussian EBMs lead to different autoencoder architectures, providing deep links between these two families of models. We compare the score matching estimator for the mPoT model, a particular Gaussian EBM, to several other training methods on a variety of tasks including image denoising and unsupervised feature extraction. We show that the regularization function induced by score matching leads to superior classification performance relative to a standard autoencoder. We also show that score matching yields classification results that are indistinguishable from better-known stochastic approximation maximum likelihood estimators.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {1201–1208},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11},
url             = {https://dl.acm.org/doi/10.5555/3104482.3104633},
}

@inproceedings{Koster-Hyvarinen2007,
	abstract = {Capturing regularities in high-dimensional data is an important problem in machine learning and signal processing. Here we present a statistical model that learns a nonlinear representation from the data that reflects abstract, invariant properties of the signal without making requirements about the kind of signal that can be processed. The model has a hierarchy of two layers, with the first layer broadly corresponding to Independent Component Analysis (ICA) and a second layer to represent higher order structure. We estimate the model using the mathematical framework of Score Matching (SM), a novel method for the estimation of non-normalized statistical models. The model incorporates a squaring nonlinearity, which we propose to be suitable for forming a higher-order code of invariances. Additionally the squaring can be viewed as modelling subspaces to capture residual dependencies, which linear models cannot capture.},
	address = {Berlin, Heidelberg},
	author = {K{\"o}ster, Urs and Hyv{\"a}rinen, Aapo},
	booktitle = {Artificial Neural Networks -- ICANN 2007},
	editor = {de S{\'a}, Joaquim Marques and Alexandre, Lu{\'\i}s A. and Duch, W{\l}odzis{\l}aw and Mandic, Danilo},
	isbn = {978-3-540-74695-9},
	pages = {798--807},
	publisher = {Springer Berlin Heidelberg},
	title = {A Two-Layer ICA-Like Model Estimated by Score Matching},
	year = {2007}}

@inproceedings{Koster+2009,
	abstract = {Markov Random Field (MRF) models with potentials learned from the data have recently received attention for learning the low-level structure of natural images. A MRF provides a principled model for whole images, unlike ICA, which can in practice be estimated for small patches only. However, learning the filters in an MRF paradigm has been problematic in the past since it required computationally expensive Monte Carlo methods. Here, we show how MRF potentials can be estimated using Score Matching (SM). With this estimation method we can learn filters of size 12 {\texttimes}12 pixels, considerably larger than traditional ''hand-crafted'' MRF potentials. We analyze the tuning properties of the filters in comparison to ICA filters, and show that the optimal MRF potentials are similar to the filters from an overcomplete ICA model.},
	address = {Berlin, Heidelberg},
	author = {K{\"o}ster, Urs and Lindgren, Jussi T. and Hyv{\"a}rinen, Aapo},
	booktitle = {Independent Component Analysis and Signal Separation},
	editor = {Adali, T{\"u}lay and Jutten, Christian and Romano, Jo{\~a}o Marcos Travassos and Barros, Allan Kardec},
	isbn = {978-3-642-00599-2},
	pages = {515--522},
	publisher = {Springer Berlin Heidelberg},
	title = {Estimating Markov Random Field Potentials for Natural Images},
	year = {2009}}

@InProceedings{Gutmann-Hyvarinen2010,
  title = 	 {Noise-contrastive estimation: A new estimation principle for unnormalized statistical models},
  author = 	 {Gutmann, Michael and Hyvärinen, Aapo},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {297--304},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/gutmann10a.html},
  abstract = 	 {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially generated noise, using the model log-density function in the regression nonlinearity.  We show that this leads to a consistent (convergent) estimator of the parameters, and analyze the asymptotic variance.  In particular, the method is shown to directly work for unnormalized models, i.e. models where the density function does not integrate to one. The normalization constant can be estimated just like any other parameter. For a tractable ICA model, we compare the method with other estimation methods that can be used to learn unnormalized models, including score matching, contrastive divergence, and maximum-likelihood where the normalization constant is estimated with importance sampling. Simulations show that noise-contrastive estimation offers the best trade-off between computational and statistical efficiency. The method is then applied to the modeling of natural images: We show that the method can successfully estimate a large-scale two-layer model and a Markov random field.}
}

@InProceedings{Perpinan-Hinton2005,
  title = 	 {On Contrastive Divergence Learning},
  author =       {Carreira-Perpi{\~n}\'an, Miguel \'A. and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
  pages = 	 {33--40},
  year = 	 {2005},
  editor = 	 {Cowell, Robert G. and Ghahramani, Zoubin},
  volume = 	 {R5},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--08 Jan},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/r5/carreira-perpinan05a/carreira-perpinan05a.pdf},
  url = 	 {https://proceedings.mlr.press/r5/carreira-perpinan05a.html},
  note =         {Reissued by PMLR on 30 March 2021.}
}

@inproceedings{Kingma-LeCun2010,
	author = {Durk P Kingma and Yann LeCun},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Lafferty and C. Williams and J. Shawe-Taylor and R. Zemel and A. Culotta},
	publisher = {Curran Associates, Inc.},
	title = {Regularized estimation of image statistics by Score Matching},
	url = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf},
	volume = {23},
	year = {2010},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2010/file/6f3e29a35278d71c7f65495871231324-Paper.pdf}}
@ARTICLE{Drucker-LeCun1992,
  author={Drucker, H. and Le Cun, Y.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Improving generalization performance using double backpropagation}, 
  year={1992},
  volume={3},
  number={6},
  pages={991-997},
  keywords={Testing;Backpropagation algorithms;Jacobian matrices;Neurons;Signal to noise ratio;Neural networks},
  doi={10.1109/72.165600}}
@inproceedings{Song+2019,
      title={{Sliced Score Matching: A Scalable Approach to Density and Score Estimation}},
      author={Yang Song and Sahaj Garg and Jiaxin Shi and Stefano Ermon},
      year={2019},
      booksubtitle    = {Uncertainty in Artificial Intelligence},
      url={https://arxiv.org/abs/1905.07088}, 
}
@ARTICLE{Hyvarinen2007,
  author={Hyvarinen, Aapo},
  journal={IEEE Transactions on Neural Networks}, 
  title={Connections Between Score Matching, Contrastive Divergence, and Pseudolikelihood for Continuous-Valued Variables}, 
  year={2007},
  volume={18},
  number={5},
  pages={1529-1531},
  keywords={Samarium;Statistical analysis;Probability density function;Parameter estimation;Monte Carlo methods;Computer science;Information technology;Normalization constant;partition function;statistical estimation},
  doi={10.1109/TNN.2007.895819}}
@inproceedings{Lyu2009,
author = {Lyu, Siwei},
title = {Interpretation and generalization of score matching},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Score matching is a recently developed parameter learning method that is particularly effective to complicated high dimensional density models with intractable partition functions. In this paper, we study two issues that have not been completely resolved for score matching. First, we provide a formal link between maximum likelihood and score matching. Our analysis shows that score matching finds model parameters that are more robust with noisy training data. Second, we develop a generalization of score matching. Based on this generalization, we further demonstrate an extension of score matching to models of discrete data.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {359–366},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}
@inproceedings{Gutmann-Hirayama2011,
author = {Gutmann, Michael U. and Hirayama, Jun-ichiro},
title = {Bregman divergence as general framework to estimate unnormalized statistical models},
year = {2011},
isbn = {9780974903972},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We show that the Bregman divergence provides a rich framework to estimate unnormalized statistical models for continuous or discrete random variables, that is, models which do not integrate or sum to one, respectively. We prove that recent estimation methods such as noise-contrastive estimation, ratio matching, and score matching belong to the proposed framework, and explain their interconnection based on supervised learning. Further, we discuss the role of boosting in un-supervised learning.},
booktitle = {Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence},
pages = {283–290},
numpages = {8},
location = {Barcelona, Spain},
series = {UAI'11}
}

@InProceedings{Chwialkowski+2016,
  title = 	 {A Kernel Test of Goodness of Fit},
  author = 	 {Chwialkowski, Kacper and Strathmann, Heiko and Gretton, Arthur},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2606--2615},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/chwialkowski16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/chwialkowski16.html},
  abstract = 	 {We propose a nonparametric statistical test for goodness-of-fit: given a set of samples, the test determines how likely it is that these were generated from a target density function. The measure of goodness-of-fit is a divergence constructed via Stein’s method using functions from a Reproducing Kernel Hilbert Space. Our test statistic is based on an empirical estimate of this divergence, taking the form of a V-statistic in terms of the log gradients of the target density and the kernel. We derive a statistical test, both for i.i.d. and non-i.i.d. samples, where we estimate the null distribution quantiles using a wild bootstrap procedure. We apply our test to quantifying convergence of approximate Markov Chain Monte Carlo methods, statistical model criticism, and evaluating quality of fit vs model complexity in nonparametric density estimation.}
}

@InProceedings{Liu+2016,
  title = 	 {A Kernelized Stein Discrepancy for Goodness-of-fit Tests},
  author = 	 {Liu, Qiang and Lee, Jason and Jordan, Michael},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {276--284},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/liub16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/liub16.html},
  abstract = 	 {We derive a new discrepancy statistic for measuring differences between two probability distributions based on combining Stein’s identity and the reproducing kernel Hilbert space theory. We apply our result to test how well a probabilistic model fits a set of observations, and derive a new class of powerful goodness-of-fit tests that are widely applicable for complex and high dimensional distributions, even for those with computationally intractable normalization constants. Both theoretical and empirical properties of our methods are studied thoroughly.}
}
@misc{McAllester2023,
      title={On the Mathematics of Diffusion Models}, 
      author={David McAllester},
      year={2023},
      eprint={2301.11108},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2301.11108}, 
}
@article{Yang+2023,
author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
year = {2023},
issue_date = {April 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {56},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3626235},
doi = {10.1145/3626235},
abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language processing, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github:},
journal = {ACM Comput. Surv.},
month = {nov},
articleno = {105},
numpages = {39},
keywords = {stochastic differential equations, score-based generative models, diffusion models, Generative models}
}
@unpublished{Kreis+2022,
    author = {Karsten Kreis and Ruiqi Gao and Arash Vahdat},
    year   = {2022},
    title  = {Denoising Diffusion-based Generative Modeling: Foundations and Applications},
    url    = {https://cvpr2022-tutorial-diffusion-models.github.io/},
    booksubtitle    = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}
@unpublished{Song+2023Tutorial,
    author = {Jiaming Song and Chenlin Meng and Arash Vahdat},
    year   = {2023},
    title  = {Denoising Diffusion Models: A Generative Learning Big Bang},
    url    = {https://cvpr.thecvf.com/virtual/2023/tutorial/18546},
    booksubtitle    = {The IEEE/CVF Conference on Computer Vision and Pattern Recognition},
}
@INPROCEEDINGS{Choi+2022,
  author={Choi, Jooyoung and Lee, Jungbeom and Shin, Chaehun and Kim, Sungwon and Kim, Hyunwoo and Yoon, Sungroh},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Perception Prioritized Training of Diffusion Models}, 
  year={2022},
  volume={},
  number={},
  pages={11462-11471},
  keywords={Training;Visualization;Computational modeling;Noise reduction;Data models;Image restoration;Pattern recognition;Image and video synthesis and generation},
  doi={10.1109/CVPR52688.2022.01118}}
@inproceedings{Kingma+2021,
author = {Kingma, Diederik P. and Salimans, Tim and Poole, Ben and Ho, Jonathan},
title = {Variational diffusion models},
year = {2021},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Diffusion-based generative models have demonstrated a capacity for perceptually impressive synthesis, but can they also be great likelihood-based models? We answer this in the affirmative, and introduce a family of diffusion-based generative models that obtain state-of-the-art likelihoods on standard image density estimation benchmarks. Unlike other diffusion-based models, our method allows for efficient optimization of the noise schedule jointly with the rest of the model. We show that the variational lower bound (VLB) simplifies to a remarkably short expression in terms of the signal-to-noise ratio of the diffused data, thereby improving our theoretical understanding of this model class. Using this insight, we prove an equivalence between several models proposed in the literature. In addition, we show that the continuous-time VLB is invariant to the noise schedule, except for the signal-to-noise ratio at its endpoints. This enables us to learn a noise schedule that minimizes the variance of the resulting VLB estimator, leading to faster optimization. Combining these advances with architectural improvements, we obtain state-of-the-art likelihoods on image density estimation benchmarks, outperforming autoregressive models that have dominated these benchmarks for many years, with often significantly faster optimization. In addition, we show how to use the model as part of a bits-back compression scheme, and demonstrate lossless compression rates close to the theoretical optimum.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {1660},
numpages = {12},
series = {NIPS '21},
url             = {https://proceedings.neurips.cc/paper/2021/hash/b578f2a52a0229873fefc2a4b06377fa-Abstract.html},
}
@inproceedings{Salimans-Ho2021,
title={Should {EBM}s model the energy or the score?},
author={Tim Salimans and Jonathan Ho},
booktitle={Energy Based Models Workshop - ICLR 2021},
year={2021},
url={https://openreview.net/forum?id=9AS-TF2jRNb}
}

@article{Anderson1982,
	abstract = {Reverse-time stochastic diffusion equation models are defined and it is shown how most processes defined via a forward-time or conventional diffusion equation model have an associated reverse-time model.},
	author = {Brian D.O. Anderson},
	doi = {https://doi.org/10.1016/0304-4149(82)90051-5},
	issn = {0304-4149},
	journal = {Stochastic Processes and their Applications},
	number = {3},
	pages = {313-326},
	title = {Reverse-time diffusion equation models},
	url = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	volume = {12},
	year = {1982},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0304414982900515},
	bdsk-url-2 = {https://doi.org/10.1016/0304-4149(82)90051-5}}

@article{Haussmann-Pardoux1986,
	author = {U. G. Haussmann and E. Pardoux},
	doi = {10.1214/aop/1176992362},
	journal = {The Annals of Probability},
	keywords = {diffusion process, Kolmogorov equation, Markov process, Martingale problem, Time reversal},
	number = {4},
	pages = {1188 -- 1205},
	publisher = {Institute of Mathematical Statistics},
	title = {{Time Reversal of Diffusions}},
	url = {https://doi.org/10.1214/aop/1176992362},
	volume = {14},
	year = {1986},
	bdsk-url-1 = {https://doi.org/10.1214/aop/1176992362}}
@inproceedings{Karras+2022,
title={Elucidating the Design Space of Diffusion-Based Generative Models},
author={Tero Karras and Miika Aittala and Timo Aila and Samuli Laine},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=k7FuTOWMOc7}
}
@ARTICLE {Croitoru+2023,
author = {F. Croitoru and V. Hondru and R. Ionescu and M. Shah},
journal = {IEEE Transactions on Pattern Analysis &amp; Machine Intelligence},
title = {Diffusion Models in Vision: A Survey},
year = {2023},
volume = {45},
number = {09},
issn = {1939-3539},
pages = {10850-10869},
abstract = {Denoising diffusion models represent a recent emerging topic in computer vision, demonstrating remarkable results in the area of generative modeling. A diffusion model is a deep generative model that is based on two stages, a forward diffusion stage and a reverse diffusion stage. In the forward diffusion stage, the input data is gradually perturbed over several steps by adding Gaussian noise. In the reverse stage, a model is tasked at recovering the original input data by learning to gradually reverse the diffusion process, step by step. Diffusion models are widely appreciated for the quality and diversity of the generated samples, despite their known computational burdens, i.e., low speeds due to the high number of steps involved during sampling. In this survey, we provide a comprehensive review of articles on denoising diffusion models applied in vision, comprising both theoretical and practical contributions in the field. First, we identify and present three generic diffusion modeling frameworks, which are based on denoising diffusion probabilistic models, noise conditioned score networks, and stochastic differential equations. We further discuss the relations between diffusion models and other deep generative models, including variational auto-encoders, generative adversarial networks, energy-based models, autoregressive models and normalizing flows. Then, we introduce a multi-perspective categorization of diffusion models applied in computer vision. Finally, we illustrate the current limitations of diffusion models and envision some interesting directions for future research.},
keywords = {computational modeling;mathematical models;noise reduction;data models;computer vision;training;task analysis},
doi = {10.1109/TPAMI.2023.3261988},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {sep},
url             = {https://doi.ieeecomputersociety.org/10.1109/TPAMI.2023.3261988},
}

@ARTICLE{Cao+2024,
  author={Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
  journal={IEEE Transactions on Knowledge and Data Engineering}, 
  title={A Survey on Generative Diffusion Models}, 
  year={2024},
  volume={36},
  number={7},
  pages={2814-2830},
  keywords={Mathematical models;Kernel;Computational modeling;Training;Surveys;Noise reduction;Markov processes;Diffusion model;deep generative model;diffusion algorithm;diffusion applications},
  doi={10.1109/TKDE.2024.3361474}}
@inproceedings{JiamingSong+2021,
title={Denoising Diffusion Implicit Models},
author={Jiaming Song and Chenlin Meng and Stefano Ermon},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=St1giarCHLP}
}
@inproceedings{Gao+2021,
title={Learning Energy-Based Models by Diffusion Recovery Likelihood},
author={Ruiqi Gao and Yang Song and Ben Poole and Ying Nian Wu and Diederik P Kingma},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=v_1Soh8QUNc}
}
@inproceedings{Xiao+2021,
title={Tackling the Generative Learning Trilemma with Denoising Diffusion {GAN}s},
author={Zhisheng Xiao and Karsten Kreis and Arash Vahdat},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=JprM0p-q0Co}
}
@inproceedings{Salimans-Ho2022,
title={Progressive Distillation for Fast Sampling of Diffusion Models},
author={Tim Salimans and Jonathan Ho},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=TIdIXIpzhoI}
}
@inproceedings{Vahdat+2021,
	author = {Vahdat, Arash and Kreis, Karsten and Kautz, Jan},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {11287--11302},
	publisher = {Curran Associates, Inc.},
	title = {{Score-based Generative Modeling in Latent Space}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/5dca4c6b9e244d24a30b4c45601d9720-Paper.pdf}}

@article{Pandey+2022,
title={Diffuse{VAE}: Efficient, Controllable and High-Fidelity Generation from Low-Dimensional Latents},
author={Kushagra Pandey and Avideep Mukherjee and Piyush Rai and Abhishek Kumar},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=ygoNPRiLxw},
note={}
}

@InProceedings{Nichol-Dhariwal2021,
  title = 	 {{Improved Denoising Diffusion Probabilistic Models}},
  author =       {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {8162--8171},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/nichol21a.html},
  abstract = 	 {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.}
}
@inproceedings{Ho-Salimans2021,
title={Classifier-Free Diffusion Guidance},
author={Jonathan Ho and Tim Salimans},
booktitle={NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications},
year={2021},
url={https://openreview.net/forum?id=qw8AKxfYbI}
}
@article{Ho+2022,
  author  = {Jonathan Ho and Chitwan Saharia and William Chan and David J. Fleet and Mohammad Norouzi and Tim Salimans},
  title   = {Cascaded Diffusion Models for High Fidelity Image Generation},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {47},
  pages   = {1--33},
  url     = {http://jmlr.org/papers/v23/21-0635.html}
}
@misc{Saharia+2022SIGGRAPH,
title={{Palette: Image-to-Image Diffusion Models}},
author={Chitwan Saharia and William Chan and Huiwen Chang and Chris A. Lee and Jonathan Ho and Tim Salimans and David J. Fleet and Mohammad Norouzi},
year={2022},
url={https://openreview.net/forum?id=FPGs276lUeq}
}
@ARTICLE{Saharia+2023,
  author={Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Image Super-Resolution via Iterative Refinement}, 
  year={2023},
  volume={45},
  number={4},
  pages={4713-4726},
  keywords={Noise reduction;Superresolution;Task analysis;Iterative methods;Data models;Faces;Diffusion processes;Image super-resolution;diffusion models;deep generative models},
  doi={10.1109/TPAMI.2022.3204461}}

@inproceedings{Austin+2021,
	author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and van den Berg, Rianne},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {17981--17993},
	publisher = {Curran Associates, Inc.},
	title = {{Structured Denoising Diffusion Models in Discrete State-Spaces}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/958c530554f78bcd8e97125b70e6973d-Paper.pdf}}
@InProceedings{Chang+2022,
    author    = {Chang, Huiwen and Zhang, Han and Jiang, Lu and Liu, Ce and Freeman, William T.},
    title     = {MaskGIT: Masked Generative Image Transformer},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {11315-11325}
}

@article{Heng+2024,
	author = {Jeremy Heng and Valentin De Bortoli and Arnaud Doucet},
	doi = {10.1214/23-STS908},
	journal = {Statistical Science},
	keywords = {Optimal transport, Schr{\"o}dinger bridge, score matching, Stochastic differential equation, Time reversal},
	number = {1},
	pages = {90 -- 99},
	publisher = {Institute of Mathematical Statistics},
	title = {{Diffusion Schr{\"o}dinger Bridges for Bayesian Computation}},
	url = {https://doi.org/10.1214/23-STS908},
	volume = {39},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1214/23-STS908}}
@inproceedings{Chung+2023,
title={Diffusion Posterior Sampling for General Noisy Inverse Problems},
author={Hyungjin Chung and Jeongsol Kim and Michael Thompson Mccann and Marc Louis Klasky and Jong Chul Ye},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=OnD9zGAGT0k}
}

@InProceedings{Song+2023,
  title = 	 {Loss-Guided Diffusion Models for Plug-and-Play Controllable Generation},
  author =       {Song, Jiaming and Zhang, Qinsheng and Yin, Hongxu and Mardani, Morteza and Liu, Ming-Yu and Kautz, Jan and Chen, Yongxin and Vahdat, Arash},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {32483--32498},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/song23k/song23k.pdf},
  url = 	 {https://proceedings.mlr.press/v202/song23k.html},
  abstract = 	 {We consider guiding denoising diffusion models with general differentiable loss functions in a plug-and-play fashion, enabling controllable generation without additional training. This paradigm, termed Loss-Guided Diffusion (LGD), can easily be integrated into all diffusion models and leverage various efficient samplers. Despite the benefits, the resulting guidance term is, unfortunately, an intractable integral and needs to be approximated. Existing methods compute the guidance term based on a point estimate. However, we show that such approaches have significant errors over the scale of the approximations. To address this issue, we propose a Monte Carlo method that uses multiple samples from a suitable distribution to reduce bias. Our method is effective in various synthetic and real-world settings, including image super-resolution, text or label-conditional image generation, and controllable motion synthesis. Notably, we show how our method can be applied to control a pretrained motion diffusion model to follow certain paths and avoid obstacles that are proven challenging to prior methods.}
}

@InProceedings{Shi+2022,
  title = 	 {Conditional simulation using diffusion {S}chr{ö}dinger bridges},
  author =       {Shi, Yuyang and De Bortoli, Valentin and Deligiannidis, George and Doucet, Arnaud},
  booktitle = 	 {Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1792--1802},
  year = 	 {2022},
  editor = 	 {Cussens, James and Zhang, Kun},
  volume = 	 {180},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--05 Aug},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v180/shi22a/shi22a.pdf},
  url = 	 {https://proceedings.mlr.press/v180/shi22a.html},
  abstract = 	 {Denoising diffusion models have recently emerged as a powerful class of generative models. They provide state-of-the-art results, not only for unconditional simulation, but also when used to solve conditional simulation problems arising in a wide range of inverse problems. A limitation of these models is that they are computationally intensive at generation time as they require simulating a diffusion process over a long time horizon. When performing unconditional simulation, a Schr{ö}dinger bridge formulation of generative modeling leads to a theoretically grounded algorithm shortening generation time which is complementary to other proposed acceleration techniques. We extend the Schrödinger bridge framework to conditional simulation. We demonstrate this novel methodology on various applications including image super-resolution, optimal filtering for state-space models and the refinement of pre-trained networks. Our code can be found at https://github.com/vdeborto/cdsb.}
}

@inproceedings{DeBortoli+2021,
	author = {De Bortoli, Valentin and Thornton, James and Heng, Jeremy and Doucet, Arnaud},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {17695--17709},
	publisher = {Curran Associates, Inc.},
	title = {Diffusion Schr\"{o}dinger Bridge with Applications to Score-Based Generative Modeling},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/940392f5f32a7ade1cc201767cf83e31-Paper.pdf}}

@InProceedings{Kurras2015,
  title = 	 {{Symmetric Iterative Proportional Fitting}},
  author = 	 {Kurras, Sven},
  booktitle = 	 {Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {526--534},
  year = 	 {2015},
  editor = 	 {Lebanon, Guy and Vishwanathan, S. V. N.},
  volume = 	 {38},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {San Diego, California, USA},
  month = 	 {09--12 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v38/kurras15.pdf},
  url = 	 {https://proceedings.mlr.press/v38/kurras15.html},
  abstract = 	 {Iterative Proportional Fitting (IPF) generates from an input matrix W a sequence of matrices that converges, under certain conditions, to a specific limit matrix W*. This limit is the relative-entropy nearest solution to W among all matrices of prescribed row marginals r and column marginals c. We prove this known fact by a novel strategy that contributes a pure algorithmic intuition. Then we focus on the symmetric setting: W=W’ and r=c. Since IPF inherently generates non-symmetric matrices, we introduce two symmetrized variants of IPF. We prove convergence for both of them. Further, we give a novel characterization for the existence of W* in terms of expansion properties of the undirected weighted graph represented by W. Finally, we show how our results contribute to recent work in machine learning.}
}
@article{Sinkhorn1967,
 ISSN = {00029890, 19300972},
 URL = {http://www.jstor.org/stable/2314570},
 author = {Richard Sinkhorn},
 journal = {The American Mathematical Monthly},
 number = {4},
 pages = {402--405},
 publisher = {[Taylor & Francis, Ltd., Mathematical Association of America]},
 title = {Diagonal Equivalence to Matrices with Prescribed Row and Column Sums},
 urldate = {2024-08-04},
 volume = {74},
 year = {1967}
}

@article{Sinkhorn-Knopp1967,
	author = {Richard Sinkhorn and Paul Knopp},
	journal = {Pacific Journal of Mathematics},
	number = {2},
	pages = {343 -- 348},
	publisher = {Pacific Journal of Mathematics, A Non-profit Corporation},
	title = {{Concerning Nonnegative Matrices and Doubly Stochastic Matrices}},
	volume = {21},
	year = {1967},
  url             = {https://projecteuclid.org/journals/pacific-journal-of-mathematics/volume-21/issue-2/Concerning-nonnegative-matrices-and-doubly-stochastic-matrices/pjm/1102992505.full},
}

@article{Fortet1940,
    author          = {Robert Fortet},
    year            = {1940},
    title           = {{Résolution d'un système d'équations de M. Schrödinger}},
    journal         = {Journal de Mathématiques Pures et Appliquées, Series 9},
    volume          = {19},
    number          = {1-4},
    pages           = {83-105},
    url             = {http://www.numdam.org/item/JMPA_1940_9_19_1-4_83_0/}
}
@article{Deming-Stephan1940,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2235722},
 author = {W. Edwards Deming and Frederick F. Stephan},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {427--444},
 publisher = {Institute of Mathematical Statistics},
 title = {On a Least Squares Adjustment of a Sampled Frequency Table When the Expected Marginal Totals are Known},
 urldate = {2024-08-04},
 volume = {11},
 year = {1940}
}
@article{Kullback1968,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239692},
 author = {S. Kullback},
 journal = {The Annals of Mathematical Statistics},
 number = {4},
 pages = {1236--1243},
 publisher = {Institute of Mathematical Statistics},
 title = {Probability Densities with Given Marginals},
 urldate = {2024-08-04},
 volume = {39},
 year = {1968}
}
@article{Ireland-Kullback1968,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334462},
 abstract = {In its simplest formulation the problem considered is to estimate the cell probabilities pij of an r × c contingency table for which the marginal probabilities $p_{i\ldot}$ and $p_{\ldot j}$ are known and fixed, so as to minimize ΣΣpijln (pij/πij), where πij are the corresponding entries in a given contingency table. An iterative procedure is given for determining the estimates and it is shown that the estimates are BAN, and that the iterative procedure is convergent. A summary of results for a four-way contingency table is given. An illustrative example is given.},
 author = {C. T. Ireland and S. Kullback},
 journal = {Biometrika},
 number = {1},
 pages = {179--188},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Contingency Tables with Given Marginals},
 urldate = {2024-08-04},
 volume = {55},
 year = {1968}
}
@inproceedings{Vargas-Grathwohl-Doucet2023,
title={{Denoising Diffusion Samplers}},
author={Francisco Vargas and Will Sussman Grathwohl and Arnaud Doucet},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=8pvnfTAbu1f}
}

@inproceedings{Follmer1985,
	abstract = {We introduce an entropy technique which allows to treat some infinite-dimensional extensions of the classical duality equations for the time reversal of diffusion processes.},
	address = {Berlin, Heidelberg},
	author = {F{\"o}llmer, H.},
	booktitle = {Stochastic Differential Systems Filtering and Control},
	editor = {Metivier, M. and Pardoux, E.},
	isbn = {978-3-540-39253-8},
	pages = {156--163},
	publisher = {Springer Berlin Heidelberg},
	title = {An entropy approach to the time reversal of diffusion processes},
	year = {1985}}

@InProceedings{Barr+2020,
  title = 	 {Quantum Ground States from Reinforcement Learning},
  author =       {Barr, Ariel and Gispen, Willem and Lamacraft, Austen},
  booktitle = 	 {Proceedings of The First Mathematical and Scientific Machine Learning Conference},
  pages = 	 {635--653},
  year = 	 {2020},
  editor = 	 {Lu, Jianfeng and Ward, Rachel},
  volume = 	 {107},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v107/barr20a/barr20a.pdf},
  url = 	 {https://proceedings.mlr.press/v107/barr20a.html},
  abstract = 	 {  Finding the ground state of a quantum mechanical system can be formulated as an optimal control problem. In this formulation, the drift of the optimally controlled process is chosen to match the distribution of paths in the Feynman–Kac (FK) representation of the solution of the imaginary time Schrödinger equation. This provides a variational principle that can be used for reinforcement learning of a neural representation of the drift. Our approach is a drop-in replacement for path integral Monte Carlo, learning an optimal importance sampler for the FK trajectories. We demonstrate the applicability of our approach to several problems of one-, two-, and many-particle physics.}
}
@inproceedings{Zhang+2021,
title={Sampling via Controlled Stochastic Dynamical Systems},
author={Benjamin Zhang and Tuhin Sahai and Youssef Marzouk},
booktitle={I (Still) Can't Believe It's Not Better! NeurIPS 2021 Workshop},
year={2021},
url={https://openreview.net/forum?id=dHruzYDH719}
}
@article{DeBortoli2022,
title={Convergence of denoising diffusion models under the manifold hypothesis},
author={Valentin De Bortoli},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=MhK5aXo3gB},
note={Expert Certification}
}
@misc{Montanari-Wu2024,
      title={Posterior Sampling from the Spiked Models via Diffusion Processes}, 
      author={Andrea Montanari and Yuchen Wu},
      year={2023},
      eprint={2304.11449},
      archivePrefix={arXiv},
      primaryClass={math.ST},
      url={https://arxiv.org/abs/2304.11449}, 
}


@article{Crooks1998,
	abstract = {An equality has recently been shown relating the free energy difference between two equilibrium ensembles of a system and an ensemble average of the work required to switch between these two configurations. In the present paper it is shown that this result can be derived under the assumption that the system's dynamics is Markovian and microscopically reversible.},
	author = {Crooks, Gavin E. },
	date = {1998/03/01},
	date-added = {2024-08-05 13:48:11 +0900},
	date-modified = {2024-08-05 13:48:11 +0900},
	doi = {10.1023/A:1023208217925},
	id = {Crooks1998},
	isbn = {1572-9613},
	journal = {Journal of Statistical Physics},
	number = {5},
	pages = {1481--1487},
	title = {{Nonequilibrium Measurements of Free Energy Differences for Microscopically Reversible Markovian Systems}},
	url = {https://doi.org/10.1023/A:1023208217925},
	volume = {90},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1023/A:1023208217925}}
@article{Jarzynski1997Equality,
  title = {{Nonequilibrium Equality for Free Energy Differences}},
  author = {Jarzynski, C.},
  journal = {Phys. Rev. Lett.},
  volume = {78},
  issue = {14},
  pages = {2690--2693},
  numpages = {0},
  year = {1997},
  month = {Apr},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.78.2690},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.78.2690}
}
@article{Jarzynski1997MasterEquation,
  title = {Equilibrium free-energy differences from nonequilibrium measurements: A master-equation approach},
  author = {Jarzynski, C.},
  journal = {Phys. Rev. E},
  volume = {56},
  issue = {5},
  pages = {5018--5035},
  numpages = {0},
  year = {1997},
  month = {Nov},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.56.5018},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.56.5018}
}
@inproceedings{Doucet+2022,
title={Score-Based Diffusion meets Annealed Importance Sampling},
author={Arnaud Doucet and Will Sussman Grathwohl and Alexander G. D. G. Matthews and Heiko Strathmann},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=9cU2iW3bz0}
}

@inproceedings{Wu+2020,
	author = {Wu, Hao and K\"{o}hler, Jonas and Noe, Frank},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {5933--5944},
	publisher = {Curran Associates, Inc.},
	title = {{Stochastic Normalizing Flows}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/41d80bfc327ef980528426fc810a6d7a-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/41d80bfc327ef980528426fc810a6d7a-Paper.pdf}}

@InProceedings{Thin+2021,
  title = 	 {Monte Carlo Variational Auto-Encoders},
  author =       {Thin, Achille and Kotelevskii, Nikita and Doucet, Arnaud and Durmus, Alain and Moulines, Eric and Panov, Maxim},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10247--10257},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/thin21a/thin21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/thin21a.html},
  abstract = 	 {Variational auto-encoders (VAE) are popular deep latent variable models which are trained by maximizing an Evidence Lower Bound (ELBO). To obtain tighter ELBO and hence better variational approximations, it has been proposed to use importance sampling to get a lower variance estimate of the evidence. However, importance sampling is known to perform poorly in high dimensions. While it has been suggested many times in the literature to use more sophisticated algorithms such as Annealed Importance Sampling (AIS) and its Sequential Importance Sampling (SIS) extensions, the potential benefits brought by these advanced techniques have never been realized for VAE: the AIS estimate cannot be easily differentiated, while SIS requires the specification of carefully chosen backward Markov kernels. In this paper, we address both issues and demonstrate the performance of the resulting Monte Carlo VAEs on a variety of applications.}
}
@article{Tang+2024,
    author          = {Weipin Tang and Yuhang Wu and Xunyu Zhou},
    year            = {2024},
    title           = {{Discrete-Time Simulated Annealing: A Convergence Analysis via the Eyring-Kramers Law}},
    journal         = {Numerical Algebra, Control and Optimization},
    volume          = {},
    number          = {},
    pages           = {},
    url             = {https://doi.org/10.3934/naco.2024015}
}

@article{Fournier-Tardif2021,
	abstract = {Using a localization procedure and the result of Holley-Kusuoka-Stroock [8] in the torus, we widely weaken the usual growth assumptions concerning the success of the continuous-time simulated annealing in Rd. Our only assumption is the existence of an invariant probability measure for a sufficiently low temperature. We also prove, in an appendix, a non-explosion criterion for a class of time-inhomogeneous diffusions.},
	author = {Nicolas Fournier and Camille Tardif},
	doi = {https://doi.org/10.1016/j.jfa.2021.109086},
	issn = {0022-1236},
	journal = {Journal of Functional Analysis},
	keywords = {Simulated annealing, Time-inhomogeneous diffusion processes, Large time behavior, Non-explosion},
	number = {5},
	pages = {109086},
	title = {On the simulated annealing in Rd},
	url = {https://www.sciencedirect.com/science/article/pii/S0022123621001683},
	volume = {281},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022123621001683},
	bdsk-url-2 = {https://doi.org/10.1016/j.jfa.2021.109086}}

@inproceedings{Chen+2018,
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Neural Ordinary Differential Equations},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf}}
@inproceedings{Grathwohl+2019,
title={{Scalable Reversible Generative Models with Free-form Continuous Dynamics}},
author={Will Grathwohl and Ricky T. Q. Chen and Jesse Bettencourt and David Duvenaud},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJxgknCcK7},
}
@article{Gortler+2019,
  author = {Görtler, Jochen and Kehlbeck, Rebecca and Deussen, Oliver},
  title = {A Visual Exploration of Gaussian Processes},
  journal = {Distill},
  year = {2019},
  note = {https://distill.pub/2019/visual-exploration-gaussian-processes},
  doi = {10.23915/distill.00017}
}
@article{Tipping2001,
    author          = {Michael E. Tipping},
    year            = {2001},
    title           = {Sparse Bayesian Learning and the Relevance Vector Machine},
    journal         = {Journal of Machine Learning Research},
    volume          = {1},
    number          = {},
    pages           = {211-244},
    url             = {https://www.jmlr.org/papers/v1/tipping01a.html}
}
@INPROCEEDINGS{Loeliger+2016,
  author={Loeliger, Hans-Andrea and Bruderer, Lukas and Malmberg, Hampus and Wadehn, Federico and Zalmai, Nour},
  booktitle={2016 Information Theory and Applications Workshop (ITA)}, 
  title={On sparsity by NUV-EM, Gaussian message passing, and Kalman smoothing}, 
  year={2016},
  volume={},
  number={},
  pages={1-10},
  keywords={Computational modeling;Covariance matrices;Message passing;Smoothing methods;Maximum likelihood estimation;Kalman filters},
  doi={10.1109/ITA.2016.7888168}}
@phdthesis{Gibbs1997,
    author      = {M. N. Gibbs},
    school      = {Cambridge University},
    title       = {Bayesian Gaussian Process Regression and Classification},
    year        = {1997},
    url             = {https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=b5a0c62c8d7cf51137bfb079947b8393c00ed169},
}

@InProceedings{Heinonen+2016,
  title = 	 {Non-Stationary Gaussian Process Regression with Hamiltonian Monte Carlo},
  author = 	 {Heinonen, Markus and Mannerström, Henrik and Rousu, Juho and Kaski, Samuel and Lähdesmäki, Harri},
  booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {732--740},
  year = 	 {2016},
  editor = 	 {Gretton, Arthur and Robert, Christian C.},
  volume = 	 {51},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Cadiz, Spain},
  month = 	 {09--11 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v51/heinonen16.pdf},
  url = 	 {https://proceedings.mlr.press/v51/heinonen16.html},
  abstract = 	 {We present a novel approach for non-stationary Gaussian process regression (GPR), where the three key parameters – noise variance, signal variance and lengthscale – can be simultaneously input-dependent. We develop gradient-based inference methods to learn the unknown function and the non-stationary model parameters, without requiring any model approximations. For inferring the full posterior distribution we use Hamiltonian Monte Carlo (HMC), which conveniently extends the analytical gradient-based GPR learning by guiding the sampling with the gradients. The MAP solution can also be learned with gradient ascent. In experiments on several synthetic datasets and in modelling of temporal gene expression, the non-stationary GPR is shown to give major improvement when modeling realistic input-dependent dynamics.}
}
@mastersthesis{Krige1951,
    author  = {D. G. Krige},
    school  = {University of the Witwatersrand, Faculty of Engineering},
    title   = {A Statistical Approach to Some Mine Valuation and Allied Problems on the Witwatersrand},
    year    = {1951},
    url             = {http://hdl.handle.net/10539/17975},
}

@inproceedings{Remes+2017,
	author = {Remes, Sami and Heinonen, Markus and Kaski, Samuel},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Non-Stationary Spectral Kernels},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/c65d7bd70fe3e5e3a2f3de681edc193d-Paper.pdf}}

@article{Kriege+2020,
	abstract = {Graph kernels have become an established and widely-used technique for solving classification tasks on graphs. This survey gives a comprehensive overview of techniques for kernel-based graph classification developed in the past 15 years. We describe and categorize graph kernels based on properties inherent to their design, such as the nature of their extracted graph features, their method of computation and their applicability to problems in practice. In an extensive experimental evaluation, we study the classification accuracy of a large suite of graph kernels on established benchmarks as well as new datasets. We compare the performance of popular kernels with several baseline methods and study the effect of applying a Gaussian RBF kernel to the metric induced by a graph kernel. In doing so, we find that simple baselines become competitive after this transformation on some datasets. Moreover, we study the extent to which existing graph kernels agree in their predictions (and prediction errors) and obtain a data-driven categorization of kernels as result. Finally, based on our experimental results, we derive a practitioner's guide to kernel-based graph classification.},
	author = {Kriege, Nils M. and Johansson, Fredrik D. and Morris, Christopher},
	date = {2020/01/14},
	date-added = {2024-08-08 14:27:51 +0900},
	date-modified = {2024-08-08 14:27:51 +0900},
	doi = {10.1007/s41109-019-0195-3},
	id = {Kriege2020},
	isbn = {2364-8228},
	journal = {Applied Network Science},
	number = {1},
	pages = {6},
	title = {A survey on graph kernels},
	url = {https://doi.org/10.1007/s41109-019-0195-3},
	volume = {5},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s41109-019-0195-3}}
@article{Lodhi+2002,
    author          = {Huma Lodhi and Craig Saunders and John Shawe-Taylor and Nello Cristianini and Chris Watkins},
    year            = {2002},
    title           = {Text Classification using String Kernels},
    journal         = {Journal of Machine Learning Research},
    volume          = {2},
    number          = {},
    pages           = {419-444},
    url             = {https://www.jmlr.org/papers/v2/lodhi02a.html}
}

@inproceedings{Borgwardt+2006,
	author = {Borgwardt, Karsten and Schraudolph, Nicol and Vishwanathan, S.v.n.},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
	publisher = {MIT Press},
	title = {Fast Computation of Graph Kernels},
	url = {https://proceedings.neurips.cc/paper_files/paper/2006/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf},
	volume = {19},
	year = {2006},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2006/file/e37b08dd3015330dcbb5d6663667b8b8-Paper.pdf}}
@article{Shervashidze+2011,
  author  = {Nino Shervashidze and Pascal Schweitzer and Erik Jan van Leeuwen and Kurt Mehlhorn and Karsten M. Borgwardt},
  title   = {Weisfeiler-Lehman Graph Kernels},
  journal = {Journal of Machine Learning Research},
  year    = {2011},
  volume  = {12},
  number  = {77},
  pages   = {2539-2561},
  url     = {http://jmlr.org/papers/v12/shervashidze11a.html}
}

@InProceedings{Wilson-Adams2013,
  title = 	 {Gaussian Process Kernels for Pattern Discovery and Extrapolation},
  author = 	 {Wilson, Andrew and Adams, Ryan},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1067--1075},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/wilson13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/wilson13.html},
  abstract = 	 {Gaussian processes are rich distributions over functions, which provide a Bayesian nonparametric approach to smoothing and interpolation.  We introduce simple closed form kernels that can be used with Gaussian processes to discover patterns and enable extrapolation.  These kernels are derived by modelling a spectral density – the Fourier transform of a kernel – with a Gaussian mixture.  The proposed kernels support a broad class of stationary covariances, but Gaussian process inference remains simple and analytic.  We demonstrate the proposed kernels by discovering patterns and performing long range extrapolation on synthetic examples, as well as atmospheric CO2 trends and airline passenger data.  We also show that it is possible to reconstruct several popular standard covariances within our framework.}
}
@inproceedings{Sutherland-Schneider2015,
author = {Sutherland, Danica J. and Schneider, Jeff},
title = {On the error of random fourier features},
year = {2015},
isbn = {9780996643108},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Kernel methods give powerful, flexible, and theoretically grounded approaches to solving many problems in machine learning. The standard approach, however, requires pairwise evaluations of a kernel function, which can lead to scalability issues for very large datasets. Rahimi and Recht (2007) suggested a popular approach to handling this problem, known as random Fourier features. The quality of this approximation, however, is not well understood. We improve the uniform error bound of that paper, as well as giving novel understandings of the embedding's variance, approximation error, and use in some machine learning methods. We also point out that surprisingly, of the two main variants of those features, the more widely used is strictly higher-variance for the Gaussian kernel and has worse bounds.},
booktitle = {Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence},
pages = {862–871},
numpages = {10},
location = {Amsterdam, Netherlands},
series = {UAI'15}
}


@inproceedings{Rahimi-Recht2007,
	author = {Rahimi, Ali and Recht, Benjamin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {Random Features for Large-Scale Kernel Machines},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf}}

@inproceedings{Rahimi-Recht2008,
	author = {Rahimi, Ali and Recht, Benjamin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	publisher = {Curran Associates, Inc.},
	title = {Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf},
	volume = {21},
	year = {2008},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf}}

@inproceedings{Yu+2016,
	author = {Yu, Felix Xinnan X and Suresh, Ananda Theertha and Choromanski, Krzysztof M and Holtmann-Rice, Daniel N and Kumar, Sanjiv},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Orthogonal Random Features},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/53adaf494dc89ef7196d73636eb2451b-Paper.pdf}}
@article{Kimeldorf-Wahba1970,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239347},
 author = {George S. Kimeldorf and Grace Wahba},
 journal = {The Annals of Mathematical Statistics},
 number = {2},
 pages = {495--502},
 publisher = {Institute of Mathematical Statistics},
 title = {A Correspondence Between Bayesian Estimation on Stochastic Processes and Smoothing by Splines},
 urldate = {2024-08-08},
 volume = {41},
 year = {1970}
}

@inproceedings{Scholkopf+2001,
	abstract = {Wahba's classical representer theorem states that the solutions of certain risk minimization problems involving an empirical risk term and a quadratic regularizer can be written as expansions in terms of the training examples. We generalize the theorem to a larger class of regularizers and empirical risk terms, and give a self-contained proof utilizing the feature space associated with a kernel. The result shows that a wide range of problems have optimal solutions that live in the finite dimensional span of the training examples mapped into feature space, thus enabling us to carry out kernel algorithms independent of the (potentially infinite) dimensionality of the feature space.},
	address = {Berlin, Heidelberg},
	author = {Sch{\"o}lkopf, Bernhard and Herbrich, Ralf and Smola, Alex J.},
	booktitle = {Computational Learning Theory},
	editor = {Helmbold, David and Williamson, Bob},
	isbn = {978-3-540-44581-4},
	pages = {416--426},
	publisher = {Springer Berlin Heidelberg},
	title = {A Generalized Representer Theorem},
	year = {2001}}
@article{Wilkinson+2023,
  author  = {William J. Wilkinson and Simo Särkkä and Arno Solin},
  title   = {Bayes-Newton Methods for Approximate Bayesian Inference with PSD Guarantees},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {83},
  pages   = {1--50},
  url     = {http://jmlr.org/papers/v24/21-1298.html}
}
@inproceedings{Wenzel+2019,
    author          = {Florian Wenzel and Théo Galy-Fajou and Christan Donner and Marius Kolft and Manfred Opper},
    year            = {2019},
    title           = {Efficient Gaussian Process Classification Using Pólya-Gamma Data Augmentation},
    booktitle       = {Proceedings of the AAAI Conference on Artificial Intelligence},
    volume          = {33},
    pages           = {},
    url             = {},
    doi             = {10.1609/aaai.v33i01.33015417},
}

@InProceedings{Galy-Fajou2020,
  title = 	 {Automated Augmented Conjugate Inference for Non-conjugate Gaussian Process Models},
  author =       {Galy-Fajou, Theo and Wenzel, Florian and Opper, Manfred},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3025--3035},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/galy-fajou20a/galy-fajou20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/galy-fajou20a.html},
  abstract = 	 {We propose automated augmented conjugate inference, a new inference method for non-conjugate Gaussian processes (GP) models.Our method automatically constructs an auxiliary variable augmentation that renders the GP model conditionally conjugate. Building on the conjugate structure of the augmented model, we develop two inference methods. First, a fast and scalable stochastic variational inference method that uses efficient block coordinate ascent updates, which are computed in closed form. Second, an asymptotically correct Gibbs sampler that is useful for small datasets.Our experiments show that our method is up two orders of magnitude faster and more robust than existing state-of-the-art black-box methods.}
}
@ARTICLE{Liu+2020,
  author={Liu, Haitao and Ong, Yew-Soon and Shen, Xiaobo and Cai, Jianfei},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={When Gaussian Process Meets Big Data: A Review of Scalable GPs}, 
  year={2020},
  volume={31},
  number={11},
  pages={4405-4423},
  keywords={Kernel;Scalability;Sparse representation;Complexity theory;Computational modeling;Ground penetrating radar;Predictive models;Big data;Gaussian process regression (GPR);local approximations;scalability;sparse approximations},
  doi={10.1109/TNNLS.2019.2957109}}

@InProceedings{Wilson+2020,
  title = 	 {Efficiently sampling functions from {G}aussian process posteriors},
  author =       {Wilson, James and Borovitskiy, Viacheslav and Terenin, Alexander and Mostowsky, Peter and Deisenroth, Marc},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10292--10302},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/wilson20a/wilson20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/wilson20a.html},
  abstract = 	 {Gaussian processes are the gold standard for many real-world modeling problems, especially in cases where a model’s success hinges upon its ability to faithfully represent predictive uncertainty. These problems typically exist as parts of larger frameworks, wherein quantities of interest are ultimately defined by integrating over posterior distributions. These quantities are frequently intractable, motivating the use of Monte Carlo methods. Despite substantial progress in scaling up Gaussian processes to large training sets, methods for accurately generating draws from their posterior distributions still scale cubically in the number of test locations. We identify a decomposition of Gaussian processes that naturally lends itself to scalable sampling by separating out the prior from the data. Building off of this factorization, we propose an easy-to-use and general-purpose approach for fast posterior sampling, which seamlessly pairs with sparse approximations to afford scalability both during training and at test time. In a series of experiments designed to test competing sampling schemes’ statistical properties and practical ramifications, we demonstrate how decoupled sample paths accurately represent Gaussian process posteriors at a fraction of the usual cost.}
}
@INPROCEEDINGS{Hartikainen-Sarkka2010,
  author={Hartikainen, Jouni and Särkkä, Simo},
  booktitle={2010 IEEE International Workshop on Machine Learning for Signal Processing}, 
  title={Kalman filtering and smoothing solutions to temporal Gaussian process regression models}, 
  year={2010},
  volume={},
  number={},
  pages={379-384},
  keywords={Kalman filters;Approximation methods;Mathematical model;Markov processes;Computational modeling;Gaussian processes;Equations},
  doi={10.1109/MLSP.2010.5589113}}

@inproceedings{Snelson-Ghahramani2005,
	author = {Snelson, Edward and Ghahramani, Zoubin},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
	publisher = {MIT Press},
	title = {Sparse Gaussian Processes using Pseudo-inputs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf},
	volume = {18},
	year = {2005},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf}}

@inproceedings{Wilson+2014,
	author = {Wilson, Andrew G and Gilboa, Elad and Nehorai, Arye and Cunningham, John P},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Fast Kernel Learning for Multidimensional Pattern Extrapolation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/77369e37b2aa1404f416275183ab055f-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/77369e37b2aa1404f416275183ab055f-Paper.pdf}}

@inproceedings{Salakhutdinov-Hinton2007,
	author = {Hinton, Geoffrey E and Salakhutdinov, Russ R},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {Using Deep Belief Nets to Learn Covariance Kernels for Gaussian Processes},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/4b6538a44a1dfdc2b83477cd76dee98e-Paper.pdf}}

@InProceedings{Ober+2021,
  title = 	 {The promises and pitfalls of deep kernel learning},
  author =       {Ober, Sebastian W. and Rasmussen, Carl E. and van der Wilk, Mark},
  booktitle = 	 {Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence},
  pages = 	 {1206--1216},
  year = 	 {2021},
  editor = 	 {de Campos, Cassio and Maathuis, Marloes H.},
  volume = 	 {161},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {27--30 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v161/ober21a/ober21a.pdf},
  url = 	 {https://proceedings.mlr.press/v161/ober21a.html},
  abstract = 	 {Deep kernel learning and related techniques promise to combine the representational power of neural networks with the reliable uncertainty estimates of Gaussian processes. One crucial aspect of these models is an expectation that, because they are treated as Gaussian process models optimized using the marginal likelihood, they are protected from overfitting. However, we identify pathological behavior, including overfitting, on a simple toy example. We explore this pathology, explaining its origins and considering how it applies to real datasets. Through careful experimentation on UCI datasets, CIFAR-10, and the UTKFace dataset, we find that the overfitting from overparameterized deep kernel learning, in which the model is “somewhat Bayesian”, can in certain scenarios be worse than that from not being Bayesian at all. However, we find that a fully Bayesian treatment of deep kernel learning can rectify this overfitting and obtain the desired performance improvements over standard neural networks and Gaussian processes.}
}
@inproceedings{Novak+2019,
title={Bayesian Deep Convolutional Networks with Many Channels are Gaussian Processes},
author={Roman Novak and Lechao Xiao and Yasaman Bahri and Jaehoon Lee and Greg Yang and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-dickstein},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1g30j0qF7},
}
@misc{Yang2020,
      title={Scaling Limits of Wide Neural Networks with Weight Sharing: Gaussian Process Behavior, Gradient Independence, and Neural Tangent Kernel Derivation}, 
      author={Greg Yang},
      year={2020},
      eprint={1902.04760},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1902.04760}, 
}

@InProceedings{Damianou-Lawrence2013,
  title = 	 {Deep {G}aussian Processes},
  author = 	 {Damianou, Andreas and Lawrence, Neil D.},
  booktitle = 	 {Proceedings of the Sixteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {207--215},
  year = 	 {2013},
  editor = 	 {Carvalho, Carlos M. and Ravikumar, Pradeep},
  volume = 	 {31},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Scottsdale, Arizona, USA},
  month = 	 {29 Apr--01 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v31/damianou13a.pdf},
  url = 	 {https://proceedings.mlr.press/v31/damianou13a.html},
  abstract = 	 {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.}
}

@article{Carvalho+2010,
    author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
    title = "{The horseshoe estimator for sparse signals}",
    journal = {Biometrika},
    volume = {97},
    number = {2},
    pages = {465-480},
    year = {2010},
    month = {04},
    abstract = "{This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator’s advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator’s tail robustness and the other that demonstrates a super-efficient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asq017},
    url = {https://doi.org/10.1093/biomet/asq017},
    eprint = {https://academic.oup.com/biomet/article-pdf/97/2/465/584621/asq017.pdf},
}

@InProceedings{Carvalho+2009,
  title = 	 {Handling Sparsity via the Horseshoe},
  author = 	 {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
  booktitle = 	 {Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {73--80},
  year = 	 {2009},
  editor = 	 {van Dyk, David and Welling, Max},
  volume = 	 {5},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  month = 	 {16--18 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf},
  url = 	 {https://proceedings.mlr.press/v5/carvalho09a.html},
  abstract = 	 {This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.}
}


@inproceedings{Jacot+2018,
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}}

@InProceedings{Woodworth+2020,
  title = 	 {Kernel and Rich Regimes in Overparametrized Models},
  author =       {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {3635--3673},
  year = 	 {2020},
  editor = 	 {Abernethy, Jacob and Agarwal, Shivani},
  volume = 	 {125},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--12 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v125/woodworth20a/woodworth20a.pdf},
  url = 	 {https://proceedings.mlr.press/v125/woodworth20a.html},
  abstract = 	 { A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e.&nbsp;when  during training the network behaves as a kernelized linear predictor, and thus, training with gradient descent has the effect of finding the corresponding minimum RKHS norm solution.  This stands in contrast to other studies which demonstrate how gradient descent on overparametrized  networks can induce rich implicit biases that are not RKHS norms.  Building on an observation by \citet{chizat2018note}, we show how the \textbf{\textit{scale of the initialization}} controls the transition between the “kernel” (aka lazy) and “rich” (aka active) regimes and affects generalization properties in multilayer homogeneous models. We provide a complete and detailed analysis for a family of simple depth-$D$ linear networks that exhibit an interesting and meaningful transition between the kernel and rich regimes, and highlight an interesting role for the \emph{width}  of the models. We further demonstrate this transition empirically for matrix factorization and multilayer non-linear networks.}
}

@InProceedings{Yang-Hu2021,
  title = 	 {Tensor Programs IV: Feature Learning in Infinite-Width Neural Networks},
  author =       {Yang, Greg and Hu, Edward J.},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11727--11737},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/yang21c/yang21c.pdf},
  url = 	 {https://proceedings.mlr.press/v139/yang21c.html},
  abstract = 	 {As its width tends to infinity, a deep neural network’s behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can *learn* features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.}
}

@inproceedings{Chizat+2019,
	author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {On Lazy Training in Differentiable Programming},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf}}
@ARTICLE{Sarkka+2013,
  author={Särkkä, Simo and Solin, Arno and Hartikainen, Jouni},
  journal={IEEE Signal Processing Magazine}, 
  title={Spatiotemporal Learning via Infinite-Dimensional Bayesian Filtering and Smoothing: A Look at Gaussian Process Regression Through Kalman Filtering}, 
  year={2013},
  volume={30},
  number={4},
  pages={51-61},
  keywords={Machine learning;Learning systems;Gaussian processes;Bayes methods;Parametric statistics;Linear regression analysis;Kalman filters;Smoothing methods;Spatiotemporal phenomena;Kernel},
  doi={10.1109/MSP.2013.2246292}}

@InProceedings{Adam+2020,
  title = 	 {Doubly Sparse Variational Gaussian Processes},
  author =       {Adam, Vincent and Eleftheriadis, Stefanos and Artemev, Artem and Durrande, Nicolas and Hensman, James},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2874--2884},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/adam20a/adam20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/adam20a.html},
  abstract = 	 {The use of Gaussian process models is typically limited to datasets with a few tens of thousands of observations due to their complexity and memory footprint.The two most commonly used methods to overcome this limitation are 1) the variational sparse approximation which relies on inducing points and 2) the state-space equivalent formulation of Gaussian processes which can be seen as exploiting some sparsity in the precision matrix.In this work, we propose to take the best of both worlds: we show that the inducing point framework is still valid for state space models and that it can bring further computational and memory savings. Furthermore, we provide the natural gradient formulation for the proposed variational parameterisation.Finally, this work makes it possible to use the state-space formulation inside deep Gaussian process models as illustrated in one of the experiments. }
}
@article{Jona-Lasinio+2012,
 ISSN = {19326157, 19417330},
 URL = {http://www.jstor.org/stable/41713483},
 abstract = {Directional data arise in various contexts such as oceanography (wave directions) and meteorology (wind directions), as well as with measurements on a periodic scale (weekdays, hours, etc.). Our contribution is to introduce a model-based approach to handle periodic data in the case of measurements taken at spatial locations, anticipating structured dependence between these measurements. We formulate a wrapped Gaussian spatial process model for this setting, induced from a customary linear Gaussian process. We build a hierarchical model to handle this situation and show that the fitting of such a model is possible using standard Markov chain Monte Carlo methods. Our approach enables spatial interpolation (and can accommodate measurement error). We illustrate with a set of wave direction data from the Adriatic coast of Italy, generated through a complex computer model.},
 author = {Giovanna Jona-Lasinio and Alan Gelfand and Mattia Jona-Lasinio},
 journal = {The Annals of Applied Statistics},
 number = {4},
 pages = {1478--1498},
 publisher = {Institute of Mathematical Statistics},
 title = {SPATIAL ANALYSIS OF WAVE DIRECTION DATA USING WRAPPED GAUSSIAN PROCESSES},
 urldate = {2024-08-08},
 volume = {6},
 year = {2012}
}
@ARTICLE{Jacobs+1991,
  author={Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
  journal={Neural Computation}, 
  title={Adaptive Mixtures of Local Experts}, 
  year={1991},
  volume={3},
  number={1},
  pages={79-87},
  keywords={},
  doi={10.1162/neco.1991.3.1.79}}

@article{Emerson+2023,
	abstract = {Where do firms innovate? Mapping their locations and directions in technological space is challenging due to its high dimensionality. We propose a new method to characterize firms' inventive activities via topological data analysis (TDA) that represents high-dimensional data in a shape graph. Applying this method to 333 major firms' patents in 1976--2005 reveals hitherto undocumented industry dynamics: some firms remain undifferentiated; others develop unique portfolios. Firms with unique trajectories, which we define and measure graph-theoretically as ``flares'' in the Mapper graph, tend to perform better. This association is statistically and economically significant, and continues to hold after we control for portfolio size, firm survivorship, and industry classification.},
	author = {Emerson G. Escolar and Yasuaki Hiraoka and Mitsuru Igami and Yasin Ozcan},
	doi = {https://doi.org/10.1016/j.respol.2023.104821},
	issn = {0048-7333},
	journal = {Research Policy},
	keywords = {Innovation, Mapper, Patents, R&D, Topological data analysis},
	number = {8},
	pages = {104821},
	title = {Mapping firms' locations in technological space: A topological analysis of patent statistics},
	url = {https://www.sciencedirect.com/science/article/pii/S0048733323001051},
	volume = {52},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0048733323001051},
	bdsk-url-2 = {https://doi.org/10.1016/j.respol.2023.104821}}
@inproceedings{Singh+2007
,
booktitle = {Eurographics Symposium on Point-Based Graphics
},
editor = {M. Botsch and R. Pajarola and B. Chen and M. Zwicker
},
title = {{Topological Methods for the Analysis of High Dimensional Data Sets and 3D Object Recognition
}},
author = {Singh, Gurjeet and 
Memoli, Facundo and 
Carlsson, Gunnar
},
year = {2007
},
publisher = {The Eurographics Association
},
ISSN = {1811-7813
},
ISBN = {978-3-905673-51-7
},
DOI = {/10.2312/SPBG/SPBG07/091-100
}
}
@phdthesis{Roberts1963,
    author      = {Lawrence G. Roberts},
    school      = {Massachusetts Institute of Technology},
    title       = {Machine Perception of Three-Dimensional Solids},
    year        = {1963},
    url             = {http://hdl.handle.net/1721.1/11589},
}

@article{Lee-Mumford2003,
	abstract = {Traditional views of visual processing suggest that early visual neurons in areas V1 and V2 are static spatiotemporal filters that extract local features from a visual scene. The extracted information is then channeled through a feedforward chain of modules in successively higher visual areas for further analysis. Recent electrophysiological recordings from early visual neurons in awake behaving monkeys reveal that there are many levels of complexity in the information processing of the early visual cortex, as seen in the long-latency responses of its neurons. These new findings suggest that activity in the early visual cortex is tightly coupled and highly interactive with the rest of the visual system. They lead us to propose a new theoretical setting based on the mathematical framework of hierarchical Bayesian inference for reasoning about the visual system. In this framework, the recurrent feedforward/feedback loops in the cortex serve to integrate top-down contextual priors and bottom-up observations so as to implement concurrent probabilistic inference along the visual hierarchy. We suggest that the algorithms of particle filtering and Bayesian-belief propagation might model these interactive cortical computations. We review some recent neurophysiological evidences that support the plausibility of these ideas.},
	author = {Tai Sing Lee and David Mumford},
	doi = {10.1364/JOSAA.20.001434},
	journal = {J. Opt. Soc. Am. A},
	keywords = {Vision modeling ; Edge detection; Information processing; Machine vision; Neural networks; Physiology; Spatial resolution},
	month = {Jul},
	number = {7},
	pages = {1434--1448},
	publisher = {Optica Publishing Group},
	title = {Hierarchical Bayesian inference in the visual cortex},
	url = {https://opg.optica.org/josaa/abstract.cfm?URI=josaa-20-7-1434},
	volume = {20},
	year = {2003},
	bdsk-url-1 = {https://opg.optica.org/josaa/abstract.cfm?URI=josaa-20-7-1434},
	bdsk-url-2 = {https://doi.org/10.1364/JOSAA.20.001434}}

@article{Lake+2015,
	abstract = {Not only do children learn effortlessly, they do so quickly and with a remarkable ability to use what they have learned as the raw material for creating new stuff. Lake et al. describe a computational model that learns in a similar fashion and does so better than current deep learning algorithms. The model classifies, parses, and recreates handwritten characters, and can generate new letters of the alphabet that look ``right'' as judged by Turing-like tests of the model's output in comparison to what real humans produce. Science, this issue p. 1332 Combining the capacity to handle noise with probabilistic learning yields humanlike performance in a computational model. People learning new concepts can often generalize successfully from just a single example, yet machine learning algorithms typically require tens or hundreds of examples to perform with similar accuracy. People can also use learned concepts in richer ways than conventional algorithms---for action, imagination, and explanation. We present a computational model that captures these human learning abilities for a large class of simple visual concepts: handwritten characters from the world's alphabets. The model represents concepts as simple programs that best explain observed examples under a Bayesian criterion. On a challenging one-shot classification task, the model achieves human-level performance while outperforming recent deep learning approaches. We also present several ``visual Turing tests'' probing the model's creative generalization abilities, which in many cases are indistinguishable from human behavior.},
	author = {Brenden M. Lake and Ruslan Salakhutdinov and Joshua B. Tenenbaum},
	doi = {10.1126/science.aab3050},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.aab3050},
	journal = {Science},
	number = {6266},
	pages = {1332-1338},
	title = {Human-level concept learning through probabilistic program induction},
	url = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	volume = {350},
	year = {2015},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.aab3050},
	bdsk-url-2 = {https://doi.org/10.1126/science.aab3050}}
@inproceedings{Donahue+2017,
title={Adversarial Feature Learning},
author={Jeff Donahue and Philipp Kr{\"a}henb{\"u}hl and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=BJtNZAFgg}
}
@inproceedings{Bao+2022,
title={{BE}iT: {BERT} Pre-Training of Image Transformers},
author={Hangbo Bao and Li Dong and Songhao Piao and Furu Wei},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=p-BhZSz59o4}
}

@inproceedings{Kivva+2021,
	author = {Kivva, Bohdan and Rajendran, Goutham and Ravikumar, Pradeep and Aragam, Bryon},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {18087--18101},
	publisher = {Curran Associates, Inc.},
	title = {Learning latent causal graphs via mixture oracles},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/966aad8981dcc75b5b8ab04427a833b2-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/966aad8981dcc75b5b8ab04427a833b2-Paper.pdf}}
@inproceedings{Kivva+2022,
title={Identifiability of deep generative models under mixture priors without auxiliary information},
author={Bohdan Kivva and Goutham Rajendran and Pradeep Kumar Ravikumar and Bryon Aragam},
booktitle={UAI 2022 Workshop on Causal Representation Learning},
year={2022},
url={https://openreview.net/forum?id=UeG3kt_Ebg2}
}

@InProceedings{Lopez+2024,
  title = 	 {Toward the Identifiability of Comparative Deep Generative Models},
  author =       {Lopez, Romain and Huetter, Jan-Christian and Hajiramezanali, Ehsan and Pritchard, Jonathan K and Regev, Aviv},
  booktitle = 	 {Proceedings of the Third Conference on Causal Learning and Reasoning},
  pages = 	 {868--912},
  year = 	 {2024},
  editor = 	 {Locatello, Francesco and Didelez, Vanessa},
  volume = 	 {236},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {01--03 Apr},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v236/lopez24a/lopez24a.pdf},
  url = 	 {https://proceedings.mlr.press/v236/lopez24a.html},
  abstract = 	 {Deep Generative Models (DGMs) are versatile tools for learning data representations while adequately incorporating domain knowledge such as the specification of conditional probability distributions. Recently proposed DGMs tackle the important task of comparing data sets from different sources. One such example is the setting of contrastive analysis that focuses on describing patterns that are enriched in a target data set compared to a background data set. The practical deployment of those models often assumes that DGMs naturally infer interpretable and modular latent representations, which is known to be an issue in practice.  Consequently, existing methods often rely on ad-hoc regularization schemes, although without any theoretical grounding. Here, we propose a theory of identifiability for comparative DGMs by extending recent advances in the field of non-linear independent component analysis. We show that, while these models lack identifiability across a general class of mixing functions, they surprisingly become identifiable when the mixing function is piece-wise affine (e.g., parameterized by a ReLU neural network).  We also investigate the impact of model misspecification, and empirically show that previously proposed regularization techniques for fitting comparative DGMs help with identifiability when the number of latent variables is not known in advance. Finally, we introduce a novel methodology for fitting comparative DGMs that improves the treatment of multiple data sources via multi-objective optimization and that helps adjust the hyperparameter for the regularization in an interpretable manner, using constrained optimization. We empirically validate our theory and new methodology using simulated data as well as a recent data set of genetic perturbations in cells profiled via single-cell RNA sequencing.}
}
@article{Locatello+2020,
  author  = {Francesco Locatello and Stefan Bauer and Mario Lucic and Gunnar Raetsch and Sylvain Gelly and Bernhard Sch{{\"o}}lkopf and Olivier Bachem},
  title   = {A Sober Look at the Unsupervised Learning of Disentangled Representations and their Evaluation},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {209},
  pages   = {1--62},
  url     = {http://jmlr.org/papers/v21/19-976.html}
}
@inproceedings{Ding+2021,
title={Cc{\{}GAN{\}}: Continuous Conditional Generative Adversarial Networks for Image Generation},
author={Xin Ding and Yongwei Wang and Zuheng Xu and William J Welch and Z. Jane Wang},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=PrzjugOsDeE}
}
@inproceedings{Hoogeboom+2021,
title={{Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions}},
author={Emiel Hoogeboom and Didrik Nielsen and Priyank Jaini and Patrick Forr{\'e} and Max Welling},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=6nbpPqUCIi7}
}
@misc{Simo2024,
  author={Simo Ryu},
  title={Minimal Implementation of a D3PM (Structured Denoising Diffusion Models in Discrete State-Spaces), in pytorch},
  year={2024},
  url             = {https://github.com/cloneofsimo/d3pm},
}

@inproceedings{Kingma+2014,
	author = {Kingma, Durk P and Mohamed, Shakir and Jimenez Rezende, Danilo and Welling, Max},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {{Semi-supervised Learning with Deep Generative Models}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf}}
@inproceedings{Chen+2023AnalogBits,
title={Analog Bits: Generating Discrete Data using Diffusion Models with Self-Conditioning},
author={Ting Chen and Ruixiang ZHANG and Geoffrey Hinton},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=3itjR9QxFw}
}

@article{Watson+2023,
	abstract = {There has been considerable recent progress in designing new proteins using deep-learning methods1--9. Despite this progress, a general deep-learning framework for protein design that enables solution of a wide range of design challenges, including de novo binder design and design of higher-order symmetric architectures, has yet to be described. Diffusion models10,11 have had considerable success in image and language generative modelling but limited success when applied to protein modelling, probably due to the complexity of protein backbone geometry and sequence--structure relationships. Here we show that by fine-tuning the RoseTTAFold structure prediction network on protein structure denoising tasks, we obtain a generative model of protein backbones that achieves outstanding performance on unconditional and topology-constrained protein monomer design, protein binder design, symmetric oligomer design, enzyme active site scaffolding and symmetric motif scaffolding for therapeutic and metal-binding protein design. We demonstrate the power and generality of the method, called RoseTTAFold diffusion (RFdiffusion), by experimentally characterizing the structures and functions of hundreds of designed symmetric assemblies, metal-binding proteins and protein binders. The accuracy of RFdiffusion is confirmed by the cryogenic electron microscopy structure of a designed binder in complex with influenza haemagglutinin that is nearly identical to the design model. In a manner analogous to networks that produce images from user-specified inputs, RFdiffusion enables the design of diverse functional proteins from simple molecular specifications.},
	author = {Watson, Joseph L. and Juergens, David and Bennett, Nathaniel R. and Trippe, Brian L. and Yim, Jason and Eisenach, Helen E. and Ahern, Woody and Borst, Andrew J. and Ragotte, Robert J. and Milles, Lukas F. and Wicky, Basile I. M. and Hanikel, Nikita and Pellock, Samuel J. and Courbet, Alexis and Sheffler, William and Wang, Jue and Venkatesh, Preetham and Sappington, Isaac and Torres, Susana V{\'a}zquez and Lauko, Anna and De Bortoli, Valentin and Mathieu, Emile and Ovchinnikov, Sergey and Barzilay, Regina and Jaakkola, Tommi S. and DiMaio, Frank and Baek, Minkyung and Baker, David},
	date = {2023/08/01},
	date-added = {2024-08-10 21:39:03 +0900},
	date-modified = {2024-08-10 21:39:03 +0900},
	doi = {10.1038/s41586-023-06415-8},
	id = {Watson2023},
	isbn = {1476-4687},
	journal = {Nature},
	number = {7976},
	pages = {1089--1100},
	title = {De novo design of protein structure and function with RFdiffusion},
	url = {https://doi.org/10.1038/s41586-023-06415-8},
	volume = {620},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-023-06415-8}}
@article{Abramson+2024,
	abstract = {The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2--6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein--ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein--nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody--antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.},
	author = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O'Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and {\v Z}emgulyt{\.e}, Akvil{\.e} and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and Cowen-Rivers, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and {\v Z}{\'\i}dek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
	date = {2024/06/01},
	date-added = {2024-08-10 21:52:55 +0900},
	date-modified = {2024-08-10 21:52:55 +0900},
	doi = {10.1038/s41586-024-07487-w},
	id = {Abramson2024},
	isbn = {1476-4687},
	journal = {Nature},
	number = {8016},
	pages = {493--500},
	title = {Accurate structure prediction of biomolecular interactions with AlphaFold 3},
	url = {https://doi.org/10.1038/s41586-024-07487-w},
	volume = {630},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41586-024-07487-w}}
@article{Krishna+2024,
	abstract = {Deep-learning methods have revolutionized protein structure prediction and design but are presently limited to protein-only systems. We describe RoseTTAFold All-Atom (RFAA), which combines a residue-based representation of amino acids and DNA bases with an atomic representation of all other groups to model assemblies that contain proteins, nucleic acids, small molecules, metals, and covalent modifications, given their sequences and chemical structures. By fine-tuning on denoising tasks, we developed RFdiffusion All-Atom (RFdiffusionAA), which builds protein structures around small molecules. Starting from random distributions of amino acid residues surrounding target small molecules, we designed and experimentally validated, through crystallography and binding measurements, proteins that bind the cardiac disease therapeutic digoxigenin, the enzymatic cofactor heme, and the light-harvesting molecule bilin. Advances in machine learning have made protein structure prediction and design much more accurate and accessible in recent years, but these tools have generally been limited to polypeptide chains. However, ligands such as small molecules, metal ions, and nucleic acids are crucial components of most proteins, both in terms of structure and biological function. Krishna et al. present a next-generation protein structure prediction and design tool, RoseTTAFold All-Atom, that can accept a wide range of ligands and covalent amino acid modifications. The authors demonstrate superior performance on protein-ligand structure prediction relative to other tools, even in the absence of an input experimental structure. They also perform de novo design of proteins to bind cofactors and small molecules and experimentally validate these designs. ---Michael A. Funk},
	author = {Rohith Krishna and Jue Wang and Woody Ahern and Pascal Sturmfels and Preetham Venkatesh and Indrek Kalvet and Gyu Rie Lee and Felix S. Morey-Burrows and Ivan Anishchenko and Ian R. Humphreys and Ryan McHugh and Dionne Vafeados and Xinting Li and George A. Sutherland and Andrew Hitchcock and C. Neil Hunter and Alex Kang and Evans Brackenbrough and Asim K. Bera and Minkyung Baek and Frank DiMaio and David Baker},
	doi = {10.1126/science.adl2528},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.adl2528},
	journal = {Science},
	number = {6693},
	pages = {eadl2528},
	title = {Generalized biomolecular modeling and design with RoseTTAFold All-Atom},
	url = {https://www.science.org/doi/abs/10.1126/science.adl2528},
	volume = {384},
	year = {2024},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.adl2528},
	bdsk-url-2 = {https://doi.org/10.1126/science.adl2528}}

@article{Zheng+2024,
	abstract = {Advances in deep learning have greatly improved structure prediction of molecules. However, many macroscopic observations that are important for real-world applications are not functions of a single molecular structure but rather determined from the equilibrium distribution of structures. Conventional methods for obtaining these distributions, such as molecular dynamics simulation, are computationally expensive and often intractable. Here we introduce a deep learning framework, called Distributional Graphormer (DiG), in an attempt to predict the equilibrium distribution of molecular systems. Inspired by the annealing process in thermodynamics, DiG uses deep neural networks to transform a simple distribution towards the equilibrium distribution, conditioned on a descriptor of a molecular system such as a chemical graph or a protein sequence. This framework enables the efficient generation of diverse conformations and provides estimations of state densities, orders of magnitude faster than conventional methods. We demonstrate applications of DiG on several molecular tasks, including protein conformation sampling, ligand structure sampling, catalyst--adsorbate sampling and property-guided structure generation. DiG presents a substantial advancement in methodology for statistically understanding molecular systems, opening up new research opportunities in the molecular sciences.},
	author = {Zheng, Shuxin and He, Jiyan and Liu, Chang and Shi, Yu and Lu, Ziheng and Feng, Weitao and Ju, Fusong and Wang, Jiaxi and Zhu, Jianwei and Min, Yaosen and Zhang, He and Tang, Shidi and Hao, Hongxia and Jin, Peiran and Chen, Chi and No{\'e}, Frank and Liu, Haiguang and Liu, Tie-Yan},
	date = {2024/05/01},
	date-added = {2024-08-10 22:01:13 +0900},
	date-modified = {2024-08-10 22:01:13 +0900},
	doi = {10.1038/s42256-024-00837-3},
	id = {Zheng2024},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	number = {5},
	pages = {558--567},
	title = {Predicting equilibrium distributions for molecular systems with deep learning},
	url = {https://doi.org/10.1038/s42256-024-00837-3},
	volume = {6},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s42256-024-00837-3}}
@unpublished{Duvenaud2014,
    author = {David Kristjanson Duvenaud},
    year   = {2014},
    title  = {The Kernel Cookbook: Advice on Covariance functions},
    url    = {https://www.cs.toronto.edu/~duvenaud/cookbook/}
}
@ARTICLE{Linsker1988,
  author={Linsker, R.},
  journal={Computer}, 
  title={{Self-Organization in a Perceptual Network}}, 
  year={1988},
  volume={21},
  number={3},
  pages={105-117},
  keywords={Intelligent networks;Biological information theory;Circuits;Biology computing;Animal structures;Neuroscience;Genetics;System testing;Neural networks;Constraint theory},
  doi={10.1109/2.36}}

@article{Torgenson1952,
	abstract = {Multidimensional scaling can be considered as involving three basic steps. In the first step, a scale of comparative distances between all pairs of stimuli is obtained. This scale is analogous to the scale of stimuli obtained in the traditional paired comparisons methods. In this scale, however, instead of locating each stimulus-object on a given continuum, the distances between each pair of stimuli are located on a distance continuum. As in paired comparisons, the procedures for obtaining a scale of comparative distances leave the true zero point undetermined. Hence, a comparative distance is not a distance in the usual sense of the term, but is a distance minus an unknown constant. The second step involves estimating this unknown constant. When the unknown constant is obtained, the comparative distances can be converted into absolute distances. In the third step, the dimensionality of the psychological space necessary to account for these absolute distances is determined, and the projections of stimuli on axes of this space are obtained. A set of analytical procedures was developed for each of the three steps given above, including a least-squares solution for obtaining comparative distances by the complete method of triads, two practical methods for estimating the additive constant, and an extension of Young and Householder's Euclidean model to include procedures for obtaining the projections of stimuli on axes from fallible absolute distances.},
	author = {Torgerson, Warren S. },
	date = {1952/12/01},
	date-added = {2024-08-10 23:19:09 +0900},
	date-modified = {2024-08-10 23:19:09 +0900},
	doi = {10.1007/BF02288916},
	id = {Torgerson1952},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {401--419},
	title = {Multidimensional scaling: I. Theory and method},
	url = {https://doi.org/10.1007/BF02288916},
	volume = {17},
	year = {1952},
	bdsk-url-1 = {https://doi.org/10.1007/BF02288916}}

@article{Kruskal1964,
	abstract = {Multidimensional scaling is the problem of representingn objects geometrically byn points, so that the interpoint distances correspond in some sense to experimental dissimilarities between objects. In just what sense distances and dissimilarities should correspond has been left rather vague in most approaches, thus leaving these approaches logically incomplete. Our fundamental hypothesis is that dissimilarities and distances are monotonically related. We define a quantitative, intuitively satisfying measure of goodness of fit to this hypothesis. Our technique of multidimensional scaling is to compute that configuration of points which optimizes the goodness of fit. A practical computer program for doing the calculations is described in a companion paper.},
	author = {Kruskal, J.  B. },
	date = {1964/03/01},
	date-added = {2024-08-10 23:20:08 +0900},
	date-modified = {2024-08-10 23:20:08 +0900},
	doi = {10.1007/BF02289565},
	id = {Kruskal1964},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {1--27},
	title = {Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis},
	url = {https://doi.org/10.1007/BF02289565},
	volume = {29},
	year = {1964},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289565}}
@article{Poole-Rosenthal1985,
 ISSN = {00925853, 15405907},
 URL = {http://www.jstor.org/stable/2111172},
 abstract = {A general nonlinear logit model is used to analyze political choice data. The model assumes probabilistic voting based on a spatial utility function. The parameters of the utility function and the spatial coordinates of the choices and the choosers can all be estimated on the basis of observed choices. Ordinary Guttman scaling is a degenerate case of this model. Estimation of the model is implemented in the NOMINATE program for one dimensional analysis of two alternative choices with no nonvoting. The robustness and face validity of the program outputs are evaluated on the basis of roll call voting data for the U.S. House and Senate.},
 author = {Keith T. Poole and Howard Rosenthal},
 journal = {American Journal of Political Science},
 number = {2},
 pages = {357--384},
 publisher = {[Midwest Political Science Association, Wiley]},
 title = {A Spatial Model for Legislative Roll Call Analysis},
 urldate = {2024-08-10},
 volume = {29},
 year = {1985}
}

@article{岡田謙介-加藤淳子2016,
	author = {岡田謙介 and 加藤淳子},
	doi = {10.2333/jbhmk.43.155},
	journal = {行動計量学},
	number = {2},
	pages = {155-166},
	title = {政治学における空間分析と認知空間},
	volume = {43},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.43.155}}
@book{Enelow-Hinich1984,
    author         = {James M. Enelow and Melvin J. Hinich},
    year           = {1984},
    title          = {The Spatial Theory of Voting: An Introduction},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {https://www.cambridge.org/us/universitypress/subjects/politics-international-relations/political-theory/spatial-theory-voting-introduction?format=PB&isbn=9780521275156},
    publisher      = {Cambridge University Press}
}

@article{Eckart-Young1936,
	abstract = {The mathematical problem of approximating one matrix by another of lower rank is closely related to the fundamental postulate of factor-theory. When formulated as a least-squares problem, the normal equations cannot be immediately written down, since the elements of the approximate matrix are not independent of one another. The solution of the problem is simplified by first expressing the matrices in a canonic form. It is found that the problem always has a solution which is usually unique. Several conclusions can be drawn from the form of this solution.},
	author = {Eckart, Carl and Young, Gale},
	date = {1936/09/01},
	date-added = {2024-08-11 11:46:20 +0900},
	date-modified = {2024-08-11 11:46:20 +0900},
	doi = {10.1007/BF02288367},
	id = {Eckart1936},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {3},
	pages = {211--218},
	title = {The approximation of one matrix by another of lower rank},
	url = {https://doi.org/10.1007/BF02288367},
	volume = {1},
	year = {1936},
	bdsk-url-1 = {https://doi.org/10.1007/BF02288367}}
@inbook{Easterling1987,
    author         = {D. V. Easterling},
    chapter        = {Political Scicnce: Using the Generalized Euclidian Model to Study Ideological Shifts in the U.S. Senate },
    editor         = {Robert M. Hamer and Forrest W. Young},
    pages          = {219-256},
    publisher      = {Psychology Press},
    title          = {Multidimensional Scaling: History, Theory, and Applications},
    year           = {1987},
    url             = {https://doi.org/10.4324/9780203767719},
}
@article{Coombs1950,
    author          = {C. H. Coombs},
    year            = {1950},
    title           = {{Psychological Scaling without a Unit of Measurement}},
    journal         = {Psychological Review},
    volume          = {57},
    number          = {3},
    pages           = {145-158},
    url             = {https://psycnet.apa.org/doi/10.1037/h0060984}
}

@article{足立浩平2000,
	author = {足立浩平},
	doi = {10.2333/jbhmk.27.12},
	journal = {行動計量学},
	number = {1},
	pages = {12-23},
	title = {計量多次元展開法の変量モデル},
	volume = {27},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.27.12}}
@book{MacRae1958,
    author         = {Duncan MacRae},
    year           = {1958},
    title          = {Dimensions of Congressional Voting: A Statistical Study of the House of Representatives in the Eighty-first Congress},
    series         = {},
    volume         = {},
    edition        = {},
    url            = {},
    publisher      = {University of California Press}
}
@article{Poole+2011,
 title={Scaling Roll Call Votes with wnominate in R},
 volume={42},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v042i14},
 doi={10.18637/jss.v042.i14},
 abstract={This paper presents a software package designed to estimate Poole and Rosenthal W-NOMINATE scores in R. The package uses a logistic regression model to analyze political choice data, usually (though not exclusively) from a legislative setting. In contrast to other scaling methods, W-NOMINATE explicitly assumes probabilistic voting based on a spatial utility function, where the parameters of the utility function and the spatial coordinates of the legislators and the votes can all be estimated on the basis of observed voting behavior. Building on software written by Poole in Fortran, the new &amp;lt;b&amp;gt;wnominate&amp;lt;/b&amp;gt; package in R facilitates easier data input and manipulation, generates bootstrapped standard errors, and includes a new suite of graphics functions to display the results. We demonstrate the functionality of this package by conducting a natural experiment using roll calls -- an experiment which is greatly simplified by the data manipulation capabilities of the &amp;lt;b&amp;gt;wnominate&amp;lt;/b&amp;gt; package in R.},
 number={14},
 journal={Journal of Statistical Software},
 author={Poole, Keith T. and Lewis, Jeffrey B. and Lo, James and Carroll, Royce},
 year={2011},
 pages={1–21}
}
@article{Lee2001,
	abstract = {Multidimensional scaling models of stimulus domains are widely used as a representational basis for cognitive modeling. These representations associate stimuli with points in a coordinate space that has some predetermined number of dimensions. Although the choice of dimensionality can significantly influence cognitive modeling, it is often made on the basis of unsatisfactory heuristics. To address this problem, a Bayesian approach to dimensionality determination, based on the Bayesian Information Criterion (BIC), is developed using a probabilistic formulation of multidimensional scaling. The BIC approach formalizes the trade-off between data-fit and model complexity implicit in the problem of dimensionality determination and allows for the explicit introduction of information regarding data precision. Monte Carlo simulations are presented that indicate, by using this approach, the determined dimensionality is likely to be accurate if either a significant number of stimuli are considered or a reasonable estimate of precision is available. The approach is demonstrated using an established data set involving the judged pairwise similarities between a set of geometric stimuli.},
	author = {Michael D. Lee},
	doi = {https://doi.org/10.1006/jmps.1999.1300},
	issn = {0022-2496},
	journal = {Journal of Mathematical Psychology},
	number = {1},
	pages = {149-166},
	title = {Determining the Dimensionality of Multidimensional Scaling Representations for Cognitive Modeling},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249699913007},
	volume = {45},
	year = {2001},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022249699913007},
	bdsk-url-2 = {https://doi.org/10.1006/jmps.1999.1300}}
@article{Baker-Poole2013,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/23359696},
 abstract = {In this article, we show how to apply Bayesian methods to noisy ratio scale distances for both the classical similarities problem as well as the unfolding problem. Bayesian methods produce essentially the same point estimates as the classical methods, but are superior in that they provide more accurate measures of uncertainty in the data. Identification is nontrivial for this class of problems because a configuration of points that reproduces the distances is identified only up to a choice of origin, angles of rotation, and sign flips on the dimensions. We prove that fixing the origin and rotation is sufficient to identify a configuration in the sense that the corresponding maxima/minima are inflection points with full-rank Hessians. However, an unavoidable result is multiple posterior distributions that are mirror images of one another. This poses a problem for Markov chain Monte Carlo (MCMC) methods. The approach we take is to find the optimal solution using standard optimizers. The configuration of points from the optimizers is then used to isolate a single Bayesian posterior that can then be easily analyzed with standard MCMC methods.},
 author = {Ryan Bakker and Keith T. Poole},
 journal = {Political Analysis},
 number = {1},
 pages = {125--140},
 publisher = {Oxford University Press},
 title = {Bayesian Metric Multidimensional Scaling},
 urldate = {2024-08-10},
 volume = {21},
 year = {2013}
}
@inproceedings{Lim+2024,
    author          = {Johan Lim and Sooahn Shin and Jong Hee Park},
    year            = {2024},
    title           = {$\ell^1$-Based Bayesian Ideal Point Model for Multidimensional Politics},
    booktitle       = {ISI World Statistics Congress},
    volume          = {64},
    pages           = {},
    url             = {https://www.isi-next.org/abstracts/submission/1310/view/}
}
@article{Jackman2001,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/25791646},
 abstract = {Vote-specific parameters are often by-products of roll call analysis, the primary goal being the measurement of legislators' ideal points. But these vote-specific parameters are more important in higher-dimensional settings: prior restrictions on vote parameters help identify the model, and researchers often have prior beliefs about the nature of the dimensions underlying the proposal space. Bayesian methods provide a straightforward and rigorous way for incorporating these prior beliefs into roll call analysis. I demonstrate this by exploiting the close connections among roll call analysis, item—response models, and "full-information" factor analysis. Vote-specific discrimination parameters are equivalent to factor loadings, and as in factor analysis, they (1) enable researchers to discern the substantive content of the recovered dimensions, (2) can be used for assessing dimensionality and model checking, and (3) are an obvious vehicle for introducing and testing researchers' prior beliefs about the dimensions. Bayesian simulation facilitates these uses of discrimination parameters, by simplifying estimation and inference for the massive number of parameters generated by roll call analysis.},
 author = {Simon Jackman},
 journal = {Political Analysis},
 number = {3},
 pages = {227--241},
 publisher = {[Oxford University Press, Society for Political Methodology]},
 title = {Multidimensional Analysis of Roll Call Data via Bayesian Simulation: Identification, Estimation, Inference, and Model Checking},
 urldate = {2024-08-11},
 volume = {9},
 year = {2001}
}
@techreport{deLeeuw1977,
    author          = {Jan {de Leeuw}},
    year            = {1977},
    title           = {Applications of Convex Analysis to Multidimensional Scaling},
    institution = {UCLA: Department of Statistics},
    url             = {https://escholarship.org/uc/item/7wg0k7xq},
}
@ARTICLE{Sammon1969,
  author={Sammon, J.W.},
  journal={IEEE Transactions on Computers}, 
  title={A Nonlinear Mapping for Data Structure Analysis}, 
  year={1969},
  volume={C-18},
  number={5},
  pages={401-409},
  keywords={Clustering, dimensionality reduction, mappings, multidimensional scaling, multivariate data analysis, nonparametric, pattern recognition, statistics.},
  doi={10.1109/T-C.1969.222678}}

@article{Tenenbaum+2000,
	abstract = {Scientists working with large volumes of high-dimensional data, such as global climate patterns, stellar spectra, or human gene distributions, regularly confront the problem of dimensionality reduction: finding meaningful low-dimensional structures hidden in their high-dimensional observations. The human brain confronts the same problem in everyday perception, extracting from its high-dimensional sensory inputs---30,000 auditory nerve fibers or 106 optic nerve fibers---a manageably small number of perceptually relevant features. Here we describe an approach to solving dimensionality reduction problems that uses easily measured local metric information to learn the underlying global geometry of a data set. Unlike classical techniques such as principal component analysis (PCA) and multidimensional scaling (MDS), our approach is capable of discovering the nonlinear degrees of freedom that underlie complex natural observations, such as human handwriting or images of a face under different viewing conditions. In contrast to previous algorithms for nonlinear dimensionality reduction, ours efficiently computes a globally optimal solution, and, for an important class of data manifolds, is guaranteed to converge asymptotically to the true structure.},
	author = {Joshua B. Tenenbaum and Vin de Silva and John C. Langford},
	doi = {10.1126/science.290.5500.2319},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2319},
	journal = {Science},
	number = {5500},
	pages = {2319-2323},
	title = {{A Global Geometric Framework for Nonlinear Dimensionality Reduction}},
	url = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
	volume = {290},
	year = {2000},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.290.5500.2319},
	bdsk-url-2 = {https://doi.org/10.1126/science.290.5500.2319}}

@article{Balasubramanian-Schwartz2002,
	author = {Mukund Balasubramanian and Eric L. Schwartz},
	doi = {10.1126/science.295.5552.7a},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.295.5552.7a},
	journal = {Science},
	number = {5552},
	pages = {7-7},
	title = {The Isomap Algorithm and Topological Stability},
	url = {https://www.science.org/doi/abs/10.1126/science.295.5552.7a},
	volume = {295},
	year = {2002},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.295.5552.7a},
	bdsk-url-2 = {https://doi.org/10.1126/science.295.5552.7a}}

@article{Choi-Choi2007,
	abstract = {Isomap is one of widely used low-dimensional embedding methods, where geodesic distances on a weighted graph are incorporated with the classical scaling (metric multidimensional scaling). In this paper we pay our attention to two critical issues that were not considered in Isomap, such as: (1) generalization property (projection property); (2) topological stability. Then we present a robust kernel Isomap method, armed with such two properties. We present a method which relates the Isomap to Mercer kernel machines, so that the generalization property naturally emerges, through kernel principal component analysis. For topological stability, we investigate the network flow in a graph, providing a method for eliminating critical outliers. The useful behavior of the robust kernel Isomap is confirmed through numerical experiments with several data sets.},
	author = {Heeyoul Choi and Seungjin Choi},
	doi = {https://doi.org/10.1016/j.patcog.2006.04.025},
	issn = {0031-3203},
	journal = {Pattern Recognition},
	keywords = {Isomap, Kernel PCA, Manifold learning, Multidimensional scaling (MDS), Nonlinear dimensionality reduction},
	number = {3},
	pages = {853-862},
	title = {Robust kernel Isomap},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320306001804},
	volume = {40},
	year = {2007},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0031320306001804},
	bdsk-url-2 = {https://doi.org/10.1016/j.patcog.2006.04.025}}
@ARTICLE{Scholkopf+1998,
  author={Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
  journal={Neural Computation}, 
  title={Nonlinear Component Analysis as a Kernel Eigenvalue Problem}, 
  year={1998},
  volume={10},
  number={5},
  pages={1299-1319},
  keywords={},
  doi={10.1162/089976698300017467}}
@inproceedings{Weinberger+2004,
author = {Weinberger, Kilian Q. and Sha, Fei and Saul, Lawrence K.},
title = {Learning a kernel matrix for nonlinear dimensionality reduction},
year = {2004},
isbn = {1581138385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1015330.1015345},
doi = {10.1145/1015330.1015345},
abstract = {We investigate how to learn a kernel matrix for high dimensional data that lies on or near a low dimensional manifold. Noting that the kernel matrix implicitly maps the data into a nonlinear feature space, we show how to discover a mapping that "unfolds" the underlying manifold from which the data was sampled. The kernel matrix is constructed by maximizing the variance in feature space subject to local constraints that preserve the angles and distances between nearest neighbors. The main optimization involves an instance of semidefinite programming---a fundamentally different computation than previous algorithms for manifold learning, such as Isomap and locally linear embedding. The optimized kernels perform better than polynomial and Gaussian kernels for problems in manifold learning, but worse for problems in large margin classification. We explain these results in terms of the geometric properties of different kernels and comment on various interpretations of other manifold learning algorithms as kernel methods.},
booktitle = {Proceedings of the Twenty-First International Conference on Machine Learning},
pages = {106},
location = {Banff, Alberta, Canada},
series = {ICML '04}
}

@article{Roweis-Saul2000,
	abstract = {Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.},
	author = {Sam T. Roweis and Lawrence K. Saul},
	doi = {10.1126/science.290.5500.2323},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.290.5500.2323},
	journal = {Science},
	number = {5500},
	pages = {2323-2326},
	title = {Nonlinear Dimensionality Reduction by Locally Linear Embedding},
	url = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
	volume = {290},
	year = {2000},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.290.5500.2323},
	bdsk-url-2 = {https://doi.org/10.1126/science.290.5500.2323}}

@inproceedings{Mikhali-Partha2001,
	author = {Belkin, Mikhail and Niyogi, Partha},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {T. Dietterich and S. Becker and Z. Ghahramani},
	publisher = {MIT Press},
	title = {Laplacian Eigenmaps and Spectral Techniques for Embedding and Clustering},
	url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf},
	volume = {14},
	year = {2001},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f106b7f99d2cb30c3db1c3cc0fde9ccb-Paper.pdf}}
@book{Chung1997,
    author         = {Fan R. K. Chung},
    year           = {1997},
    title          = {Spectral Graph Theory},
    series         = {CBMS Regional Conference Series in Mathematics},
    volume         = {92},
    edition        = {},
    url            = {https://doi.org/10.1090/cbms/092},
    publisher      = {American Mathematical Society}
}

@inproceedings{Hinton-Roweis2002,
	author = {Hinton, Geoffrey E and Roweis, Sam},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Becker and S. Thrun and K. Obermayer},
	publisher = {MIT Press},
	title = {Stochastic Neighbor Embedding},
	url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf},
	volume = {15},
	year = {2002},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2002/file/6150ccc6069bea6b5716254057a194ef-Paper.pdf}}
@article{Maaten-Hinton2008,
  author  = {Laurens {van der Maaten} and Geoffrey Hinton},
  title   = {Visualizing Data using t-SNE},
  journal = {Journal of Machine Learning Research},
  year    = {2008},
  volume  = {9},
  number  = {86},
  pages   = {2579--2605},
  url     = {http://jmlr.org/papers/v9/vandermaaten08a.html}
}

@article{Coifman+2005,
	author = {R. R. Coifman and S. Lafon and A. B. Lee and M. Maggioni and B. Nadler and F. Warner and S. W. Zucker},
	doi = {10.1073/pnas.0500334102},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0500334102},
	journal = {Proceedings of the National Academy of Sciences},
	number = {21},
	pages = {7426-7431},
	title = {Geometric diffusions as a tool for harmonic analysis and structure definition of data: Diffusion maps},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0500334102},
	volume = {102},
	year = {2005},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0500334102},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0500334102}}
@inproceedings{Carreira-Perpinan2010,
author = {Carreira-Perpi\~{n}an, Miguel \'{A}.},
title = {The elastic embedding algorithm for dimensionality reduction},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {We propose a new dimensionality reduction method, the elastic embedding (EE), that optimises an intuitive, nonlinear objective function of the low-dimensional coordinates of the data. The method reveals a fundamental relation betwen a spectral method, Laplacian eigenmaps, and a nonlinear method, stochastic neighbour embedding; and shows that EE can be seen as learning both the coordinates and the affinities between data points. We give a homotopy method to train EE, characterise the critical value of the homotopy parameter, and study the method's behaviour. For a fixed homotopy parameter, we give a globally convergent iterative algorithm that is very effective and requires no user parameters. Finally, we give an extension to out-of-sample points. In standard datasets, EE obtains results as good or better than those of SNE, but more efficiently and robustly.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {167–174},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}
@inproceedings{Wu-Fischer2020,
title={Phase Transitions for the Information Bottleneck in Representation Learning},
author={Tailin Wu and Ian Fischer},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJloElBYvB}
}
@article{Wattemnerg+2016,
  author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
  title = {How to Use t-SNE Effectively},
  journal = {Distill},
  year = {2016},
  url = {http://distill.pub/2016/misread-tsne},
  doi = {10.23915/distill.00002}
}
@article{vanderMaaten2014,
  author  = {Laurens van der Maaten},
  title   = {Accelerating t-SNE using Tree-Based Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {93},
  pages   = {3221--3245},
  url     = {http://jmlr.org/papers/v15/vandermaaten14a.html}
}
@article{McInnes+2018, doi = {10.21105/joss.00861}, url = {https://doi.org/10.21105/joss.00861}, year = {2018}, publisher = {The Open Journal}, volume = {3}, number = {29}, pages = {861}, author = {Leland McInnes and John Healy and Nathaniel Saul and Lukas Großberger}, title = {UMAP: Uniform Manifold Approximation and Projection}, journal = {Journal of Open Source Software} } 

@inproceedings{Weinberger+2005,
	author = {Weinberger, Kilian Q and Blitzer, John and Saul, Lawrence},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
	publisher = {MIT Press},
	title = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
	url = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf},
	volume = {18},
	year = {2005},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2005/file/a7f592cef8b130a6967a90617db5681b-Paper.pdf}}
@article{Weinberger-Saul2009,
  author  = {Kilian Q. Weinberger and Lawrence K. Saul},
  title   = {Distance Metric Learning for Large Margin Nearest Neighbor Classification},
  journal = {Journal of Machine Learning Research},
  year    = {2009},
  volume  = {10},
  number  = {9},
  pages   = {207-244},
  url     = {http://jmlr.org/papers/v10/weinberger09a.html}
}

@inproceedings{Goldberger+2004,
	author = {Goldberger, Jacob and Hinton, Geoffrey E and Roweis, Sam and Salakhutdinov, Russ R},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {L. Saul and Y. Weiss and L. Bottou},
	publisher = {MIT Press},
	title = {Neighbourhood Components Analysis},
	url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/42fe880812925e520249e808937738d2-Paper.pdf},
	volume = {17},
	year = {2004},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2004/file/42fe880812925e520249e808937738d2-Paper.pdf}}

@inproceedings{Musgrave+2020,
	abstract = {Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental methodology of numerous metric learning papers, and show that the actual improvements over time have been marginal at best. Code is available at github.com/KevinMusgrave/powerful-benchmarker.},
	address = {Cham},
	author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
	booktitle = {Computer Vision -- ECCV 2020},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58595-2},
	pages = {681--699},
	publisher = {Springer International Publishing},
	title = {A Metric Learning Reality Check},
	year = {2020}}
@article{Chopra+2005,
  title={Learning a similarity metric discriminatively, with application to face verification},
  author={Sumit Chopra and Raia Hadsell and Yann LeCun},
  journal={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05)},
  year={2005},
  volume={1},
  pages={539-546},
  url={https://doi.org/10.1109/CVPR.2005.202}
}
@InProceedings{Schroff+2015,
author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
title = {FaceNet: A Unified Embedding for Face Recognition and Clustering},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015},
url             = {https://openaccess.thecvf.com/content_cvpr_2015/html/Schroff_FaceNet_A_Unified_2015_CVPR_paper.html},
}

@inproceedings{Sohn2016,
	author = {Sohn, Kihyuk},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Improved Deep Metric Learning with Multi-class N-pair Loss Objective},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf}}
@INPROCEEDINGS{Movshovitz-Attias+2017,
author = {Y. Movshovitz-Attias and A. Toshev and T. K. Leung and S. Ioffe and S. Singh},
booktitle = {2017 IEEE International Conference on Computer Vision (ICCV)},
title = {No Fuss Distance Metric Learning Using Proxies},
year = {2017},
volume = {},
issn = {2380-7504},
pages = {360-368},
abstract = {We address the problem of distance metric learning (DML), defined as learning a distance consistent with a notion of semantic similarity. Traditionally, for this problem supervision is expressed in the form of sets of points that follow an ordinal relationship - an anchor point x is similar to a set of positive points Y , and dissimilar to a set of negative points Z, and a loss defined over these distances is minimized. While the specifics of the optimization differ, in this work we collectively call this type of supervision Triplets and all methods that follow this pattern Triplet-Based methods. These methods are challenging to optimize. A main issue is the need for finding informative triplets, which is usually achieved by a variety of tricks such as increasing the batch size, hard or semi-hard triplet mining, etc. Even with these tricks, the convergence rate of such methods is slow. In this paper we propose to optimize the triplet loss on a different space of triplets, consisting of an anchor data point and similar and dissimilar proxy points which are learned as well. These proxies approximate the original data points, so that a triplet loss over the proxies is a tight upper bound of the original loss. This proxy-based loss is empirically better behaved. As a result, the proxy-loss improves on state-of-art results for three standard zero-shot learning datasets, by up to 15% points, while converging three times as fast as other triplet-based losses.},
keywords = {training;measurement;computer vision;optimization;convergence;fasteners;training data},
doi = {10.1109/ICCV.2017.47},
url = {https://doi.ieeecomputersociety.org/10.1109/ICCV.2017.47},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {oct}
}

@InProceedings{Qian+2019,
author = {Qian, Qi and Shang, Lei and Sun, Baigui and Hu, Juhua and Li, Hao and Jin, Rong},
title = {SoftTriple Loss: Deep Metric Learning Without Triplet Sampling},
booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019},
url             = {https://openaccess.thecvf.com/content_ICCV_2019/html/Qian_SoftTriple_Loss_Deep_Metric_Learning_Without_Triplet_Sampling_ICCV_2019_paper.html},
}

@article{Nadaraya1964,
	abstract = { A study is made of certain properties of an approximation to the regression line on the basis of sampling data when the sample size increases unboundedly. },
	author = {Nadaraya, E. A.},
	doi = {10.1137/1109020},
	eprint = {https://doi.org/10.1137/1109020},
	journal = {Theory of Probability \& Its Applications},
	number = {1},
	pages = {141-142},
	title = {On Estimating Regression},
	url = {https://doi.org/10.1137/1109020},
	volume = {9},
	year = {1964},
	bdsk-url-1 = {https://doi.org/10.1137/1109020}}
@article{Watson1964,
 ISSN = {0581572X},
 URL = {http://www.jstor.org/stable/25049340},
 abstract = {Few would deny that the most powerful statistical tool is graph paper. When however there are many observations (and/or many variables) graphical procedures become tedious. It seems to the author that the most characteristic problem for statisticians at the moment is the development of methods for analyzing the data poured out by electronic observing systems. The present paper gives a simple computer method for obtaining a "graph" from a large number of observations.},
 author = {Geoffrey S. Watson},
 journal = {Sankhyā: The Indian Journal of Statistics, Series A (1961-2002)},
 number = {4},
 pages = {359--372},
 publisher = {Springer},
 title = {Smooth Regression Analysis},
 urldate = {2024-08-11},
 volume = {26},
 year = {1964}
}

@article{Cleveland-Devlin1988,
	author = {William S. Cleveland and Susan J. Devlin},
	doi = {10.1080/01621459.1988.10478639},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478639},
	journal = {Journal of the American Statistical Association},
	number = {403},
	pages = {596--610},
	publisher = {Taylor \& Francis},
	title = {Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478639},
	volume = {83},
	year = {1988},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478639},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1988.10478639}}

@article{Cleveland1979,
	author = {William S. Cleveland},
	doi = {10.1080/01621459.1979.10481038},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1979.10481038},
	journal = {Journal of the American Statistical Association},
	number = {368},
	pages = {829--836},
	publisher = {Taylor \& Francis},
	title = {Robust Locally Weighted Regression and Smoothing Scatterplots},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1979.10481038},
	volume = {74},
	year = {1979},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1979.10481038},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1979.10481038}}

@article{Savitzky-Golay1964,
	annote = {doi: 10.1021/ac60214a047},
	author = {Savitzky, Abraham. and Golay, M. J. E.},
	date = {1964/07/01},
	date-added = {2024-08-12 00:30:38 +0900},
	date-modified = {2024-08-12 00:30:38 +0900},
	doi = {10.1021/ac60214a047},
	isbn = {0003-2700},
	journal = {Analytical Chemistry},
	journal1 = {Analytical Chemistry},
	journal2 = {Anal. Chem.},
	month = {07},
	number = {8},
	pages = {1627--1639},
	publisher = {American Chemical Society},
	title = {Smoothing and Differentiation of Data by Simplified Least Squares Procedures.},
	type = {doi: 10.1021/ac60214a047},
	url = {https://doi.org/10.1021/ac60214a047},
	volume = {36},
	year = {1964},
	year1 = {1964},
	bdsk-url-1 = {https://doi.org/10.1021/ac60214a047}}

@inproceedings{Chaudhuri-DasGupta2014,
	author = {Chaudhuri, Kamalika and Dasgupta, Sanjoy},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Rates of Convergence for Nearest Neighbor Classification},
	url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf},
	volume = {27},
	year = {2014},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2014/file/db957c626a8cd7a27231adfbf51e20eb-Paper.pdf}}

@article{Gonzalez1985,
	abstract = {The problem of clustering a set of points so as to minimize the maximum intercluster distance is studied. An O(kn) approximation algorithm, where n is the number of points and k is the number of clusters, that guarantees solutions with an objective function value within two times the optimal solution value is presented. This approximation algorithm succeeds as long as the set of points satisfies the triangular inequality. We also show that our approximation algorithm is best possible, with respect to the approximation bound, if P ≠ NP.},
	author = {Teofilo F. Gonzalez},
	doi = {https://doi.org/10.1016/0304-3975(85)90224-5},
	issn = {0304-3975},
	journal = {Theoretical Computer Science},
	keywords = {Algorithms, clustering, NP-completeness, approximation algorithms, minimizing the maximum intercluster distance},
	pages = {293-306},
	title = {Clustering to minimize the maximum intercluster distance},
	url = {https://www.sciencedirect.com/science/article/pii/0304397585902245},
	volume = {38},
	year = {1985},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0304397585902245},
	bdsk-url-2 = {https://doi.org/10.1016/0304-3975(85)90224-5}}
@inproceedings{David-Sergei2007,
author = {Arthur, David and Vassilvitskii, Sergei},
title = {k-means++: the advantages of careful seeding},
year = {2007},
isbn = {9780898716245},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, randomized seeding technique, we obtain an algorithm that is Θ(logk)-competitive with the optimal clustering. Preliminary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
booktitle = {Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {1027–1035},
numpages = {9},
location = {New Orleans, Louisiana},
series = {SODA '07}
}
@inproceedings{Kaufmann-Rousseeuw1987,
    author          = {L. Kaufmann and P. Rousseeuw},
    year            = {1987},
    title           = {{Clustering by Means of Medoids}},
    booktitle       = {Proceedings of Statistical Data Analysis Based on the L1 Norm Conference},
    volume          = {},
    pages           = {405-416},
    url             = {}
}

@article{Park-Jun2009,
	abstract = {This paper proposes a new algorithm for K-medoids clustering which runs like the K-means algorithm and tests several methods for selecting initial medoids. The proposed algorithm calculates the distance matrix once and uses it for finding new medoids at every iterative step. To evaluate the proposed algorithm, we use some real and artificial data sets and compare with the results of other algorithms in terms of the adjusted Rand index. Experimental results show that the proposed algorithm takes a significantly reduced time in computation with comparable performance against the partitioning around medoids.},
	author = {Hae-Sang Park and Chi-Hyuck Jun},
	doi = {https://doi.org/10.1016/j.eswa.2008.01.039},
	issn = {0957-4174},
	journal = {Expert Systems with Applications},
	keywords = {Clustering, K-means, K-medoids, Rand index},
	number = {2, Part 2},
	pages = {3336-3341},
	title = {A simple and fast algorithm for K-medoids clustering},
	url = {https://www.sciencedirect.com/science/article/pii/S095741740800081X},
	volume = {36},
	year = {2009},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S095741740800081X},
	bdsk-url-2 = {https://doi.org/10.1016/j.eswa.2008.01.039}}

@inproceedings{Mnih-Kavukcuoglu2013,
	author = {Mnih, Andriy and Kavukcuoglu, Koray},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {C.J. Burges and L. Bottou and M. Welling and Z. Ghahramani and K.Q. Weinberger},
	publisher = {Curran Associates, Inc.},
	title = {Learning word embeddings efficiently with noise-contrastive estimation},
	url = {https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf},
	volume = {26},
	year = {2013},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2013/file/db2b4182156b2f1f817860ac9f409ad7-Paper.pdf}}

@InProceedings{Chen+2020SimCLR,
  title = 	 {A Simple Framework for Contrastive Learning of Visual Representations},
  author =       {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {1597--1607},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/chen20j/chen20j.pdf},
  url = 	 {https://proceedings.mlr.press/v119/chen20j.html},
  abstract = 	 {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5% top-1 accuracy, which is a 7% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1% of the labels, we achieve 85.8% top-5 accuracy, outperforming AlexNet with 100X fewer labels.}
}

@inproceedings{Tian+2020,
	abstract = {Humans view the world through many sensory channels, e.g., the long-wavelength light channel, viewed by the left eye, or the high-frequency vibrations channel, heard by the right ear. Each view is noisy and incomplete, but important factors, such as physics, geometry, and semantics, tend to be shared between all views (e.g., a ``dog'' can be seen, heard, and felt). We investigate the classic hypothesis that a powerful representation is one that models view-invariant factors. We study this hypothesis under the framework of multiview contrastive learning, where we learn a representation that aims to maximize mutual information between different views of the same scene but is otherwise compact. Our approach scales to any number of views, and is view-agnostic. We analyze key properties of the approach that make it work, finding that the contrastive loss outperforms a popular alternative based on cross-view prediction, and that the more views we learn from, the better the resulting representation captures underlying scene semantics. Code is available at: http://github.com/HobbitLong/CMC/.},
	address = {Cham},
	author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
	booktitle = {Computer Vision -- ECCV 2020},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	isbn = {978-3-030-58621-8},
	pages = {776--794},
	publisher = {Springer International Publishing},
	title = {Contrastive Multiview Coding},
	year = {2020}}
@misc{Faghri+2018,
      title={VSE++: Improving Visual-Semantic Embeddings with Hard Negatives}, 
      author={Fartash Faghri and David J. Fleet and Jamie Ryan Kiros and Sanja Fidler},
      year={2018},
      eprint={1707.05612},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1707.05612}, 
}

@inproceedings{Tian+2020WhatMakes,
	author = {Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {6827--6839},
	publisher = {Curran Associates, Inc.},
	title = {What Makes for Good Views for Contrastive Learning?},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/4c2e5eaae9152079b9e95845750bb9ab-Paper.pdf}}

@inproceedings{Khosla+2020,
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {18661--18673},
	publisher = {Curran Associates, Inc.},
	title = {Supervised Contrastive Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d89a66c7c80a29b1bdbab0f2a1a94af8-Paper.pdf}}
@INPROCEEDINGS{Caron+2021,
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jegou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Emerging Properties in Self-Supervised Vision Transformers}, 
  year={2021},
  volume={},
  number={},
  pages={9630-9640},
  keywords={Training;Image segmentation;Computer vision;Semantics;Layout;Image retrieval;Computer architecture;Representation learning;Recognition and classification;Transfer/Low-shot/Semi/Unsupervised Learning},
  doi={10.1109/ICCV48922.2021.00951}}

@inproceedings{Grill+2020,
	author = {Grill, Jean-Bastien and Strub, Florian and Altch\'{e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and Piot, Bilal and kavukcuoglu, koray and Munos, Remi and Valko, Michal},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {21271--21284},
	publisher = {Curran Associates, Inc.},
	title = {Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/f3ada80d5c4ee70142b17b8192b2958e-Paper.pdf}}

@InProceedings{Zbontar+2021,
  title = 	 {Barlow Twins: Self-Supervised Learning via Redundancy Reduction},
  author =       {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stephane},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {12310--12320},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/zbontar21a/zbontar21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/zbontar21a.html},
  abstract = 	 {Self-supervised learning (SSL) is rapidly closing the gap with supervised methods on large computer vision benchmarks. A successful approach to SSL is to learn embeddings which are invariant to distortions of the input sample. However, a recurring issue with this approach is the existence of trivial constant solutions. Most current methods avoid such solutions by careful implementation details. We propose an objective function that naturally avoids collapse by measuring the cross-correlation matrix between the outputs of two identical networks fed with distorted versions of a sample, and making it as close to the identity matrix as possible. This causes the embedding vectors of distorted versions of a sample to be similar, while minimizing the redundancy between the components of these vectors. The method is called Barlow Twins, owing to neuroscientist H. Barlow’s redundancy-reduction principle applied to a pair of identical networks. Barlow Twins does not require large batches nor asymmetry between the network twins such as a predictor network, gradient stopping, or a moving average on the weight updates. Intriguingly it benefits from very high-dimensional output vectors. Barlow Twins outperforms previous methods on ImageNet for semi-supervised classification in the low-data regime, and is on par with current state of the art for ImageNet classification with a linear classifier head, and for transfer tasks of classification and object detection.}
}

@inproceedings{Gretton+2007,
	author = {Gretton, Arthur and Fukumizu, Kenji and Teo, Choon and Song, Le and Sch\"{o}lkopf, Bernhard and Smola, Alex},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	publisher = {Curran Associates, Inc.},
	title = {A Kernel Statistical Test of Independence},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf},
	volume = {20},
	year = {2007},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d5cfead94f5350c12c322b5b664544c1-Paper.pdf}}

@InProceedings{Roeder+2021,
  title = 	 {On Linear Identifiability of Learned Representations},
  author =       {Roeder, Geoffrey and Metz, Luke and Kingma, Durk},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {9030--9039},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/roeder21a/roeder21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/roeder21a.html},
  abstract = 	 {Identifiability is a desirable property of a statistical model: it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions lack identifiability in parameter space, because they are over-parameterized by design. In this paper, building on recent advances in nonlinear Independent Components Analysis, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data.}
}

@inproceedings{Halva+2021,
	author = {H\"{a}lv\"{a}, Hermanni and Le Corff, Sylvain and Leh\'{e}ricy, Luc and So, Jonathan and Zhu, Yongjie and Gassiat, Elisabeth and Hyvarinen, Aapo},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
	pages = {1624--1633},
	publisher = {Curran Associates, Inc.},
	title = {Disentangling Identifiable Features from Noisy Data with Structured Nonlinear ICA},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0cdbb4e65815fbaf79689b15482e7575-Paper.pdf},
	volume = {34},
	year = {2021},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2021/file/0cdbb4e65815fbaf79689b15482e7575-Paper.pdf}}
@article{Bell-Sejnowski1995,
    author = {Bell, Anthony J. and Sejnowski, Terrence J.},
    title = "{An Information-Maximization Approach to Blind Separation and Blind Deconvolution}",
    journal = {Neural Computation},
    volume = {7},
    number = {6},
    pages = {1129-1159},
    year = {1995},
    month = {11},
    abstract = "{We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in "blind" signal processing.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1995.7.6.1129},
    url = {https://doi.org/10.1162/neco.1995.7.6.1129},
    eprint = {https://direct.mit.edu/neco/article-pdf/7/6/1129/813064/neco.1995.7.6.1129.pdf},
}
@incollection{Barlow1961,
    author = {Barlow, H. B.},
    isbn = {9780262518420},
    title = "{Possible Principles Underlying the Transformations of Sensory Messages}",
    booktitle = "{Sensory Communication}",
    publisher = {The MIT Press},
    year = {1961},
    month = {09},
    abstract = "{This chapter is an attempt to formulate ideas about the operations performed by physiological mechanisms, and not merely a discussion of the physiological mechanisms of sensory pathways. It presents three hypotheses regarding the purpose of sensory relays. The first one is the “password” hypothesis, which posits that, since animals respond specifically to specific stimuli, their sensory pathways must possess mechanisms for detecting such stimuli and discriminating between them. The second hypothesis is the fashionable one that relays act as control points at which the flow of information is modulated according to the requirements of other parts of the nervous system. Finally, the third hypothesis theorizes that reduction of redundancy is an important principle guiding the organization of sensory messages and is carried out at relays in the sensory pathways.}",
    doi = {10.7551/mitpress/9780262518420.003.0013},
    url = {https://doi.org/10.7551/mitpress/9780262518420.003.0013},
    eprint = {https://academic.oup.com/mit-press-scholarship-online/book/0/chapter/180090664/chapter-ag-pdf/44697172/book\_20714\_section\_180090664.ag.pdf},
}

@article{Barlow1972,
	abstract = { The problem discussed is the relationship between the firing of single neurons in sensory pathways and subjectively experienced sensations. The conclusions are formulated as the following five dogmas:To understand nervous function one needs to look at interactions at a cellular level, rather than either a more macroscopic or microscopic level, because behaviour depends upon the organized pattern of these intercellular interactions.The sensory system is organized to achieve as complete a representation of the sensory stimulus as possible with the minimum number of active neurons.Trigger features of sensory neurons are matched to redundant patterns of stimulation by experience as well as by developmental processes.Perception corresponds to the activity of a small selection from the very numerous high-level neurons, each of which corresponds to a pattern of external events of the order of complexity of the events symbolized by a word.High impulse frequency in such neurons corresponds to high certainty that the trigger feature is present.The development of the concepts leading up to these speculative dogmas, their experimental basis, and some of their limitations are discussed. },
	author = {H B Barlow},
	doi = {10.1068/p010371},
	eprint = {https://doi.org/10.1068/p010371},
	journal = {Perception},
	note = {PMID: 4377168},
	number = {4},
	pages = {371-394},
	title = {Single Units and Sensation: A Neuron Doctrine for Perceptual Psychology?},
	url = {https://doi.org/10.1068/p010371},
	volume = {1},
	year = {1972},
	bdsk-url-1 = {https://doi.org/10.1068/p010371}}

@article{島崎秀昭2019,
	author = {島崎秀昭},
	doi = {10.3902/jnns.26.72},
	journal = {日本神経回路学会誌},
	number = {3},
	pages = {72-98},
	title = {ベイズ統計と熱力学から見る生物の学習と認識のダイナミクス},
	volume = {26},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.3902/jnns.26.72}}

@article{Laughlin1981,
	author = {Simon Laughlin},
	doi = {doi:10.1515/znc-1981-9-1040},
	journal = {Zeitschrift f{\"u}r Naturforschung C},
	lastchecked = {2024-08-12},
	number = {9-10},
	pages = {910--912},
	url = {https://doi.org/10.1515/znc-1981-9-1040},
	volume = {36},
	year = {1981},
	bdsk-url-1 = {https://doi.org/10.1515/znc-1981-9-1040},
	title     = {A Simple Coding Procedure Enhances a Neuron's Information Capacity},
	}

@article{Brenner+2000,
	abstract = {Adaptation is a widespread phenomenon in nervous systems, providing flexibility to function under varying external conditions. Here, we relate an adaptive property of a sensory system directly to its function as a carrier of information about input signals. We show that the input/output relation of a sensory system in a dynamic environment changes with the statistical properties of the environment. Specifically, when the dynamic range of inputs changes, the input/output relation rescales so as to match the dynamic range of responses to that of the inputs. We give direct evidence that the scaling of the input/output relation is set to maximize information transmission for each distribution of signals. This adaptive behavior should be particularly useful in dealing with the intermittent statistics of natural signals.},
	author = {Naama Brenner and William Bialek and Rob {de Ruyter van Steveninck}},
	doi = {https://doi.org/10.1016/S0896-6273(00)81205-2},
	issn = {0896-6273},
	journal = {Neuron},
	number = {3},
	pages = {695-702},
	title = {Adaptive Rescaling Maximizes Information Transmission},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627300812052},
	volume = {26},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0896627300812052},
	bdsk-url-2 = {https://doi.org/10.1016/S0896-6273(00)81205-2}}
@book{Doya+2006,
    author = {Doya, Kenji and Ishii, Shin and Pouget, Alexandre and Rao, Rajesh P.N.},
    title = "{Bayesian Brain: Probabilistic Approaches to Neural Coding }",
    publisher = {The MIT Press},
    year = {2006},
    month = {12},
    abstract = "{Experimental and theoretical neuroscientists use Bayesian approaches to analyze the brain mechanisms of perception, decision-making, and motor control.A Bayesian approach can contribute to an understanding of the brain on multiple levels, by giving normative predictions about how an ideal sensory system should combine prior knowledge and observation, by providing mechanistic interpretation of the dynamic functioning of the brain circuit, and by suggesting optimal ways of deciphering experimental data. Bayesian Brain brings together contributions from both experimental and theoretical neuroscientists that examine the brain mechanisms of perception, decision making, and motor control according to the concepts of Bayesian estimation.After an overview of the mathematical concepts, including Bayes' theorem, that are basic to understanding the approaches discussed, contributors discuss how Bayesian concepts can be used for interpretation of such neurobiological data as neural spikes and functional brain imaging. Next, contributors examine the modeling of sensory processing, including the neural coding of information about the outside world. Finally, contributors explore dynamic processes for proper behaviors, including the mathematics of the speed and accuracy of perceptual decisions and neural models of belief propagation.}",
    isbn = {9780262294188},
    doi = {10.7551/mitpress/9780262042383.001.0001},
    url = {https://doi.org/10.7551/mitpress/9780262042383.001.0001},
}
@INPROCEEDINGS{Romaszko+2017,
  author={Romaszko, Lukasz and Williams, Christopher K. I. and Moreno, Pol and Kohli, Pushmeet},
  booktitle={2017 IEEE International Conference on Computer Vision Workshops (ICCVW)}, 
  title={Vision-as-Inverse-Graphics: Obtaining a Rich 3D Explanation of a Scene from a Single Image}, 
  year={2017},
  volume={},
  number={},
  pages={940-948},
  keywords={Cameras;Probabilistic logic;Lighting;Detectors;Object detection;Graphics;Transforms},
  doi={10.1109/ICCVW.2017.115}}
@INPROCEEDINGS{Tishby-Zaslavsky2015,
  author={Tishby, Naftali and Zaslavsky, Noga},
  booktitle={2015 IEEE Information Theory Workshop (ITW)}, 
  title={Deep learning and the information bottleneck principle}, 
  year={2015},
  volume={},
  number={},
  pages={1-5},
  keywords={Distortion;Complexity theory;Mutual information;Bifurcation;Computer architecture;Feature extraction;Training},
  doi={10.1109/ITW.2015.7133169}}
@misc{Tishby+2000,
      title={The information bottleneck method}, 
      author={Naftali Tishby and Fernando C. Pereira and William Bialek},
      year={2000},
      eprint={physics/0004057},
      archivePrefix={arXiv},
      primaryClass={physics.data-an},
      url={https://arxiv.org/abs/physics/0004057}, 
}
@misc{Andrieu+2024,
      title={Gradient-free optimization via integration}, 
      author={Christophe Andrieu and Nicolas Chopin and Ettore Fincato and Mathieu Gerber},
      year={2024},
      eprint={2408.00888},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2408.00888}, 
}

@book{足立-山本2024,
	author         = {足立浩平 and 山本倫生},
	year           = {2024},
	title          = {主成分分析と因子分析―特異値分解を出発点として―},
	series         = {},
	volume         = {},
	edition        = {},
	url            = {https://www.kyoritsu-pub.co.jp/book/b10085699.html},
	publisher      = {共立出版}
}

@book{Thurstone1947,
	author         = {L. L. Thurstone},
	year           = {1947},
	title          = {Multiple Factor Analysis},
	series         = {},
	volume         = {},
	edition        = {},
	url            = {},
	publisher      = {University of Chicago Press}
}

@article{Stewart1993,
	abstract = { This paper surveys the contributions of five mathematicians---Eugenio Beltrami (1835--1899), Camille Jordan (1838--1921), James Joseph Sylvester (1814--1897), Erhard Schmidt (1876--1959), and Hermann Weyl (1885--1955)---who were responsible for establishing the existence of the singular value decomposition and developing its theory. },
	author = {Stewart, G. W.},
	doi = {10.1137/1035134},
	eprint = {https://doi.org/10.1137/1035134},
	journal = {SIAM Review},
	number = {4},
	pages = {551-566},
	title = {On the Early History of the Singular Value Decomposition},
	url = {https://doi.org/10.1137/1035134},
	volume = {35},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1137/1035134}}
@article{Sylvester1889,
	author          = {James Joseph Sylvester},
	year            = {1889},
	title           = {On The Reduction of a Bilinear Quantic of the $n$-th Order to the Form of a Sum of $n$ Products by a Double Orthogonal Substitution},
	journal         = {The Messenger of Mathematics},
	volume          = {18},
	number          = {},
	pages           = {42-46},
	url             = {https://books.google.co.jp/books?id=cxRKAQAAMAAJ&pg=PA1&hl=ja&source=gbs_toc_r&cad=2#v=onepage&q&f=false}
}
@article{Beltrami1873,
	author          = {E. Beltrami},
	year            = {1873},
	title           = {Sulle Funzioni Bilineari},
	journal         = {Giornale di Matematiche ad Uso degli Studenti Delle Universita},
	volume          = {11},
	number          = {},
	pages           = {98-106},
	url             = {https://gallica.bnf.fr/ark:/12148/bpt6k99434d/f442}
}
@article{Moore1920,
	author          = {E. H. Moore},
	year            = {1920},
	title           = {On the Reciprocal of the General Algebraic Matrix},
	journal         = {Bulletin of the American Mathematical Society},
	volume          = {26},
	number          = {9},
	pages           = {394-395},
	url             = {https://doi.org/10.1090%2FS0002-9904-1920-03322-7},
	note            = {In The fourteenth western meeting of the American Mathematical Society},
}
@article{Penrose1955, title={A generalized inverse for matrices}, volume={51}, DOI={10.1017/S0305004100030401}, number={3}, journal={Mathematical Proceedings of the Cambridge Philosophical Society}, author={Penrose, R.}, year={1955}, pages={406–413}}

@article{Halko+2011,
	abstract = { Low-rank matrix approximations, such as the truncated singular value decomposition and the rank-revealing QR decomposition, play a central role in data analysis and scientific computing. This work surveys and extends recent research which demonstrates that randomization offers a powerful tool for performing low-rank matrix approximation. These techniques exploit modern computational architectures more fully than classical methods and open the possibility of dealing with truly massive data sets. This paper presents a modular framework for constructing randomized algorithms that compute partial matrix decompositions. These methods use random sampling to identify a subspace that captures most of the action of a matrix. The input matrix is then compressed---either explicitly or implicitly---to this subspace, and the reduced matrix is manipulated deterministically to obtain the desired low-rank factorization. In many cases, this approach beats its classical competitors in terms of accuracy, robustness, and/or speed. These claims are supported by extensive numerical experiments and a detailed error analysis. The specific benefits of randomized techniques depend on the computational environment. Consider the model problem of finding the k dominant components of the singular value decomposition of an \$m \times n\$ matrix. (i) For a dense input matrix, randomized algorithms require \$\bigO(mn \log(k))\$ floating-point operations (flops) in contrast to \$ \bigO(mnk)\$ for classical algorithms. (ii) For a sparse input matrix, the flop count matches classical Krylov subspace methods, but the randomized approach is more robust and can easily be reorganized to exploit multiprocessor architectures. (iii) For a matrix that is too large to fit in fast memory, the randomized techniques require only a constant number of passes over the data, as opposed to \$\bigO(k)\$ passes for classical algorithms. In fact, it is sometimes possible to perform matrix approximation with a single pass over the data. },
	author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
	doi = {10.1137/090771806},
	eprint = {https://doi.org/10.1137/090771806},
	journal = {SIAM Review},
	number = {2},
	pages = {217-288},
	title = {Finding Structure with Randomness: Probabilistic Algorithms for Constructing Approximate Matrix Decompositions},
	url = {https://doi.org/10.1137/090771806},
	volume = {53},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1137/090771806}}
@techreport{Murray+2023,
    Author= {Murray, Riley and Demmel, James and Mahoney, Michael W. and Erichson, N. Benjamin and Melnichenko, Maksim and Malik, Osman Asif and Grigori, Laura and Luszczek, Piotr and Derezinski, Michal and Lopes, Miles E. and Liang, Tianyu and Luo, Hengrui and Dongarra, Jack},
    Title= {Randomized Numerical Linear Algebra: A Perspective on the Field With an Eye to Software},
    Year= {2023},
    Month= {Feb},
    Url= {http://www2.eecs.berkeley.edu/Pubs/TechRpts/2023/EECS-2023-19.html},
    Number= {UCB/EECS-2023-19},
}
@article{Drineas+2016,
author = {Drineas, Petros and Mahoney, Michael W.},
title = {RandNLA: randomized numerical linear algebra},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/2842602},
doi = {10.1145/2842602},
abstract = {Randomization offers new benefits for large-scale linear algebra computations.},
journal = {Commun. ACM},
month = {may},
pages = {80–90},
numpages = {11}
}

@inproceedings{Roweis1997,
	author = {Roweis, Sam},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Jordan and M. Kearns and S. Solla},
	publisher = {MIT Press},
	title = {EM Algorithms for PCA and SPCA},
	url = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf},
	volume = {10},
	year = {1997},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/1997/file/d9731321ef4e063ebbee79298fa36f56-Paper.pdf}}
@article{Gabriel1971,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334381},
 abstract = {Any matrix of rank two can be displayed as a biplot which consists of a vector for each row and a vector for each column, chosen so that any element of the matrix is exactly the inner product of the vectors corresponding to its row and to its column. If a matrix is of higher rank, one may display it approximately by a biplot of a matrix of rank two which approximates the original matrix. The biplot provides a useful tool of data analysis and allows the visual appraisal of the structure of large data matrices. It is especially revealing in principal component analysis, where the biplot can show inter-unit distances and indicate clustering of units as well as display variances and correlations of the variables.},
 author = {K. R. Gabriel},
 journal = {Biometrika},
 number = {3},
 pages = {453--467},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Biplot Graphic Display of Matrices with Application to Principal Component Analysis},
 urldate = {2024-08-12},
 volume = {58},
 year = {1971}
}
@article{Gower2004,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/20441132},
 abstract = {A simple geometry allows the main properties of matrix approximations used in biplot displays to be developed. It establishes orthogonal components of an analysis of variance, from which different contributions to approximations may be assessed. Particular attention is paid to approximations that share the same singular vectors, in which case the solution space is a convex cone. Two- and three-dimensional approximations are examined in detail and then the geometry is interpreted for different forms of the matrix being approximated.},
 author = {J. C. Gower},
 journal = {Biometrika},
 number = {3},
 pages = {705--714},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {The Geometry of Biplot Scaling},
 urldate = {2024-08-12},
 volume = {91},
 year = {2004}
}
@article{Spearman1904,
    author          = {Spearman, Charles},
    title           = {'General intelligence,' objectively determined and measured},
    year            = {1904},
    journal         = {The American Journal of Psychology},
    volume          = {15},
    number          = {2},
    pages           = {201-293},
	url             = {https://psycnet.apa.org/doi/10.2307/1412107},
}
@book{豊田秀樹1992,
	author         = {豊田秀樹},
	year           = {1992},
	title          = {SASによる共分散構造分析 },
	series         = {ＳＡＳで学ぶ統計的データ解析},
	volume         = {3},
	edition        = {},
	url            = {https://www.utp.or.jp/book/b302422.html},
	publisher      = {東京大学出版会}
}

@article{Muthen2002,
	abstract = {This article gives an overview of statistical analysis with latent variables. Using traditional structural equation modeling as a starting point, it shows how the idea of latent variables captures a wide variety of statistical concepts, including random effects, missing data, sources of variation in hierarchical data, finite mixtures. latent classes, and clusters. These latent variable applications go beyond the traditional latent variable useage in psychometrics with its focus on measurement error and hypothetical constructs measured by multiple indicators. The article argues for the value of integrating statistical and psychometric modeling ideas. Different applications are discussed in a unifying framework that brings together in one general model such different analysis types as factor models, growth curve models, multilevel models, latent class models and discrete-time survival models. Several possible combinations and extensions of these models are made clear due to the unifying framework.},
	author = {Muth{\'e}n, Bengt O. },
	date = {2002/01/01},
	date-added = {2024-08-13 12:06:06 +0900},
	date-modified = {2024-08-13 12:06:06 +0900},
	doi = {10.2333/bhmk.29.81},
	id = {Muth{\'e}n2002},
	isbn = {1349-6964},
	journal = {Behaviormetrika},
	number = {1},
	pages = {81--117},
	title = {{Beyond SEM: General Latent Variable Modeling}},
	url = {https://doi.org/10.2333/bhmk.29.81},
	volume = {29},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.2333/bhmk.29.81}}

@article{狩野裕2002,
	author = {狩野裕},
	doi = {10.2333/jbhmk.29.138},
	journal = {行動計量学},
	number = {2},
	pages = {138-159},
	title = {構造方程式モデリングは，因子分析，分散分析，パス解析の すべてにとって代わるのか？},
	volume = {29},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.29.138}}

@article{狩野裕2019,
	author = {狩野裕},
	doi = {10.11329/jjssj.48.199},
	journal = {日本統計学会誌},
	number = {2},
	pages = {199-214},
	title = {欠測データ解析のmissとmyth},
	volume = {48},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.11329/jjssj.48.199}}


@phdthesis{Socan2003,
	author      = {G. Socan},
	school      = {Rijksuniversiteit Groningen },
	title       = {The Incremental Value of Rank Factor Analysis},
	year        = {2003}
}

@article{足立浩平+2019,
	author = {足立浩平 and 伊藤真道 and 宇野光平},
	doi = {10.20551/jscswabun.32.1_61},
	journal = {計算機統計学},
	number = {1},
	pages = {61-77},
	title = {行列分解に基づく因子分析とその新展開},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.20551/jscswabun.32.1_61}}
@article{Lawley1942, title={XIV.—Further Investigations in Factor Estimation}, volume={61}, DOI={10.1017/S0080454100006178}, number={2}, journal={Proceedings of the Royal Society of Edinburgh. Section A. Mathematical and Physical Sciences}, author={Lawley, D. N.}, year={1942}, pages={176–185}}
@inproceedings{Anderson-Rubin1956,
	author          = {T. W. Anderson and Herman Rubin},
	year            = {1956},
	title           = {Statistical Inference in Factor Analysis},
	booktitle       = {Proceedings of the Thrid Berkeley Symposium on Mathematical Statistics and Probability},
	volume          = {5},
	pages           = {111-150},
	url             = {https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Third-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/Statistical-Inference-in-Factor-Analysis/bsmsp/1200511860}
}

@article{Harman-Jones1966,
	abstract = {This paper is addressed to the classical problem of estimating factor loadings under the condition that the sum of squares of off-diagonal residuals be minimized. Communalities consistent with this criterion are produced as a by-product. The experimental work included several alternative algorithms before a highly efficient method was developed. The final procedure is illustrated with a numerical example. Some relationships of minres to principal-factor analysis and maximum-likelihood factor estimates are discussed, and several unresolved problems are pointed out.},
	author = {Harman, Harry H. and Jones, Wayne H.},
	date = {1966/09/01},
	date-added = {2024-08-13 13:15:59 +0900},
	date-modified = {2024-08-13 13:15:59 +0900},
	doi = {10.1007/BF02289468},
	id = {Harman1966},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {3},
	pages = {351--368},
	title = {Factor analysis by minimizing residuals (minres)},
	url = {https://doi.org/10.1007/BF02289468},
	volume = {31},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289468}}

@article{Harman-Fukuda1966,
	abstract = {In the course of developing the minres method of factor analysis the troublesome situation of communalities greater than one arose. This problem---referred to as the generalized Heywood case---is resolved in this paper by means of a process of minimizing the sum of squares of off-diagonal residuals. The resulting solution is superior to the otherwise very efficient original minres method without requiring additional computing time.},
	author = {Harman, Harry H. and Fukuda, Yoichiro},
	date = {1966/12/01},
	date-added = {2024-08-13 13:19:47 +0900},
	date-modified = {2024-08-13 13:19:47 +0900},
	doi = {10.1007/BF02289525},
	id = {Harman1966},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {563--571},
	title = {Resolution of the heywood case in the minres solution},
	url = {https://doi.org/10.1007/BF02289525},
	volume = {31},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289525}}

@article{Rubin-Thayer1982,
	abstract = {The details of EM algorithms for maximum likelihood factor analysis are presented for both the exploratory and confirmatory models. The algorithm is essentially the same for both cases and involves only simple least squares regression operations; the largest matrix inversion required is for aq ×q symmetric matrix whereq is the matrix of factors. The example that is used demonstrates that the likelihood for the factor analysis model may have multiple modes that are not simply rotations of each other; such behavior should concern users of maximum likelihood factor analysis and certainly should cast doubt on the general utility of second derivatives of the log likelihood as measures of precision of estimation.},
	author = {Rubin, Donald B. and Thayer, Dorothy T.},
	date = {1982/03/01},
	date-added = {2024-08-13 13:21:17 +0900},
	date-modified = {2024-08-13 13:21:17 +0900},
	doi = {10.1007/BF02293851},
	id = {Rubin1982},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {69--76},
	title = {EM algorithms for ML factor analysis},
	url = {https://doi.org/10.1007/BF02293851},
	volume = {47},
	year = {1982},
	bdsk-url-1 = {https://doi.org/10.1007/BF02293851}}
@Inbook{Jöreskog1988,
author="J{\"o}reskog, Karl G.",
editor="Nesselroade, John R.
and Cattell, Raymond B.",
title="Analysis of Covariance Structures",
bookTitle="Handbook of Multivariate Experimental Psychology",
year="1988",
publisher="Springer US",
address="Boston, MA",
pages="207--230",
abstract="Analysis of covariance structures is the common term for a number of techniques for analyzing multivariate data in order to detect and assess latent (unobserved) sources of variation and covariation in the observed measurements. The techniques of covariance structure analysis are general and flexible in that they can handle many types of covariance structures useful especially in the behavioral and social sciences. Although these techniques can be used for exploratory analysis, they have been most successfully applied to confirmatory analysis where the type of covariance structure is specified in advance. A covariance structure of a specified kind may arise because of a specified substantive theory or hypothesis, a given classificatory design for the measures, known experimental conditions, or because of results from previous studies based on extensive data. Sometimes the observed variables are ordered through time, as in longitudinal studies, or according to linear or circular patterns, as in Guttman's (1954) simplex and circumplex models, or according to a given causal scheme, as in path analysis.",
isbn="978-1-4613-0893-5",
doi="10.1007/978-1-4613-0893-5_5",
url="https://doi.org/10.1007/978-1-4613-0893-5_5"
}


@article{Bock-Bargmann1966,
	abstract = {A general method is presented for estimating variance components when the experimental design has one random way of classification and a possibly unbalanced fixed classification. The procedure operates on a sample covariance matrix in which the fixed classes play the role of variables and the random classes correspond to observations. Cases are considered which assume (i) homogeneous and (ii) nonhomogeneous error variance, and (iii) arbitrary scale factors in the measurements and homogeneous error variance. The results include maximum-likelihood estimations of the variance components and scale factors, likelihood-ratio tests of the goodness-of-fit of the model assumed for the design, and large-sample variances and covariances of the estimates. Applications to mental test data are presented. In these applications the subjects constitute the random dimension of the design, and a classification of the mental tests according to objective features of format or content constitute the fixed dimensions.},
	author = {Bock, R. Darrell and Bargmann, Rolf E.},
	date = {1966/12/01},
	date-added = {2024-08-13 13:32:44 +0900},
	date-modified = {2024-08-13 13:32:44 +0900},
	doi = {10.1007/BF02289521},
	id = {Bock1966},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {507--534},
	title = {Analysis of covariance structures},
	url = {https://doi.org/10.1007/BF02289521},
	volume = {31},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289521}}
@article{Joreskog1970,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334833},
 abstract = {It is assumed that observations on a set of variables have a multivariate normal distribution with a general parametric form of the mean vector and the variance-covariance matrix. Any parameter of the model may be fixed, free or constrained to be equal to other parameters. The free and constrained parameters are estimated by maximum likelihood. A wide range of models is obtained from the general model by imposing various specifications on the parametric structure of the general model. Examples are given of areas and problems, especially in the behavioural sciences, where the method may be useful.},
 author = {K. G. Jöreskog},
 journal = {Biometrika},
 number = {2},
 pages = {239--251},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {A General Method for Analysis of Covariance Structures},
 urldate = {2024-08-13},
 volume = {57},
 year = {1970}
}

@article{Joreskog1978,
	abstract = {A general approach to the analysis of covariance structures is considered, in which the variances and covariances or correlations of the observed variables are directly expressed in terms of the parameters of interest. The statistical problems of identification, estimation and testing of such covariance or correlation structures are discussed.},
	author = {J{\"o}reskog, Karl G. },
	date = {1978/12/01},
	date-added = {2024-08-13 13:34:21 +0900},
	date-modified = {2024-08-13 13:34:21 +0900},
	doi = {10.1007/BF02293808},
	id = {J{\"o}reskog1978},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {443--477},
	title = {Structural analysis of covariance and correlation matrices},
	url = {https://doi.org/10.1007/BF02293808},
	volume = {43},
	year = {1978},
	bdsk-url-1 = {https://doi.org/10.1007/BF02293808}}
@techreport{Fornell1985,
	author      = {Claes Fornell},
	institution = {Business, Stephen M. Ross School, University of Michigan},
	title       = {A Second generation of multivariate analysis: classification of methods and implications for marketing research},
	year        = {1985},
	url             = {https://hdl.handle.net/2027.42/35621},
}

@article{豊田秀樹1991,
	author = {豊田秀樹},
	doi = {10.5926/jjep1953.39.4_467},
	journal = {教育心理学研究},
	number = {4},
	pages = {467-478},
	title = {共分散構造分析の下位モデルとその適用例},
	volume = {39},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.5926/jjep1953.39.4_467}}
@inbook{Joreskog-Wold1982,
	author         = {K. G. Jöreskog and H. Wold},
	chapter        = {The ML and PLS Techniques for Modeling with Latent Variables: Historical and Comparative Aspects},
	editor         = {},
	pages          = {263-270},
	publisher      = {North-Holland},
	title          = {Systems under Indirect Observation: Causality, Structure, Prediction},
	year           = {1982}
}

@article{星野崇宏+2005,
	author = {星野崇宏 and 岡田謙介 and 前田忠彦},
	doi = {10.2333/jbhmk.32.209},
	journal = {行動計量学},
	number = {2},
	pages = {209-235},
	title = {構造方程式モデリングにおける適合度指標とモデル改善について : 展望とシミュレーション研究による新たな知見},
	volume = {32},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.2333/jbhmk.32.209}}

@article{Joreskog1967a,
	abstract = {A new computational method for the maximum likelihood solution in factor analysis is presented. This method takes into account the fact that the likelihood function may not have a maximum in a point of the parameter space where all unique variances are positive. Instead, the maximum may be attained on the boundary of the parameter space where one or more of the unique variances are zero. It is demonstrated that suchimproper (Heywood) solutions occur more often than is usually expected. A general procedure to deal with such improper solutions is proposed. The proposed methods are illustrated using two small sets of empirical data, and results obtained from the analyses of many other sets of data are reported. These analyses verify that the new computational method converges rapidly and that the maximum likelihood solution can be determined very accurately. A by-product obtained by the method is a large sample estimate of the variance-covariance matrix of the estimated unique variances. This can be used to set up approximate confidence intervals for communalities and unique variances.},
	author = {J{\"o}reskog, K.  G. },
	date = {1967/12/01},
	date-added = {2024-08-13 14:29:19 +0900},
	date-modified = {2024-08-13 14:29:19 +0900},
	doi = {10.1007/BF02289658},
	id = {J{\"o}reskog1967},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {443--482},
	title = {Some contributions to maximum likelihood factor analysis},
	url = {https://doi.org/10.1007/BF02289658},
	volume = {32},
	year = {1967},
	bdsk-url-1 = {https://doi.org/10.1007/BF02289658}}
@techreport{Joreskog1966,
	author      = {Karl G. Jöreskog},
	institution = {ETS},
	title       = {UMLFA: A Computer Program for Unrestricted Maximum Likelihood Factor Analysis},
	year        = {1966},
	url             = {https://www.ets.org/research/policy_research_reports/publications/report/1966/iazh.html},
}
@book{Grimm-Yarnold2016,
    author         = {Grimm, Laurence G. and Yarnold, Paul R.},
    title          = {研究論文を読み解くための多変量解析入門 応用篇},
    year           = {2016},
    publisher      = {北大路書房},
    series         = {},
    volume         = {},
    month          = {9},
    edition        = {},
    howpublished   = {Reading and Understanding MORE Multivariate Statistics (2020) の翻訳書}
}

@article{Sorbom1974,
	abstract = {A statistical model is developed for the study of similarities and differences in factor structure between several groups. The model assumes that the observed variables satisfy a factor analysis model in each group. A method of data analysis is presented which, in contrast to earlier work, makes use of information in the observed means as well as the observed variances and covariances to estimate the parameters in each group, i.e. factor means, factor loadings, factor variances and covariances and unique variances. Usually the units of measurement in the observed variables have no intrinsic meaning and therefore it is only meaningful to compare the relative magnitudes of the parameters for the different groups. The method estimates the parameters for all groups simultaneously and can take into account a priori information about factorial invariance of various degrees.},
	author = {S{\"o}rbom, Dag},
	doi = {https://doi.org/10.1111/j.2044-8317.1974.tb00543.x},
	eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1974.tb00543.x},
	journal = {British Journal of Mathematical and Statistical Psychology},
	number = {2},
	pages = {229-239},
	title = {{A General Method for Studying Differences in Factor Means and Factor Structure between Groups}},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1974.tb00543.x},
	volume = {27},
	year = {1974},
	bdsk-url-1 = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1974.tb00543.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2044-8317.1974.tb00543.x}}
@article{清水和秋1994,
	author          = {清水和秋},
	year            = {1994},
	title           = {JöreskogとSörbomによるコンピュータ・プログラムと構造方程式モデル},
	journal         = {関西大学社会学部紀要},
	volume          = {25},
	number          = {3},
	pages           = {1-41},
	url             = {http://hdl.handle.net/10112/13345}
}
@article{清水和秋1989,
	author          = {清水和秋},
	year            = {1989},
	title           = {検証的因子分析，LISRELそしてRAMの概要},
	journal         = {関西大学社会学部紀要},
	volume          = {20},
	number          = {2},
	pages           = {61-86},
	url             = {http://hdl.handle.net/10112/13348}
}

@article{McArdle1984,
	author = {J. Jack McArdle},
	doi = {10.1080/00273171.1984.9676927},
	eprint = {https://doi.org/10.1080/00273171.1984.9676927},
	journal = {Multivariate Behavioral Research},
	note = {PMID: 26781893},
	number = {2-3},
	pages = {245--267},
	publisher = {Routledge},
	title = {On the Madness in His Method: R. B. Cattell's Contributions to Structural Equation Modeling},
	url = {https://doi.org/10.1080/00273171.1984.9676927},
	volume = {19},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1080/00273171.1984.9676927}}

@article{Bentler1980,
	author = {Bentler, P M},
	doi = {https://doi.org/10.1146/annurev.ps.31.020180.002223},
	issn = {1545-2085},
	journal = {Annual Review of Psychology},
	number = {Volume 31, 1980},
	pages = {419-456},
	publisher = {Annual Reviews},
	title = {Multivariate Analysis with Latent Variables: Causal Modeling},
	type = {Journal Article},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev.ps.31.020180.002223},
	volume = {31},
	year = {1980},
	bdsk-url-1 = {https://www.annualreviews.org/content/journals/10.1146/annurev.ps.31.020180.002223},
	bdsk-url-2 = {https://doi.org/10.1146/annurev.ps.31.020180.002223}}
@article{白倉幸男1984,
	author          = {白倉幸男},
	year            = {1984},
	title           = {多重指標線形構造モデルとその応用 : 研究ノート },
	journal         = {大阪大学人間科学部紀要},
	volume          = {10},
	number          = {},
	pages           = {25-45},
	url             = {https://doi.org/10.18910/4301}
}
@techreport{Ghahramani-Hinton1996,
	author      = {Zoubin Ghahramani and Geoffrey E. Hinton},
	institution = {Department of Computer Science, University of Toronto},
	title       = {The EM Algorithm for Mixtures of Factor Analyzers},
	year        = {1996},
	url             = {https://www.cs.toronto.edu/~hinton/absps/tr96-1.html},
}

@article{Bernaards-Jennrich2003,
	abstract = {A loading matrix has perfect simple structure if each row has at most one nonzero element. It is shown that if there is an orthogonal rotation of an initial loading matrix that has perfect simple structure, then orthomax rotation with 0 ≤γ≤1 of the initial loading matrix will produce the perfect simple structure. In particular, varimax and quartimax will produce rotations with perfect simple structure whenever they exist.},
	author = {Bernaards, Coen A. and Jennrich, Robert I.},
	date = {2003/12/01},
	date-added = {2024-08-13 15:52:15 +0900},
	date-modified = {2024-08-13 15:52:15 +0900},
	doi = {10.1007/BF02295613},
	id = {Bernaards2003},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {585--588},
	title = {Orthomax rotation and perfect simple structure},
	url = {https://doi.org/10.1007/BF02295613},
	volume = {68},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1007/BF02295613}}
@article{Zou+2006,
 ISSN = {10618600},
 URL = {http://www.jstor.org/stable/27594179},
 abstract = {Principal component analysis (PCA) is widely used in data processing and dimensionality reduction. However, PCA suffers from the fact that each principal component is a linear combination of all the original variables, thus it is often difficult to interpret the results. We introduce a new method called sparse principal component analysis (SPCA) using the lasso (elastic net) to produce modified principal components with sparse loadings. We first show that PCA can be formulated as a regression-type optimization problem; sparse loadings are then obtained by imposing the lasso (elastic net) constraint on the regression coefficients. Efficient algorithms are proposed to fit our SPCA models for both regular multivariate data and gene expression arrays. We also give a new formula to compute the total variance of modified principal components. As illustrations, SPCA is applied to real and simulated data with encouraging results.},
 author = {Hui Zou and Trevor Hastie and Robert Tibshirani},
 journal = {Journal of Computational and Graphical Statistics},
 number = {2},
 pages = {265--286},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Sparse Principal Component Analysis},
 urldate = {2024-08-13},
 volume = {15},
 year = {2006}
}

@article{Jolliffe+2003,
	author = {Ian T Jolliffe, Nickolay T Trendafilov and Mudassir Uddin},
	doi = {10.1198/1061860032148},
	eprint = {https://doi.org/10.1198/1061860032148},
	journal = {Journal of Computational and Graphical Statistics},
	number = {3},
	pages = {531--547},
	publisher = {Taylor \& Francis},
	title = {A Modified Principal Component Technique Based on the LASSO},
	url = {https://doi.org/10.1198/1061860032148},
	volume = {12},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1198/1061860032148}}

@inproceedings{Bishop1998,
	author = {Bishop, Christopher},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {M. Kearns and S. Solla and D. Cohn},
	publisher = {MIT Press},
	title = {Bayesian PCA},
	url = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf},
	volume = {11},
	year = {1998},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/1998/file/c88d8d0a6097754525e02c2246d8d27f-Paper.pdf}}
@article{Rattray+2009,
doi = {10.1088/1742-6596/197/1/012002},
url = {https://dx.doi.org/10.1088/1742-6596/197/1/012002},
year = {2009},
month = {dec},
publisher = {},
volume = {197},
number = {1},
pages = {012002},
author = {Magnus Rattray and  Oliver Stegle and  Kevin Sharp and  John Winn},
title = {Inference algorithms and learning theory for Bayesian sparse factor analysis},
journal = {Journal of Physics: Conference Series},
abstract = {Bayesian sparse factor analysis has many applications; for example, it has been applied to the problem of inferring a sparse regulatory network from gene expression data. We describe a number of inference algorithms for Bayesian sparse factor analysis using a slab and spike mixture prior. These include well-established Markov chain Monte Carlo (MCMC) and variational Bayes (VB) algorithms as well as a novel hybrid of VB and Expectation Propagation (EP). For the case of a single latent factor we derive a theory for learning performance using the replica method. We compare the MCMC and VB/EP algorithm results with simulated data to the theoretical prediction. The results for MCMC agree closely with the theory as expected. Results for VB/EP are slightly sub-optimal but show that the new algorithm is effective for sparse inference. In large-scale problems MCMC is infeasible due to computational limitations and the VB/EP algorithm then provides a very useful computationally efficient alternative.}
}

@inproceedings{Archambeau-Bach2008,
	author = {Archambeau, C\'{e}dric and Bach, Francis},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
	publisher = {Curran Associates, Inc.},
	title = {Sparse probabilistic projections},
	url = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf},
	volume = {21},
	year = {2008},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2008/file/d93ed5b6db83be78efb0d05ae420158e-Paper.pdf}}

@inproceedings{Collins+2001,
	author = {Collins, Michael and Dasgupta, S. and Schapire, Robert E},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {T. Dietterich and S. Becker and Z. Ghahramani},
	publisher = {MIT Press},
	title = {A Generalization of Principal Components Analysis to the Exponential Family},
	url = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f410588e48dc83f2822a880a68f78923-Paper.pdf},
	volume = {14},
	year = {2001},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2001/file/f410588e48dc83f2822a880a68f78923-Paper.pdf}}

@article{Joreskog-Lawley1968,
	abstract = {Until recently the main difficulty in the use of maximum-likelihood estimation in factor analysis has been the lack of satisfactory methods of obtaining numerical solutions. This defect has now been remedied, and this paper describes new rapid methods of finding maximum-likelihood estimates.},
	author = {J{\"o}reskog, K. G. and Lawley, D. N.},
	doi = {https://doi.org/10.1111/j.2044-8317.1968.tb00399.x},
	eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.1968.tb00399.x},
	journal = {British Journal of Mathematical and Statistical Psychology},
	number = {1},
	pages = {85-96},
	title = {New methods in maximum likelihood factor analysis},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1968.tb00399.x},
	volume = {21},
	year = {1968},
	bdsk-url-1 = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.1968.tb00399.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2044-8317.1968.tb00399.x}}

@article{Joreskog-vanThillo1972,
	author = {J{\H o}reskog, Karl G. and van Thiilo, Marielle},
	doi = {https://doi.org/10.1002/j.2333-8504.1972.tb00827.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.2333-8504.1972.tb00827.x},
	journal = {ETS Research Bulletin Series},
	number = {2},
	pages = {i-71},
	title = {{LISREL: A General Computer Program for Estimating a Linear Structural Equation System Involving Multiple Indicators of Unmeasured Variables}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1972.tb00827.x},
	volume = {1972},
	year = {1972},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1972.tb00827.x},
	bdsk-url-2 = {https://doi.org/10.1002/j.2333-8504.1972.tb00827.x}}
@inproceedings{Yu+2006,
author = {Yu, Shipeng and Yu, Kai and Tresp, Volker and Kriegel, Hans-Peter and Wu, Mingrui},
title = {Supervised probabilistic principal component analysis},
year = {2006},
isbn = {1595933395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150402.1150454},
doi = {10.1145/1150402.1150454},
abstract = {Principal component analysis (PCA) has been extensively applied in data mining, pattern recognition and information retrieval for unsupervised dimensionality reduction. When labels of data are available, e.g., in a classification or regression task, PCA is however not able to use this information. The problem is more interesting if only part of the input data are labeled, i.e., in a semi-supervised setting. In this paper we propose a supervised PCA model called SPPCA and a semi-supervised PCA model called S2PPCA, both of which are extensions of a probabilistic PCA model. The proposed models are able to incorporate the label information into the projection phase, and can naturally handle multiple outputs (i.e., in multi-task learning problems). We derive an efficient EM learning algorithm for both models, and also provide theoretical justifications of the model behaviors. SPPCA and S2PPCA are compared with other supervised projection methods on various learning tasks, and show not only promising performance but also good scalability.},
booktitle = {Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {464–473},
numpages = {10},
keywords = {supervised projection, semi-supervised projection, principal component analysis, dimensionality reduction},
location = {Philadelphia, PA, USA},
series = {KDD '06}
}

@article{Gustafsson2001,
	annote = {doi: 10.1021/ci0003909},
	author = {Gustafsson, Mats G. },
	date = {2001/03/01},
	date-added = {2024-08-13 16:43:51 +0900},
	date-modified = {2024-08-13 16:43:51 +0900},
	doi = {10.1021/ci0003909},
	isbn = {0095-2338},
	journal = {Journal of Chemical Information and Computer Sciences},
	journal1 = {Journal of Chemical Information and Computer Sciences},
	journal2 = {J. Chem. Inf. Comput. Sci.},
	month = {03},
	number = {2},
	pages = {288--294},
	publisher = {American Chemical Society},
	title = {A Probabilistic Derivation of the Partial Least-Squares Algorithm},
	type = {doi: 10.1021/ci0003909},
	url = {https://doi.org/10.1021/ci0003909},
	volume = {41},
	year = {2001},
	year1 = {2001},
	bdsk-url-1 = {https://doi.org/10.1021/ci0003909}}
@techreport{Bach-Jordan2005,
	author      = {Francis R. Bach and Michael I. Jordan},
	institution = {University of California, Berkeley},
	title       = {A Probabilistic Interpretation of Canonical Correlation Analysis},
	year        = {2005},
	url             = {https://statistics.berkeley.edu/tech-reports/688},
}

@article{Nounou+2002,
	abstract = {Abstract Large quantities of measured data are being routinely collected in various industries and used for extracting linear models for tasks such as process control, fault diagnosis, and process monitoring. Existing linear modeling methods, however, do not fully utilize all the information contained in the measurements. A new approach for linear process modeling makes maximum use of available process data and process knowledge. Bayesian latent-variable regression (BLVR) permits extraction and incorporation of knowledge about the statistical behavior of measurements in developing linear process models. Furthermore, BLVR can handle noise in inputs and outputs, collinear variables, and incorporate prior knowledge about regression parameters and measured variables. The model is usually more accurate than those of existing methods, including OLS, PCR, and PLS. BLVR considers a univariate output and assumes the underlying variables and noise to be Gaussian, but it can be used for multivariate outputs and other distributions. An empirical Bayes approach is developed to extract the prior information from historical data or maximum-likelihood solution of available data. Examples of steady-state, dynamic and inferential modeling demonstrate the superior accuracy of BLVR over existing methods even when the assumptions of Gaussian distributions are violated. The relationship between BLVR and existing methods and opportunities for future work based on this framework are also discussed.},
	author = {Nounou, Mohamed N. and Bakshi, Bhavik R. and Goel, Prem K. and Shen, Xiaotong},
	doi = {https://doi.org/10.1002/aic.690480818},
	eprint = {https://aiche.onlinelibrary.wiley.com/doi/pdf/10.1002/aic.690480818},
	journal = {AIChE Journal},
	number = {8},
	pages = {1775-1793},
	title = {Process modeling by Bayesian latent variable regression},
	url = {https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690480818},
	volume = {48},
	year = {2002},
	bdsk-url-1 = {https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690480818},
	bdsk-url-2 = {https://doi.org/10.1002/aic.690480818}}
@article{Horst1961,
author = {Horst, Paul},
title = {Generalized canonical correlations and their applications to experimental data},
journal = {Journal of Clinical Psychology},
volume = {17},
number = {4},
pages = {331-347},
doi = {https://doi.org/10.1002/1097-4679(196110)17:4<331::AID-JCLP2270170402>3.0.CO;2-D},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1097-4679%28196110%2917%3A4%3C331%3A%3AAID-JCLP2270170402%3E3.0.CO%3B2-D},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-4679%28196110%2917%3A4%3C331%3A%3AAID-JCLP2270170402%3E3.0.CO%3B2-D},
year = {1961}
}

@inproceedings{Klami+2010,
author = {Klami, Arto and Virtanen, Seppo and Kaski, Samuel},
title = {Bayesian exponential family projections for coupled data sources},
year = {2010},
isbn = {9780974903965},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {Exponential family extensions of principal component analysis (EPCA) have received a considerable amount of attention in recent years, demonstrating the growing need for basic modeling tools that do not assume the squared loss or Gaussian distribution. We extend the EPCA model toolbox by presenting the first exponential family multi-view learning methods of the partial least squares and canonical correlation analysis, based on a unified representation of EPCA as matrix factorization of the natural parameters of exponential family. The models are based on a new family of priors that are generally usable for all such factorizations. We also introduce new inference strategies, and demonstrate how the methods outperform earlier ones when the Gaussianity assumption does not hold.},
booktitle = {Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence},
pages = {286–293},
numpages = {8},
location = {Catalina Island, CA},
series = {UAI'10}
}
@misc{Wang+2017,
title={Deep Variational Canonical Correlation Analysis},
author={Weiran Wang and Xinchen Yan and Honglak Lee and Karen Livescu},
year={2017},
url={https://openreview.net/forum?id=H1Heentlx}
}
@misc{Suzuki+2017,
title={Joint Multimodal Learning with Deep Generative Models},
author={Masahiro Suzuki and Kotaro Nakayama and Yutaka Matsuo},
year={2017},
url={https://openreview.net/forum?id=Hk8rlUqge}
}
@inproceedings{Sun+2009,
author = {Sun, Liang and Ji, Shuiwang and Yu, Shipeng and Ye, Jieping},
title = {On the equivalence between canonical correlation analysis and orthonormalized partial least squares},
year = {2009},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Canonical correlation analysis (CCA) and partial least squares (PLS) are well-known techniques for feature extraction from two sets of multi-dimensional variables. The fundamental difference between CCA and PLS is that CCA maximizes the correlation while PLS maximizes the covariance. Although both CCA and PLS have been applied successfully in various applications, the intrinsic relationship between them remains unclear. In this paper, we attempt to address this issue by showing the equivalence relationship between CCA and orthonormalized partial least squares (OPLS), a variant of PLS. We further extend the equivalence relationship to the case when regularization is employed for both sets of variables. In addition, we show that the CCA projection for one set of variables is independent of the regularization on the other set of variables. We have performed experimental studies using both synthetic and real data sets and our results confirm the established equivalence relationship. The presented analysis provides novel insights into the connection between these two existing algorithms as well as the effect of the regularization.},
booktitle = {Proceedings of the 21st International Joint Conference on Artificial Intelligence},
pages = {1230–1235},
numpages = {6},
location = {Pasadena, California, USA},
series = {IJCAI'09}
}

@InProceedings{Hoffman2017,
  title = 	 {Learning Deep Latent {G}aussian Models with {M}arkov Chain {M}onte {C}arlo},
  author =       {Matthew D. Hoffman},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1510--1519},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/hoffman17a/hoffman17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/hoffman17a.html},
  abstract = 	 {Deep latent Gaussian models are powerful and popular probabilistic models of high-dimensional data. These models are almost always fit using variational expectation-maximization, an approximation to true maximum-marginal-likelihood estimation. In this paper, we propose a different approach: rather than use a variational approximation (which produces biased gradient signals), we use Markov chain Monte Carlo (MCMC, which allows us to trade bias for computation). We find that our MCMC-based approach has several advantages: it yields higher held-out likelihoods, produces sharper images, and does not suffer from the variational overpruning effect. MCMC’s additional computational overhead proves to be significant, but not prohibitive.}
}
@inproceedings{Canny2004,
author = {Canny, John},
title = {GaP: a factor model for discrete data},
year = {2004},
isbn = {1581138814},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1008992.1009016},
doi = {10.1145/1008992.1009016},
abstract = {We present a probabilistic model for a document corpus that combines many of the desirable features of previous models. The model is called "GaP" for Gamma-Poisson, the distributions of the first and last random variable. GaP is a factor model, that is it gives an approximate factorization of the document-term matrix into a product of matrices Λ and X. These factors have strictly non-negative terms. GaP is a generative probabilistic model that assigns finite probabilities to documents in a corpus. It can be computed with an efficient and simple EM recurrence. For a suitable choice of parameters, the GaP factorization maximizes independence between the factors. So it can be used as an independent-component algorithm adapted to document data. The form of the GaP model is empirically as well as analytically motivated. It gives very accurate results as a probabilistic model (measured via perplexity) and as a retrieval model. The GaP model projects documents and terms into a low-dimensional space of "themes," and models texts as "passages" of terms on the same theme.},
booktitle = {Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {122–129},
numpages = {8},
keywords = {probabilistic models, latent semantic analysis, EM algorithm},
location = {Sheffield, United Kingdom},
series = {SIGIR '04}
}

@article{Paatero-Tapper1994,
	abstract = {Abstract A new variant `PMF' of factor analysis is described. It is assumed that X is a matrix of observed data and σ is the known matrix of standard deviations of elements of X. Both X and σ are of dimensions n × m. The method solves the bilinear matrix problem X = GF + E where G is the unknown left hand factor matrix (scores) of dimensions n × p, F is the unknown right hand factor matrix (loadings) of dimensions p × m, and E is the matrix of residuals. The problem is solved in the weighted least squares sense: G and F are determined so that the Frobenius norm of E divided (element-by-element) by σ is minimized. Furthermore, the solution is constrained so that all the elements of G and F are required to be non-negative. It is shown that the solutions by PMF are usually different from any solutions produced by the customary factor analysis (FA, i.e. principal component analysis (PCA) followed by rotations). Usually PMF produces a better fit to the data than FA. Also, the result of PF is guaranteed to be non-negative, while the result of FA often cannot be rotated so that all negative entries would be eliminated. Different possible application areas of the new method are briefly discussed. In environmental data, the error estimates of data can be widely varying and non-negativity is often an essential feature of the underlying models. Thus it is concluded that PMF is better suited than FA or PCA in many environmental applications. Examples of successful applications of PMF are shown in companion papers.},
	author = {Paatero, Pentti and Tapper, Unto},
	doi = {https://doi.org/10.1002/env.3170050203},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.3170050203},
	journal = {Environmetrics},
	keywords = {Factor analysis, Principal component analysis, Weighted least squares, Alternating regression, Error estimates, Scaling, Repetitive measurements},
	number = {2},
	pages = {111-126},
	title = {Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203},
	volume = {5},
	year = {1994},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203},
	bdsk-url-2 = {https://doi.org/10.1002/env.3170050203}}

@inproceedings{Buntine-Jakulin2006,
	abstract = {This article presents a unified theory for analysis of components in discrete data, and compares the methods with techniques such as independent component analysis, non-negative matrix factorisation and latent Dirichlet allocation. The main families of algorithms discussed are a variational approximation, Gibbs sampling, and Rao-Blackwellised Gibbs sampling. Applications are presented for voting records from the United States Senate for 2003, and for the Reuters-21578 newswire collection.},
	address = {Berlin, Heidelberg},
	author = {Buntine, Wray and Jakulin, Aleks},
	booktitle = {Subspace, Latent Structure and Feature Selection},
	editor = {Saunders, Craig and Grobelnik, Marko and Gunn, Steve and Shawe-Taylor, John},
	isbn = {978-3-540-34138-3},
	pages = {1--33},
	publisher = {Springer Berlin Heidelberg},
	title = {Discrete Component Analysis},
	year = {2006}}

@article{Bhattacharya-Dunson2012,
	author = {Anirban Bhattacharya and David B. Dunson},
	doi = {10.1080/01621459.2011.646934},
	eprint = {https://doi.org/10.1080/01621459.2011.646934},
	journal = {Journal of the American Statistical Association},
	note = {PMID: 23908561},
	number = {497},
	pages = {362--377},
	publisher = {Taylor \& Francis},
	title = {Simplex Factor Models for Multivariate Unordered Categorical Data},
	url = {https://doi.org/10.1080/01621459.2011.646934},
	volume = {107},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2011.646934}}

@article{Erosheva+2004,
	abstract = {PNAS is one of world's most cited multidisciplinary scientific journals. The PNAS official classification structure of subjects is reflected in topic labels submitted by the authors of articles, largely related to traditionally established disciplines. These include broad field classifications into physical sciences, biological sciences, social sciences, and further subtopic classifications within the fields. Focusing on biological sciences, we explore an internal soft-classification structure of articles based only on semantic decompositions of abstracts and bibliographies and compare it with the formal discipline classifications. Our model assumes that there is a fixed number of internal categories, each characterized by multinomial distributions over words (in abstracts) and references (in bibliographies). Soft classification for each article is based on proportions of the article's content coming from each category. We discuss the appropriateness of the model for the PNAS database as well as other features of the data relevant to soft classification.},
	author = {Elena Erosheva and Stephen Fienberg and John Lafferty},
	doi = {10.1073/pnas.0307760101},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0307760101},
	journal = {Proceedings of the National Academy of Sciences},
	number = {suppl\_1},
	pages = {5220-5227},
	title = {Mixed-membership models of scientific publications},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0307760101},
	volume = {101},
	year = {2004},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0307760101},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0307760101}}
@article{Pritchard+2000,
    author = {Pritchard, Jonathan K and Stephens, Matthew and Donnelly, Peter},
    title = "{Inference of Population Structure Using Multilocus Genotype Data}",
    journal = {Genetics},
    volume = {155},
    number = {2},
    pages = {945-959},
    year = {2000},
    month = {06},
    abstract = "{We describe a model-based clustering method for using multilocus genotype data to infer population structure and assign individuals to populations. We assume a model in which there are K populations (where K may be unknown), each of which is characterized by a set of allele frequencies at each locus. Individuals in the sample are assigned (probabilistically) to populations, or jointly to two or more populations if their genotypes indicate that they are admixed. Our model does not assume a particular mutation process, and it can be applied to most of the commonly used genetic markers, provided that they are not closely linked. Applications of our method include demonstrating the presence of population structure, assigning individuals to populations, studying hybrid zones, and identifying migrants and admixed individuals. We show that the method can produce highly accurate assignments using modest numbers of loci—e.g., seven microsatellite loci in an example using genotype data from an endangered bird species. The software used for this article is available from http://www.stats.ox.ac.uk/~pritch/home.html.}",
    issn = {1943-2631},
    doi = {10.1093/genetics/155.2.945},
    url = {https://doi.org/10.1093/genetics/155.2.945},
    eprint = {https://academic.oup.com/genetics/article-pdf/155/2/945/42030266/genetics0945.pdf},
}


@inproceedings{Marlin2003,
	author = {Marlin, Benjamin M},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
	publisher = {MIT Press},
	title = {Modeling User Rating Profiles For Collaborative Filtering},
	url = {https://proceedings.neurips.cc/paper_files/paper/2003/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf},
	volume = {16},
	year = {2003},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2003/file/269d837afada308dd4aeab28ca2d57e4-Paper.pdf}}


@article{Hyvarinen-Oja2000,
	abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of non-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
	author = {A. Hyv{\"a}rinen and E. Oja},
	doi = {https://doi.org/10.1016/S0893-6080(00)00026-5},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Independent component analysis, Projection pursuit, Blind signal separation, Source separation, Factor analysis, Representation},
	number = {4},
	pages = {411-430},
	title = {Independent component analysis: algorithms and applications},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608000000265},
	volume = {13},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608000000265},
	bdsk-url-2 = {https://doi.org/10.1016/S0893-6080(00)00026-5}}
@ARTICLE{Friedman-Tukey1974,
  author={Friedman, J.H. and Tukey, J.W.},
  journal={IEEE Transactions on Computers}, 
  title={A Projection Pursuit Algorithm for Exploratory Data Analysis}, 
  year={1974},
  volume={C-23},
  number={9},
  pages={881-890},
  keywords={Clustering, dimensionality reduction, mappings, multidimensional scaling, multivariate data analysis, nonparametric pattern recognition, statistics.},
  doi={10.1109/T-C.1974.224051}}

@inproceedings{Ricahrdson-Weiss2018,
	author = {Richardson, Eitan and Weiss, Yair},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {On GANs and GMMs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2018/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf},
	volume = {31},
	year = {2018},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2018/file/0172d289da48c48de8c5ebf3de9f7ee1-Paper.pdf}}
@inproceedings{Zong+2018,
title={Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection},
author={Bo Zong and Qi Song and Martin Renqiang Min and Wei Cheng and Cristian Lumezanu and Daeki Cho and Haifeng Chen},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJJLHbb0-},
}
@inproceedings{Paisley-Carin2009,
author = {Paisley, John and Carin, Lawrence},
title = {Nonparametric factor analysis with beta process priors},
year = {2009},
isbn = {9781605585161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553474},
doi = {10.1145/1553374.1553474},
abstract = {We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {777–784},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}
@misc{Chen+2024,
      title={DiJiang: Efficient Large Language Models through Compact Kernelization}, 
      author={Hanting Chen and Zhicheng Liu and Xutao Wang and Yuchuan Tian and Yunhe Wang},
      year={2024},
      eprint={2403.19928},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2403.19928}, 
}
@INPROCEEDINGS{Zoran-Weiss2011,
  author={Zoran, Daniel and Weiss, Yair},
  booktitle={2011 International Conference on Computer Vision}, 
  title={From learning models of natural image patches to whole image restoration}, 
  year={2011},
  volume={},
  number={},
  pages={479-486},
  keywords={Image restoration;Noise reduction;Equations;Noise measurement;Image reconstruction;Mathematical model;Estimation},
  doi={10.1109/ICCV.2011.6126278}}
@ARTICLE{Papyam-Elad2016,
  author={Papyan, Vardan and Elad, Michael},
  journal={IEEE Transactions on Image Processing}, 
  title={Multi-Scale Patch-Based Image Restoration}, 
  year={2016},
  volume={25},
  number={1},
  pages={249-261},
  keywords={Noise reduction;Image restoration;Mathematical model;Closed-form solutions;Approximation methods;Context awareness;Image restoration;expected patch log likelihood (EPLL);Gaussian mixture model;multi-scale;denoising;deblurring;super-resolution;Image restoration;expected patch log likelihood (EPLL);Gaussian mixture model;multi-scale;denoising;deblurring;super-resolution},
  doi={10.1109/TIP.2015.2499698}}
@article{Deerwester+1990,
author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
title = {Indexing by latent semantic analysis},
journal = {Journal of the American Society for Information Science},
volume = {41},
number = {6},
pages = {391-407},
doi = {https://doi.org/10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
url = {https://asistdl.onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
eprint = {https://asistdl.onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
abstract = {Abstract A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising. © 1990 John Wiley \& Sons, Inc.},
year = {1990}
}

@inproceedings{Hofmann1999,
author = {Hofmann, Thomas},
title = {Probabilistic latent semantic indexing},
year = {1999},
isbn = {1581130961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/312624.312649},
doi = {10.1145/312624.312649},
booktitle = {Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {50–57},
numpages = {8},
location = {Berkeley, California, USA},
series = {SIGIR '99}
}
@article{Blei+2003,
	author          = {David M. Blei and Andrew Y. Ng and Michael I. Jordan},
	year            = {2003},
	title           = {{Latent Dirichlet Allocation}},
	journal         = {Journal of Machine Learning Research},
	volume          = {3},
	number          = {},
	pages           = {993-1022},
	url             = {https://www.jmlr.org/papers/v3/blei03a.html}
}
@article{Blei2012,
author = {Blei, David M.},
title = {Probabilistic topic models},
year = {2012},
issue_date = {April 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {4},
issn = {0001-0782},
url = {https://doi.org/10.1145/2133806.2133826},
doi = {10.1145/2133806.2133826},
abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
journal = {Commun. ACM},
month = {apr},
pages = {77–84},
numpages = {8}
}

@article{Church-Gale1991,
	abstract = {This paper describes a new program, CORRECT, which takes words rejected by the Unix{\textregistered}SPELL program, proposes a list of candidate corrections, and sorts them by probability score. The probability scores are the novel contribution of this work. They are based on a noisy channel model. It is assumed that the typist knows what words he or she wants to type but some noise is added on the way to the keyboard (in the form of typos and spelling errors). Using a classic Bayesian argument of the kind that is popular in recognition applications, especially speech recognition (Jelinek, 1985), one can often recover the intended correction,c, from a typo,t, by finding the correctionc that maximizesPr(c) Pr(t/c). The first factor,Pr(c), is a prior model of word probabilities; the second factor,Pr(t/c), is a model of the noisy channel that accounts for spelling transformations on letter sequences (insertions, deletions, substitutions and reversals). Both sets of probabilities were estimated using data collected from the Associated Press (AP) newswire over 1988 and 1989 as a training set. The AP generates about 1 million words and 500 typos per week.},
	author = {Church, Kenneth W. and Gale, William A.},
	date = {1991/12/01},
	date-added = {2024-08-13 22:12:23 +0900},
	date-modified = {2024-08-13 22:12:23 +0900},
	doi = {10.1007/BF01889984},
	id = {Church1991},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {2},
	pages = {93--103},
	title = {Probability scoring for spelling correction},
	url = {https://doi.org/10.1007/BF01889984},
	volume = {1},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1007/BF01889984}}

@InProceedings{Arova+2013,
  title = 	 {A Practical Algorithm for Topic Modeling with Provable Guarantees},
  author = 	 {Arora, Sanjeev and Ge, Rong and Halpern, Yonatan and Mimno, David and Moitra, Ankur and Sontag, David and Wu, Yichen and Zhu, Michael},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {280--288},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/arora13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/arora13.html},
  abstract = 	 {Topic models provide a useful method for dimensionality reduction and exploratory data analysis in large text corpora. Most approaches to topic model learning have been based on a maximum likelihood objective. Efficient algorithms exist that attempt to approximate this objective, but they have no provable guarantees. Recently, algorithms have been introduced that provide provable bounds, but these algorithms are not practical because they are inefficient and not robust to violations of model assumptions. In this paper we present an algorithm for learning topic models that is both provable and practical. The algorithm produces results comparable to the best MCMC implementations while running orders of magnitude faster.}
}

@article{Griffith-Steyvers2004,
	abstract = {A first step in identifying the content of a document is determining which topics that document addresses. We describe a generative model for documents, introduced by Blei, Ng, and Jordan [Blei, D. M., Ng, A. Y. \&amp; Jordan, M. I. (2003) J. Machine Learn. Res. 3, 993-1022], in which each document is generated by choosing a distribution over topics and then choosing each word in the document from a topic selected according to this distribution. We then present a Markov chain Monte Carlo algorithm for inference in this model. We use this algorithm to analyze abstracts from PNAS by using Bayesian model selection to establish the number of topics. We show that the extracted topics capture meaningful structure in the data, consistent with the class designations provided by the authors of the articles, and outline further applications of this analysis, including identifying ``hot topics'' by examining temporal dynamics and tagging abstracts to illustrate semantic content.},
	author = {Thomas L. Griffiths and Mark Steyvers},
	doi = {10.1073/pnas.0307752101},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0307752101},
	journal = {Proceedings of the National Academy of Sciences},
	number = {suppl\_1},
	pages = {5228-5235},
	title = {Finding scientific topics},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.0307752101},
	volume = {101},
	year = {2004},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.0307752101},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.0307752101}}
@inproceedings{Blei+2006,
author = {Blei, David M. and Lafferty, John D.},
title = {Dynamic topic models},
year = {2006},
isbn = {1595933832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1143844.1143859},
doi = {10.1145/1143844.1143859},
abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
booktitle = {Proceedings of the 23rd International Conference on Machine Learning},
pages = {113–120},
numpages = {8},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICML '06}
}

@inproceedings{Griffiths+2004,
	author = {Griffiths, Thomas and Steyvers, Mark and Blei, David and Tenenbaum, Joshua},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {L. Saul and Y. Weiss and L. Bottou},
	publisher = {MIT Press},
	title = {Integrating Topics and Syntax},
	url = {https://proceedings.neurips.cc/paper_files/paper/2004/file/ef0917ea498b1665ad6c701057155abe-Paper.pdf},
	volume = {17},
	year = {2004},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2004/file/ef0917ea498b1665ad6c701057155abe-Paper.pdf}}
@inproceedings{Dieng+2017,
title={Topic{RNN}: A Recurrent Neural Network with Long-Range Semantic Dependency},
author={Adji B. Dieng and Chong Wang and Jianfeng Gao and John Paisley},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=rJbbOLcex}
}
@article{Zou-Hastie2005,
    author = {Zou, Hui and Hastie, Trevor},
    title = "{Regularization and Variable Selection Via the Elastic Net}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {67},
    number = {2},
    pages = {301-320},
    year = {2005},
    month = {03},
    abstract = "{We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.}",
    issn = {1369-7412},
    doi = {10.1111/j.1467-9868.2005.00503.x},
    url = {https://doi.org/10.1111/j.1467-9868.2005.00503.x},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/67/2/301/49795094/jrsssb\_67\_2\_301.pdf},
}
@book{豊田秀樹2009,
    author         = {豊田秀樹},
    title          = {共分散構造分析［実践編］},
    year           = {2009},
    publisher      = {朝倉書店},
    series         = {統計ライブラリー},
    volume         = {},
    month          = {10},
    edition        = {},
    howpublished   = {}
}
@book{豊田秀樹2007,
    author         = {豊田秀樹},
    title          = {共分散構造分析［理論編］},
    year           = {2007},
    publisher      = {朝倉書店},
    series         = {統計ライブラリー},
    volume         = {},
    month          = {10},
    edition        = {},
    howpublished   = {}
}

@book{Herbert-Simon57-ModelsOfMan,
    author         = {Herbert Simon},
    title          = {Models of man; social and rational.},
    year           = {1957},
    publisher      = {Wiley},
    series         = {},
    volume         = {},
    month          = {},
    edition        = {},
    howpublished   = {}
}

@article{岩瀬-中山2016,
	author          = {岩瀬智亮 and 中山英樹},
	year            = {2016},
	title           = {深層一般化正準相関分析},
	journal         = {情報処理学会第78回全国大会講演論文集},
	volume          = {2016},
	number          = {1},
	pages           = {183-184},
	url             = {http://id.nii.ac.jp/1001/00162588/}
}

@InProceedings{Andrew+2013,
  title = 	 {Deep Canonical Correlation Analysis},
  author = 	 {Andrew, Galen and Arora, Raman and Bilmes, Jeff and Livescu, Karen},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1247--1255},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/andrew13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/andrew13.html},
  abstract = 	 {We introduce Deep Canonical Correlation Analysis (DCCA), a method to learn complex nonlinear transformations of two views of data such that the resulting representations are highly linearly correlated. Parameters of both transformations are jointly learned to maximize the (regularized) total correlation.   It can be viewed as a nonlinear extension of the linear method \emphcanonical correlation analysis (CCA).  It is an alternative to the nonparametric method \emphkernel canonical correlation analysis (KCCA) for learning correlated nonlinear transformations. Unlike KCCA, DCCA does not require an inner product, and has the advantages of a parametric method: training time scales well with data size and the training data need not be referenced when computing the representations of unseen instances.  In experiments on two real-world datasets, we find that DCCA learns representations with significantly higher correlation than those learned by CCA and KCCA. We also introduce a novel non-saturating sigmoid function based on the cube root that may be useful more generally in feedforward neural networks.}
}

@article{赤穂昭太郎2013,
	author = {赤穂昭太郎},
	doi = {10.3902/jnns.20.62},
	journal = {日本神経回路学会誌},
	number = {2},
	pages = {62-72},
	title = {正準相関分析入門},
	volume = {20},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.3902/jnns.20.62}}

@article{Sei-Yano2024,
	author = {Tomonari Sei and Keisuke Yano},
	doi = {10.3150/23-BEJ1687},
	journal = {Bernoulli},
	keywords = {conditional inference, copula, earthquake data, Graphical model, mixed-domain, Monte Carlo method},
	number = {4},
	pages = {2623 -- 2643},
	publisher = {Bernoulli Society for Mathematical Statistics and Probability},
	title = {{Minimum information dependence modeling}},
	url = {https://doi.org/10.3150/23-BEJ1687},
	volume = {30},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.3150/23-BEJ1687}}

@article{清智也2021,
	author = {清智也},
	doi = {10.11329/jjssj.51.75},
	journal = {日本統計学会誌},
	number = {1},
	pages = {75-99},
	title = {最小情報コピュラとその周辺},
	volume = {51},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.11329/jjssj.51.75}}

@article{Bedford+2016,
	abstract = {Many applications of risk analysis require us to jointly model multiple uncertain quantities. Bayesian networks and copulas are two common approaches to modeling joint uncertainties with probability distributions. This article focuses on new methodologies for copulas by developing work of Cooke, Bedford, Kurowica, and others on vines as a way of constructing higher dimensional distributions that do not suffer from some of the restrictions of alternatives such as the multivariate Gaussian copula. The article provides a fundamental approximation result, demonstrating that we can approximate any density as closely as we like using vines. It further operationalizes this result by showing how minimum information copulas can be used to provide parametric classes of copulas that have such good levels of approximation. We extend previous approaches using vines by considering nonconstant conditional dependencies, which are particularly relevant in financial risk modeling. We discuss how such models may be quantified, in terms of expert judgment or by fitting data, and illustrate the approach by modeling two financial data sets.},
	author = {Bedford, Tim and Daneshkhah, Alireza and Wilson, Kevin J.},
	doi = {https://doi.org/10.1111/risa.12471},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/risa.12471},
	journal = {Risk Analysis},
	keywords = {Copula, entropy, information, risk modeling, vine},
	number = {4},
	pages = {792-815},
	title = {Approximate Uncertainty Modeling in Risk Analysis with Vine Copulas},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.12471},
	volume = {36},
	year = {2016},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.12471},
	bdsk-url-2 = {https://doi.org/10.1111/risa.12471}}
@article{江口真透1999,
	author          = {江口真透},
	year            = {1999},
	title           = {概パラメトリック推測 － 柔らかなモデルの構築 －},
	journal         = {統計数理},
	volume          = {47},
	number          = {1},
	pages           = {29-48},
	url             = {http://hdl.handle.net/10787/295}
}

@article{飽戸弘1966,
	author = {飽戸弘},
	doi = {10.4992/jjpsy.37.204},
	journal = {心理学研究},
	number = {4},
	pages = {204-218},
	title = {政治的態度の構造に関する研究 I},
	volume = {37},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.4992/jjpsy.37.204}}

@article{Young-Hausholder1938,
	abstract = {Necessary and sufficient conditions are given for a set of numbers to be the mutual distances of a set of real points in Euclidean space, and matrices are found whose ranks determine the dimension of the smallest Euclidean space containing such points. Methods are indicated for determining the configuration of these points, and for approximating to them by points in a space of lower dimensionality.},
	author = {Young, Gale and Householder, A. S.},
	date = {1938/03/01},
	date-added = {2024-08-14 13:45:15 +0900},
	date-modified = {2024-08-14 13:45:15 +0900},
	doi = {10.1007/BF02287916},
	id = {Young1938},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {1},
	pages = {19--22},
	title = {Discussion of a set of points in terms of their mutual distances},
	url = {https://doi.org/10.1007/BF02287916},
	volume = {3},
	year = {1938},
	bdsk-url-1 = {https://doi.org/10.1007/BF02287916}}

@article{Unkel-Trendafilov2010,
	abstract = {Summary The classical exploratory factor analysis (EFA) finds estimates for the factor loadings matrix and the matrix of unique factor variances which give the best fit to the sample correlation matrix with respect to some goodness-of-fit criterion. Common factor scores can be obtained as a function of these estimates and the data. Alternatively to the classical EFA, the EFA model can be fitted directly to the data which yields factor loadings and common factor scores simultaneously. Recently, new algorithms were introduced for the simultaneous least squares estimation of all EFA model unknowns. The new methods are based on the numerical procedure for singular value decomposition of matrices and work equally well when the number of variables exceeds the number of observations. This paper provides an account that is intended as an expository review of methods for simultaneous parameter estimation in EFA. The methods are illustrated on Harman's five socio-economic variables data and a high-dimensional data set from genome research.},
	author = {Unkel, Steffen and Trendafilov, Nickolay T.},
	doi = {https://doi.org/10.1111/j.1751-5823.2010.00120.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1751-5823.2010.00120.x},
	journal = {International Statistical Review},
	keywords = {Factor analysis, indeterminacies, least squares estimation, matrix fitting problems, constrained optimization, principal component analysis, rotation},
	number = {3},
	pages = {363-382},
	title = {Simultaneous Parameter Estimation in Exploratory Factor Analysis: An Expository Review},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2010.00120.x},
	volume = {78},
	year = {2010},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2010.00120.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.1751-5823.2010.00120.x}}
@misc{Ghojogh+2022,
      title={Factor Analysis, Probabilistic Principal Component Analysis, Variational Inference, and Variational Autoencoder: Tutorial and Survey}, 
      author={Benyamin Ghojogh and Ali Ghodsi and Fakhri Karray and Mark Crowley},
      year={2022},
      eprint={2101.00734},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2101.00734}, 
}

@article{Kohonen1982,
	abstract = {This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails.},
	author = {Kohonen, Teuvo},
	date = {1982/01/01},
	date-added = {2024-08-14 14:28:55 +0900},
	date-modified = {2024-08-14 14:28:55 +0900},
	doi = {10.1007/BF00337288},
	id = {Kohonen1982},
	isbn = {1432-0770},
	journal = {Biological Cybernetics},
	number = {1},
	pages = {59--69},
	title = {Self-organized formation of topologically correct feature maps},
	url = {https://doi.org/10.1007/BF00337288},
	volume = {43},
	year = {1982},
	bdsk-url-1 = {https://doi.org/10.1007/BF00337288}}

@article{Kohonen2013,
	abstract = {The self-organizing map (SOM) is an automatic data-analysis method. It is widely applied to clustering problems and data exploration in industry, finance, natural sciences, and linguistics. The most extensive applications, exemplified in this paper, can be found in the management of massive textual databases and in bioinformatics. The SOM is related to the classical vector quantization (VQ), which is used extensively in digital signal processing and transmission. Like in VQ, the SOM represents a distribution of input data items using a finite set of models. In the SOM, however, these models are automatically associated with the nodes of a regular (usually two-dimensional) grid in an orderly fashion such that more similar models become automatically associated with nodes that are adjacent in the grid, whereas less similar models are situated farther away from each other in the grid. This organization, a kind of similarity diagram of the models, makes it possible to obtain an insight into the topographic relationships of data, especially of high-dimensional data items. If the data items belong to certain predetermined classes, the models (and the nodes) can be calibrated according to these classes. An unknown input item is then classified according to that node, the model of which is most similar with it in some metric used in the construction of the SOM. A new finding introduced in this paper is that an input item can even more accurately be represented by a linear mixture of a few best-matching models. This becomes possible by a least-squares fitting procedure where the coefficients in the linear mixture of models are constrained to nonnegative values.},
	author = {Teuvo Kohonen},
	doi = {https://doi.org/10.1016/j.neunet.2012.09.018},
	issn = {0893-6080},
	journal = {Neural Networks},
	keywords = {Self-organizing map, SOM, Data analysis, Brain map, Similarity, Vector quantization},
	note = {Twenty-fifth Anniversay Commemorative Issue},
	pages = {52-65},
	title = {Essentials of the self-organizing map},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608012002596},
	volume = {37},
	year = {2013},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0893608012002596},
	bdsk-url-2 = {https://doi.org/10.1016/j.neunet.2012.09.018}}
@article{Baum+1970,
 ISSN = {00034851, 21688990},
 URL = {http://www.jstor.org/stable/2239727},
 author = {Leonard E. Baum and Ted Petrie and George Soules and Norman Weiss},
 journal = {The Annals of Mathematical Statistics},
 number = {1},
 pages = {164--171},
 publisher = {Institute of Mathematical Statistics},
 title = {A Maximization Technique Occurring in the Statistical Analysis of Probabilistic Functions of Markov Chains},
 urldate = {2024-08-14},
 volume = {41},
 year = {1970}
}
@unpublished{Murphy-Linderman2022,
	author = {Kevin Murphy and Scott Linderman},
	year   = {2022},
	title  = {State Space Models: A Modern Approach},
	url    = {https://github.com/probml/ssm-book}
}

@article{Hsu+2012,
	abstract = {Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations---it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.},
	author = {Daniel Hsu and Sham M. Kakade and Tong Zhang},
	doi = {https://doi.org/10.1016/j.jcss.2011.12.025},
	issn = {0022-0000},
	journal = {Journal of Computer and System Sciences},
	keywords = {Hidden Markov Models, Latent variable models, Observable operator models, Time series, Spectral algorithm, Singular value decomposition, Learning probability distributions, Unsupervised learning},
	note = {JCSS Special Issue: Cloud Computing 2011},
	number = {5},
	pages = {1460-1480},
	title = {A spectral algorithm for learning Hidden Markov Models},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000012000244},
	volume = {78},
	year = {2012},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022000012000244},
	bdsk-url-2 = {https://doi.org/10.1016/j.jcss.2011.12.025}}

@InProceedings{Anandkumar+2012,
  title = 	 {A Method of Moments for Mixture Models and Hidden Markov Models},
  author = 	 {Anandkumar, Animashree and Hsu, Daniel and Kakade, Sham M.},
  booktitle = 	 {Proceedings of the 25th Annual Conference on Learning Theory},
  pages = 	 {33.1--33.34},
  year = 	 {2012},
  editor = 	 {Mannor, Shie and Srebro, Nathan and Williamson, Robert C.},
  volume = 	 {23},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Edinburgh, Scotland},
  month = 	 {25--27 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v23/anandkumar12/anandkumar12.pdf},
  url = 	 {https://proceedings.mlr.press/v23/anandkumar12.html},
  abstract = 	 {Mixture models are a fundamental tool in applied statistics and machine learning for treating data taken from multiple subpopulations. The current practice for estimating the parameters of such models relies on local search heuristics (\emphe.g., the EM algorithm) which are prone to failure, and existing consistent methods are unfavorable due to their high computational and sample complexity which typically scale exponentially with the number of mixture components. This work develops an efficient \emphmethod of moments approach to parameter estimation for a broad class of high-dimensional mixture models with many components, including multi-view mixtures of Gaussians (such as mixtures of axis-aligned Gaussians) and hidden Markov models. The new method leads to rigorous unsupervised learning results for mixture models that were not achieved by previous works; and, because of its simplicity, it offers a viable alternative to EM for practical deployment.}
}

@inproceedings{Anandkumar+2015,
	abstract = {This note is a short version of that in [1]. It is intended as a survey for the 2015 Algorithmic Learning Theory (ALT) conference.},
	address = {Cham},
	author = {Anandkumar, Anima and Ge, Rong and Hsu, Daniel and Kakade, Sham M. and Telgarsky, Matus},
	booktitle = {Algorithmic Learning Theory},
	editor = {Chaudhuri, Kamalika and GENTILE, CLAUDIO and Zilles, Sandra},
	isbn = {978-3-319-24486-0},
	pages = {19--38},
	publisher = {Springer International Publishing},
	title = {Tensor Decompositions for Learning Latent Variable Models (A Survey for ALT)},
	year = {2015}}

@InProceedings{Obermeyer+2019,
  title = 	 {Tensor Variable Elimination for Plated Factor Graphs},
  author =       {Obermeyer, Fritz and Bingham, Eli and Jankowiak, Martin and Pradhan, Neeraj and Chiu, Justin and Rush, Alexander and Goodman, Noah},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4871--4880},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/obermeyer19a/obermeyer19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/obermeyer19a.html},
  abstract = 	 {A wide class of machine learning algorithms can be reduced to variable elimination on factor graphs. While factor graphs provide a unifying notation for these algorithms, they do not provide a compact way to express repeated structure when compared to plate diagrams for directed graphical models. To exploit efficient tensor algebra in graphs with plates of variables, we generalize undirected factor graphs to plated factor graphs and variable elimination to a tensor variable elimination algorithm that operates directly on plated factor graphs. Moreover, we generalize complexity bounds based on treewidth and characterize the class of plated factor graphs for which inference is tractable. As an application, we integrate tensor variable elimination into the Pyro probabilistic programming language to enable exact inference in discrete latent variable models with repeated structure. We validate our methods with experiments on both directed and undirected graphical models, including applications to polyphonic music modeling, animal movement modeling, and latent sentiment analysis.}
}

@article{Scott2002,
	author = {Steven L Scott},
	doi = {10.1198/016214502753479464},
	eprint = {https://doi.org/10.1198/016214502753479464},
	journal = {Journal of the American Statistical Association},
	number = {457},
	pages = {337--351},
	publisher = {Taylor \& Francis},
	title = {Bayesian Methods for Hidden Markov Models},
	url = {https://doi.org/10.1198/016214502753479464},
	volume = {97},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1198/016214502753479464}}

@inproceedings{Gu+2020,
	author = {Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
	pages = {1474--1487},
	publisher = {Curran Associates, Inc.},
	title = {HiPPO: Recurrent Memory with Optimal Polynomial Projections},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf},
	volume = {33},
	year = {2020},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2020/file/102f0bb6efb3a6128a3c750dd16729be-Paper.pdf}}

@InProceedings{Goel+2022,
  title = 	 {It’s Raw! {A}udio Generation with State-Space Models},
  author =       {Goel, Karan and Gu, Albert and Donahue, Chris and Re, Christopher},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {7616--7633},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/goel22a/goel22a.pdf},
  url = 	 {https://proceedings.mlr.press/v162/goel22a.html},
  abstract = 	 {Developing architectures suitable for modeling raw audio is a challenging problem due to the high sampling rates of audio waveforms. Standard sequence modeling approaches like RNNs and CNNs have previously been tailored to fit the demands of audio, but the resultant architectures make undesirable computational tradeoffs and struggle to model waveforms effectively. We propose SaShiMi, a new multi-scale architecture for waveform modeling built around the recently introduced S4 model for long sequence modeling. We identify that S4 can be unstable during autoregressive generation, and provide a simple improvement to its parameterization by drawing connections to Hurwitz matrices. SaShiMi yields state-of-the-art performance for unconditional waveform generation in the autoregressive setting. Additionally, SaShiMi improves non-autoregressive generation performance when used as the backbone architecture for a diffusion model. Compared to prior architectures in the autoregressive generation setting, SaShiMi generates piano and speech waveforms which humans find more musical and coherent respectively, e.g. 2{\texttimes} better mean opinion scores than WaveNet on an unconditional speech generation task. On a music generation task, SaShiMi outperforms WaveNet on density estimation and speed at both training and inference even when using 3{\texttimes} fewer parameters}
}
@unpublished{Oudot2016,
	author = {Steve Oudot},
	year   = {2016},
	title  = {Reeb Graph and Mapper},
	url    = {https://geometrica.saclay.inria.fr/team/Steve.Oudot/courses/EMA/Slides_Reeb_Mapper.pdf}
}
@unpublished{Schnider2024,
	author = {Patrick Schnider},
	year   = {2024},
	title  = {Introduction to Topological Data Analysi},
	url    = {https://ti.inf.ethz.ch/ew/courses/TDA24/index.html}
}
@inproceedings{Tang+2016,
author = {Tang, Jian and Liu, Jingzhou and Zhang, Ming and Mei, Qiaozhu},
title = {Visualizing Large-scale and High-dimensional Data},
year = {2016},
isbn = {9781450341431},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/2872427.2883041},
doi = {10.1145/2872427.2883041},
abstract = {We study the problem of visualizing large-scale and high-dimensional data in a low-dimensional (typically 2D or 3D) space. Much success has been reported recently by techniques that first compute a similarity structure of the data points and then project them into a low-dimensional space with the structure preserved. These two steps suffer from considerable computational costs, preventing the state-of-the-art methods such as the t-SNE from scaling to large-scale and high-dimensional data (e.g., millions of data points and hundreds of dimensions). We propose the LargeVis, a technique that first constructs an accurately approximated K-nearest neighbor graph from the data and then layouts the graph in the low-dimensional space. Comparing to t-SNE, LargeVis significantly reduces the computational cost of the graph construction step and employs a principled probabilistic model for the visualization step, the objective of which can be effectively optimized through asynchronous stochastic gradient descent with a linear time complexity. The whole procedure thus easily scales to millions of high-dimensional data points. Experimental results on real-world data sets demonstrate that the LargeVis outperforms the state-of-the-art methods in both efficiency and effectiveness. The hyper-parameters of LargeVis are also much more stable over different data sets.},
booktitle = {Proceedings of the 25th International Conference on World Wide Web},
pages = {287–297},
numpages = {11},
keywords = {visualization, high-dimensional data, big data},
location = {Montr\'{e}al, Qu\'{e}bec, Canada},
series = {WWW '16}
}
@inproceedings{DasGupta-Freud2008,
author = {Dasgupta, Sanjoy and Freund, Yoav},
title = {Random projection trees and low dimensional manifolds},
year = {2008},
isbn = {9781605580470},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1374376.1374452},
doi = {10.1145/1374376.1374452},
abstract = {We present a simple variant of the k-d tree which automatically adapts to intrinsic low dimensional structure in data without having to explicitly learn this structure.},
booktitle = {Proceedings of the Fortieth Annual ACM Symposium on Theory of Computing},
pages = {537–546},
numpages = {10},
keywords = {curse of dimension, k-d tree, manifold, random projection},
location = {Victoria, British Columbia, Canada},
series = {STOC '08}
}

@article{Saul2020,
	abstract = {Latent variable models (LVMs) are powerful tools for discovering hidden structure in data. Canonical LVMs include factor analysis, which explains the correlation of a large number of observed variables in terms of a smaller number of unobserved ones, and Gaussian mixture models, which reveal clusters of data arising from an underlying multimodal distribution. In this paper, we describe a conceptually simple and equally effective LVM for nonlinear dimensionality reduction (NLDR), where the goal is to discover faithful, neighborhood-preserving embeddings of high-dimensional data. Tools for NLDR can help researchers across all areas of science and engineering to better understand and visualize their data. Our approach elevates NLDR into the family of problems that can be studied by especially tractable LVMs. We propose a latent variable model to discover faithful low-dimensional representations of high-dimensional data. The model computes a low-dimensional embedding that aims to preserve neighborhood relationships encoded by a sparse graph. The model both leverages and extends current leading approaches to this problem. Like t-distributed Stochastic Neighborhood Embedding, the model can produce two- and three-dimensional embeddings for visualization, but it can also learn higher-dimensional embeddings for other uses. Like LargeVis and Uniform Manifold Approximation and Projection, the model produces embeddings by balancing two goals---pulling nearby examples closer together and pushing distant examples further apart. Unlike these approaches, however, the latent variables in our model provide additional structure that can be exploited for learning. We derive an Expectation--Maximization procedure with closed-form updates that monotonically improve the model's likelihood: In this procedure, embeddings are iteratively adapted by solving sparse, diagonally dominant systems of linear equations that arise from a discrete graph Laplacian. For large problems, we also develop an approximate coarse-graining procedure that avoids the need for negative sampling of nonadjacent nodes in the graph. We demonstrate the model's effectiveness on datasets of images and text.},
	author = {Lawrence K. Saul},
	doi = {10.1073/pnas.1916012117},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1916012117},
	journal = {Proceedings of the National Academy of Sciences},
	number = {27},
	pages = {15403-15408},
	title = {A tractable latent variable model for nonlinear dimensionality reduction},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1916012117},
	volume = {117},
	year = {2020},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1916012117},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1916012117}}

@article{Tutte1963,
	author = {Tutte, W. T.},
	doi = {https://doi.org/10.1112/plms/s3-13.1.743},
	eprint = {https://londmathsoc.onlinelibrary.wiley.com/doi/pdf/10.1112/plms/s3-13.1.743},
	journal = {Proceedings of the London Mathematical Society},
	number = {1},
	pages = {743-767},
	title = {{How to Draw a Graph}},
	url = {https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/plms/s3-13.1.743},
	volume = {s3-13},
	year = {1963},
	bdsk-url-1 = {https://londmathsoc.onlinelibrary.wiley.com/doi/abs/10.1112/plms/s3-13.1.743},
	bdsk-url-2 = {https://doi.org/10.1112/plms/s3-13.1.743}}
@article{Eades1984,
	author          = {Peter Eades},
	year            = {1984},
	title           = {{A Heuristic for Graph Drawing}},
	journal         = {Congress Numerantium},
	volume          = {42},
	number          = {11},
	pages           = {149-160},
	url             = {https://www.cs.ubc.ca/~will/536E/papers/Eades1984.pdf}
}

@article{Kamada-Kawai1989,
	author = {Tomihisa Kamada and Satoru Kawai},
	doi = {https://doi.org/10.1016/0020-0190(89)90102-6},
	issn = {0020-0190},
	journal = {Information Processing Letters},
	keywords = {Graph, network structure, layout, drawing algorithm},
	number = {1},
	pages = {7-15},
	title = {An algorithm for drawing general undirected graphs},
	url = {https://www.sciencedirect.com/science/article/pii/0020019089901026},
	volume = {31},
	year = {1989},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0020019089901026},
	bdsk-url-2 = {https://doi.org/10.1016/0020-0190(89)90102-6}}
@ARTICLE{Fisk+1967,
  author={Fisk, C.J. and Caskey, D.L. and West, L.E.},
  journal={Proceedings of the IEEE}, 
  title={ACCEL: Automated circuit card etching layout}, 
  year={1967},
  volume={55},
  number={11},
  pages={1971-1982},
  keywords={Etching;Printed circuits;Assembly;Insulation;Joining processes;Libraries;Production systems;Drilling;Connectors;Contacts},
  doi={10.1109/PROC.1967.6027}}
@ARTICLE{Quinn-Breuer1979,
  author={Quinn, N. and Breuer, M.},
  journal={IEEE Transactions on Circuits and Systems}, 
  title={A forced directed component placement procedure for printed circuit boards}, 
  year={1979},
  volume={26},
  number={6},
  pages={377-388},
  keywords={Printed circuits;Equations;Wire;Personal communication networks;DH-HEMTs;Environmental management;Ceramics;Routing;Length measurement},
  doi={10.1109/TCS.1979.1084652}}

@article{Lee-Seung1999,
	abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	author = {Lee, Daniel D. and Seung, H. Sebastian},
	date = {1999/10/01},
	date-added = {2024-08-14 16:57:13 +0900},
	date-modified = {2024-08-14 16:57:13 +0900},
	doi = {10.1038/44565},
	id = {Lee1999},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6755},
	pages = {788--791},
	title = {Learning the parts of objects by non-negative matrix factorization},
	url = {https://doi.org/10.1038/44565},
	volume = {401},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1038/44565}}
@inbook{Howard-Matheson1981,
	author         = {R. A. Howard and J. E. Matheson},
	chapter        = {Influence Diagrams},
	editor         = {R. A. Howard and J. E. Matheson},
	pages          = {},
	publisher      = {Strategic Decision Group},
	title          = {Readings on the Principles and Applications of Decision Analysis},
	year           = {1981}
}
@INPROCEEDINGS{Gori+2005,
  author={Gori, M. and Monfardini, G. and Scarselli, F.},
  booktitle={Proceedings. 2005 IEEE International Joint Conference on Neural Networks, 2005.}, 
  title={A new model for learning in graph domains}, 
  year={2005},
  volume={2},
  number={},
  pages={729-734 vol. 2},
  keywords={Neural networks;Focusing;Application software;Machine learning;Recurrent neural networks;Encoding;Data structures;Machine learning algorithms;Tree graphs;Software engineering},
  doi={10.1109/IJCNN.2005.1555942}}
@ARTICLE{Scarselli+2009,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks}, 
  title={The Graph Neural Network Model}, 
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  keywords={Neural networks;Biological system modeling;Data engineering;Computer vision;Chemistry;Biology;Pattern recognition;Data mining;Supervised learning;Parameter estimation;Graphical domains;graph neural networks (GNNs);graph processing;recursive neural networks},
  doi={10.1109/TNN.2008.2005605}}

@InProceedings{Gilmer+2017,
  title = 	 {Neural Message Passing for Quantum Chemistry},
  author =       {Justin Gilmer and Samuel S. Schoenholz and Patrick F. Riley and Oriol Vinyals and George E. Dahl},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1263--1272},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/gilmer17a/gilmer17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/gilmer17a.html},
  abstract = 	 {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.}
}
@article{Battahlia+2018,title	= {Relational inductive biases, deep learning, and graph networks},author	= {Peter Battaglia and Jessica Blake Chandler Hamrick and Victor Bapst and Alvaro Sanchez and Vinicius Zambaldi and Mateusz Malinowski and Andrea Tacchetti and David Raposo and Adam Santoro and Ryan Faulkner and Caglar Gulcehre and Francis Song and Andy Ballard and Justin Gilmer and George E. Dahl and Ashish Vaswani and Kelsey Allen and Charles Nash and Victoria Jayne Langston and Chris Dyer and Nicolas Heess and Daan Wierstra and Pushmeet Kohli and Matt Botvinick and Oriol Vinyals and Yujia Li and Razvan Pascanu},year	= {2018},URL	= {https://arxiv.org/pdf/1806.01261.pdf},journal	= {arXiv}}

@inproceedings{Bruna+2014,
	author          = {Joan Bruna and Wojciech Zaremba and Arthur Szlam and Yann LeCun},
	year            = {2014},
	title           = {Spectral Networks and Locally Connected Networks on Graphs},
	booktitle       = {International Conference on Learning Representation},
	volume          = {},
	pages           = {},
	url             = {https://openreview.net/forum?id=DQNsQf-UsoDBa}
}
@inproceedings{Kipf-Welling2017,
title={Semi-Supervised Classification with Graph Convolutional Networks},
author={Thomas N. Kipf and Max Welling},
booktitle={International Conference on Learning Representations},
year={2017},
url={https://openreview.net/forum?id=SJU4ayYgl}
}

@inproceedings{Hamilton+2017,
	author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Inductive Representation Learning on Large Graphs},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf}}
@inproceedings{Velickovic+2018,
title={Graph Attention Networks},
author={Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rJXMpikCZ},
}
@INPROCEEDINGS{Masci+2015,
  author={Masci, Jonathan and Boscaini, Davide and Bronstein, Michael M. and Vandergheynst, Pierre},
  booktitle={2015 IEEE International Conference on Computer Vision Workshop (ICCVW)}, 
  title={Geodesic Convolutional Neural Networks on Riemannian Manifolds}, 
  year={2015},
  volume={},
  number={},
  pages={832-840},
  keywords={Shape;Manifolds;Heating;Eigenvalues and eigenfunctions;Kernel;Neural networks;Geometry},
  doi={10.1109/ICCVW.2015.112}}

@inproceedings{Boscaini+2016,
	author = {Boscaini, Davide and Masci, Jonathan and Rodol\`{a}, Emanuele and Bronstein, Michael},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Learning shape correspondence with anisotropic convolutional neural networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf},
	volume = {29},
	year = {2016},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2016/file/228499b55310264a8ea0e27b6e7c6ab6-Paper.pdf}}
@INPROCEEDINGS{Monti+2017,
  author={Monti, Federico and Boscaini, Davide and Masci, Jonathan and Rodolà, Emanuele and Svoboda, Jan and Bronstein, Michael M.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Geometric Deep Learning on Graphs and Manifolds Using Mixture Model CNNs}, 
  year={2017},
  volume={},
  number={},
  pages={5425-5434},
  keywords={Manifolds;Machine learning;Convolution;Laplace equations;Three-dimensional displays;Shape;Computational modeling},
  doi={10.1109/CVPR.2017.576}}

@inproceedings{Chami+2019,
	author = {Chami, Ines and Ying, Zhitao and R\'{e}, Christopher and Leskovec, Jure},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Hyperbolic Graph Convolutional Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/0415740eaa4d9decbc8da001d3fd805f-Paper.pdf}}

@inproceedings{Liu+2019,
	author = {Liu, Qi and Nickel, Maximilian and Kiela, Douwe},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {Hyperbolic Graph Neural Networks},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/103303dd56a731e377d01f6a37badae3-Paper.pdf},
	volume = {32},
	year = {2019},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2019/file/103303dd56a731e377d01f6a37badae3-Paper.pdf}}
@article{Bell-Sejnowski1995,
    author = {Bell, Anthony J. and Sejnowski, Terrence J.},
    title = "{An Information-Maximization Approach to Blind Separation and Blind Deconvolution}",
    journal = {Neural Computation},
    volume = {7},
    number = {6},
    pages = {1129-1159},
    year = {1995},
    month = {11},
    abstract = "{We derive a new self-organizing learning algorithm that maximizes the information transferred in a network of nonlinear units. The algorithm does not assume any knowledge of the input distributions, and is defined here for the zero-noise limit. Under these conditions, information maximization has extra properties not found in the linear case (Linsker 1989). The nonlinearities in the transfer function are able to pick up higher-order moments of the input distributions and perform something akin to true redundancy reduction between units in the output representation. This enables the network to separate statistically independent components in the inputs: a higher-order generalization of principal components analysis. We apply the network to the source separation (or cocktail party) problem, successfully separating unknown mixtures of up to 10 speakers. We also show that a variant on the network architecture is able to perform blind deconvolution (cancellation of unknown echoes and reverberation in a speech signal). Finally, we derive dependencies of information transfer on time delays. We suggest that information maximization provides a unifying framework for problems in "blind" signal processing.}",
    issn = {0899-7667},
    doi = {10.1162/neco.1995.7.6.1129},
    url = {https://doi.org/10.1162/neco.1995.7.6.1129},
    eprint = {https://direct.mit.edu/neco/article-pdf/7/6/1129/813064/neco.1995.7.6.1129.pdf},
}
@unpublished{Yao2011,
	author = {Yuan Yao},
	year   = {2011},
	title  = {Mathematics of Data -- Laplacian, Diffusion, and Hessian LLE},
	note            = {Lecture 10},
	url    = {https://www.math.pku.edu.cn/teachers/yaoy/Spring2011/}
}

@article{Jordan-Kinderlehrer-Otto1998,
	abstract = { The Fokker--Planck equation, or forward Kolmogorov equation, describes the evolution of the probability density for a stochastic process associated with an Ito stochastic differential equation. It pertains to a wide variety of time-dependent systems in which randomness plays a role. In this paper, we are concerned with Fokker--Planck equations for which the drift term is given by the gradient of a potential. For a broad class of potentials, we construct a time discrete, iterative variational scheme whose solutions converge to the solution of the Fokker--Planck equation. The major novelty of this iterative scheme is that the time-step is governed by the Wasserstein metric on probability measures. This formulation enables us to reveal an appealing, and previously unexplored, relationship between the Fokker--Planck equation and the associated free energy functional. Namely, we demonstrate that the dynamics may be regarded as a gradient flux, or a steepest descent, for the free energy with respect to the Wasserstein metric. },
	author = {Jordan, Richard and Kinderlehrer, David and Otto, Felix},
	doi = {10.1137/S0036141096303359},
	eprint = {https://doi.org/10.1137/S0036141096303359},
	journal = {SIAM Journal on Mathematical Analysis},
	number = {1},
	pages = {1-17},
	title = {The Variational Formulation of the Fokker--Planck Equation},
	url = {https://doi.org/10.1137/S0036141096303359},
	volume = {29},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1137/S0036141096303359}}
@unpublished{Hairer2018,
    author = {Martin Hairer},
    note   = {Lecture Note},
    title  = {Ergodic Properties of Markov Processes},
    year   = {2018},
    url    = {https://www.hairer.org/notes/Markov.pdf}
}

@article{Zhang-Zha2004,
	abstract = { We present a new algorithm for manifold learning and nonlinear dimensionality reduction. Based on a set of unorganized data points sampled with noise from a parameterized manifold, the local geometry of the manifold is learned by constructing an approximation for the tangent space at each data point, and those tangent spaces are then aligned to give the global coordinates of the data points with respect to the underlying manifold. We also present an error analysis of our algorithm showing that reconstruction errors can be quite small in some cases. We illustrate our algorithm using curves and surfaces both in two-dimensional/three-dimensional (2D/3D) Euclidean spaces and in higher-dimensional Euclidean spaces. We also address several theoretical and algorithmic issues for further research and improvements. },
	author = {Zhang, Zhenyue and Zha, Hongyuan},
	doi = {10.1137/S1064827502419154},
	eprint = {https://doi.org/10.1137/S1064827502419154},
	journal = {SIAM Journal on Scientific Computing},
	number = {1},
	pages = {313-338},
	title = {Principal Manifolds and Nonlinear Dimensionality Reduction via Tangent Space Alignment},
	url = {https://doi.org/10.1137/S1064827502419154},
	volume = {26},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1137/S1064827502419154}}

@article{Donoho-Grimes2003,
	author = {David L. Donoho and Carrie Grimes},
	doi = {10.1073/pnas.1031596100},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1031596100},
	journal = {Proceedings of the National Academy of Sciences},
	number = {10},
	pages = {5591-5596},
	title = {Hessian eigenmaps: Locally linear embedding techniques for high-dimensional data},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1031596100},
	volume = {100},
	year = {2003},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1031596100},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1031596100}}

@article{Moon+2019,
	abstract = {The high-dimensional data created by high-throughput technologies require visualization tools that reveal data structure and patterns in an intuitive form. We present PHATE, a visualization method that captures both local and global nonlinear structure using an information-geometric distance between data points. We compare PHATE to other tools on a variety of artificial and biological datasets, and find that it consistently preserves a range of patterns in data, including continual progressions, branches and clusters, better than other tools. We define a manifold preservation metric, which we call denoised embedding manifold preservation (DEMaP), and show that PHATE produces lower-dimensional embeddings that are quantitatively better denoised as compared to existing visualization methods. An analysis of a newly generated single-cell RNA sequencing dataset on human germ-layer differentiation demonstrates how PHATE reveals unique biological insight into the main developmental branches, including identification of three previously undescribed subpopulations. We also show that PHATE is applicable to a wide variety of data types, including mass cytometry, single-cell RNA sequencing, Hi-C and gut microbiome data.},
	author = {Moon, Kevin R. and van Dijk, David and Wang, Zheng and Gigante, Scott and Burkhardt, Daniel B. and Chen, William S. and Yim, Kristina and Elzen, Antonia van den and Hirn, Matthew J. and Coifman, Ronald R. and Ivanova, Natalia B. and Wolf, Guy and Krishnaswamy, Smita},
	date = {2019/12/01},
	date-added = {2024-08-15 14:48:06 +0900},
	date-modified = {2024-08-15 14:48:06 +0900},
	doi = {10.1038/s41587-019-0336-3},
	id = {Moon2019},
	isbn = {1546-1696},
	journal = {Nature Biotechnology},
	number = {12},
	pages = {1482--1492},
	title = {Visualizing structure and transitions in high-dimensional biological data},
	url = {https://doi.org/10.1038/s41587-019-0336-3},
	volume = {37},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41587-019-0336-3}}

@article{Szubert+2019,
	abstract = {Single-cell technologies offer an unprecedented opportunity to effectively characterize cellular heterogeneity in health and disease. Nevertheless, visualisation and interpretation of these multi-dimensional datasets remains a challenge. We present a novel framework, ivis, for dimensionality reduction of single-cell expression data. ivis utilizes a siamese neural network architecture that is trained using a novel triplet loss function. Results on simulated and real datasets demonstrate that ivis preserves global data structures in a low-dimensional space, adds new data points to existing embeddings using a parametric mapping function, and scales linearly to hundreds of thousands of cells. ivis is made publicly available through Python and R interfaces on https://github.com/beringresearch/ivis.},
	author = {Szubert, Benjamin and Cole, Jennifer E. and Monaco, Claudia and Drozdov, Ignat},
	date = {2019/06/20},
	date-added = {2024-08-16 20:22:42 +0900},
	date-modified = {2024-08-16 20:22:42 +0900},
	doi = {10.1038/s41598-019-45301-0},
	id = {Szubert2019},
	isbn = {2045-2322},
	journal = {Scientific Reports},
	number = {1},
	pages = {8914},
	title = {Structure-preserving visualisation of high dimensional single-cell datasets},
	url = {https://doi.org/10.1038/s41598-019-45301-0},
	volume = {9},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41598-019-45301-0}}
@misc{Campbell+2024,
      title={Generative Flows on Discrete State-Spaces: Enabling Multimodal Flows with Applications to Protein Co-Design}, 
      author={Andrew Campbell and Jason Yim and Regina Barzilay and Tom Rainforth and Tommi Jaakkola},
      year={2024},
      eprint={2402.04997},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2402.04997}, 
}
@article{Stimper+2023, doi = {10.21105/joss.05361}, url = {https://doi.org/10.21105/joss.05361}, year = {2023}, publisher = {The Open Journal}, volume = {8}, number = {86}, pages = {5361}, author = {Vincent Stimper and David Liu and Andrew Campbell and Vincent Berenz and Lukas Ryll and Bernhard Schölkopf and José Miguel Hernández-Lobato}, title = {normflows: A PyTorch Package for Normalizing Flows}, journal = {Journal of Open Source Software} } 

@InProceedings{Germain+2015,
  title = 	 {{MADE: Masked Autoencoder for Distribution Estimation}},
  author = 	 {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {881--889},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/germain15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/germain15.html},
  abstract = 	 {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder’s parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.}
}
@book{Pontryagin+1962,
	author         = {L. S. Pontryagin and V. G. Boltyanskii and R. V. Gamkrelidze and E. F. Mishchenko},
	year           = {1962},
	title          = {The Mathematical Theory of Optimal Processes},
	series         = {},
	volume         = {},
	edition        = {},
	url            = {https://doi.org/10.1201/9780203749319},
	doi            = {},
	publisher      = {John Wiley \& Sons}
}

@article{Lettermann+2024,
	abstract = {Building a representative model of a complex dynamical system from empirical evidence remains a highly challenging problem. Classically, these models are described by systems of differential equations that depend on parameters that need to be optimized by comparison with data. In this tutorial, we introduce the most common multi-parameter estimation techniques, highlighting their successes and limitations. We demonstrate how to use the adjoint method, which allows efficient handling of large systems with many unknown parameters, and present prototypical examples across several fields of physics. Our primary objective is to provide a practical introduction to adjoint optimization, catering for a broad audience of scientists and engineers.},
	author = {Lettermann, Leon and Jurado, Alejandro and Betz, Timo and W{\"o}rg{\"o}tter, Florentin and Herzog, Sebastian},
	date = {2024/04/15},
	date-added = {2024-08-20 13:44:11 +0900},
	date-modified = {2024-08-20 13:44:11 +0900},
	doi = {10.1038/s42005-024-01606-9},
	id = {Lettermann2024},
	isbn = {2399-3650},
	journal = {Communications Physics},
	number = {1},
	pages = {128},
	title = {Tutorial: a beginner's guide to building a representative model of dynamical systems using the adjoint method},
	url = {https://doi.org/10.1038/s42005-024-01606-9},
	volume = {7},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s42005-024-01606-9}}
@article{sanchez-lengeling+2021,
  author = {Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B.},
  title = {A Gentle Introduction to Graph Neural Networks},
  journal = {Distill},
  year = {2021},
  note = {https://distill.pub/2021/gnn-intro},
  doi = {10.23915/distill.00033}
}
@misc{Adams+2018,
      title={Estimating the Spectral Density of Large Implicit Matrices}, 
      author={Ryan P. Adams and Jeffrey Pennington and Matthew J. Johnson and Jamie Smith and Yaniv Ovadia and Brian Patton and James Saunderson},
      year={2018},
      eprint={1802.03451},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1802.03451}, 
}
@article{Avron-Toledo2011,
author = {Avron, Haim and Toledo, Sivan},
title = {Randomized algorithms for estimating the trace of an implicit symmetric positive semi-definite matrix},
year = {2011},
issue_date = {April 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0004-5411},
url = {https://doi.org/10.1145/1944345.1944349},
doi = {10.1145/1944345.1944349},
abstract = {We analyze the convergence of randomized trace estimators. Starting at 1989, several algorithms have been proposed for estimating the trace of a matrix by 1/MΣi=1M ziT Azi, where the zi are random vectors; different estimators use different distributions for the zis, all of which lead to E(1/MΣi=1M ziT Azi) = trace(A). These algorithms are useful in applications in which there is no explicit representation of A but rather an efficient method compute zTAz given z. Existing results only analyze the variance of the different estimators. In contrast, we analyze the number of samples M required to guarantee that with probability at least 1-δ, the relative error in the estimate is at most ϵ. We argue that such bounds are much more useful in applications than the variance. We found that these bounds rank the estimators differently than the variance; this suggests that minimum-variance estimators may not be the best.We also make two additional contributions to this area. The first is a specialized bound for projection matrices, whose trace (rank) needs to be computed in electronic structure calculations. The second is a new estimator that uses less randomness than all the existing estimators.},
journal = {J. ACM},
month = {apr},
articleno = {8},
numpages = {34},
keywords = {implicit linear operators, Trace estimation}
}
@inbook{Meyer+2021,
author = {Raphael A. Meyer and Cameron Musco and Christopher Musco and David P. Woodruff},
title = {Hutch++: Optimal Stochastic Trace Estimation},
booktitle = {2021 Symposium on Simplicity in Algorithms (SOSA)},
chapter = {},
pages = {142-155},
doi = {10.1137/1.9781611976496.16},
URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611976496.16},
eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611976496.16},
}
@inproceedings{Zhang-Chen2023,
title={Fast Sampling of Diffusion Models with Exponential Integrator},
author={Qinsheng Zhang and Yongxin Chen},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=Loek7hfb46P}
}
@article{McCann1997,
title = {A Convexity Principle for Interacting Gases},
journal = {Advances in Mathematics},
volume = {128},
number = {1},
pages = {153-179},
year = {1997},
issn = {0001-8708},
doi = {https://doi.org/10.1006/aima.1997.1634},
url = {https://www.sciencedirect.com/science/article/pii/S0001870897916340},
author = {Robert J. McCann},
abstract = {A new set of inequalities is introduced, based on a novel but natural interpolation between Borel probability measures onRd. Using these estimates in lieu of convexity or rearrangement inequalities, the existence and uniqueness problems are solved for a family of attracting gas models. In these models, the gas interacts with itself through a force which increases with distance and is governed by an equation of stateP=P(ϱ) relating pressure to density.P(ϱ)/ϱ>(d−1)/dis assumed non-decreasing for ad-dimensional gas. By showing that the internal and potential energies for the system are convex functions of the interpolation parameter, an energy minimizing state—unique up to translation—is proven to exist. The concavity established for ¶ρt¶−p/dqas a function oft∈[0,1] generalizes the Brunn–Minkowski inequality from sets to measures.}
}
@Article{Maoutsa+2020,
AUTHOR = {Maoutsa, Dimitra and Reich, Sebastian and Opper, Manfred},
TITLE = {Interacting Particle Solutions of Fokker–Planck Equations Through Gradient–Log–Density Estimation},
JOURNAL = {Entropy},
VOLUME = {22},
YEAR = {2020},
NUMBER = {8},
ARTICLE-NUMBER = {802},
URL = {https://www.mdpi.com/1099-4300/22/8/802},
PubMedID = {33286573},
ISSN = {1099-4300},
ABSTRACT = {Fokker–Planck equations are extensively employed in various scientific fields as they characterise the behaviour of stochastic systems at the level of probability density functions. Although broadly used, they allow for analytical treatment only in limited settings, and often it is inevitable to resort to numerical solutions. Here, we develop a computational approach for simulating the time evolution of Fokker–Planck solutions in terms of a mean field limit of an interacting particle system. The interactions between particles are determined by the gradient of the logarithm of the particle density, approximated here by a novel statistical estimator. The performance of our method shows promising results, with more accurate and less fluctuating statistics compared to direct stochastic simulations of comparable particle number. Taken together, our framework allows for effortless and reliable particle-based simulations of Fokker–Planck equations in low and moderate dimensions. The proposed gradient–log–density estimator is also of independent interest, for example, in the context of optimal control.},
DOI = {10.3390/e22080802}
}
@inproceedings{Heitz+2023,
author = {Heitz, Eric and Belcour, Laurent and Chambon, Thomas},
title = {Iterative α -(de)Blending: a&nbsp;Minimalist&nbsp;Deterministic&nbsp;Diffusion&nbsp;Model},
year = {2023},
isbn = {9798400701597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3588432.3591540},
doi = {10.1145/3588432.3591540},
abstract = {We derive a minimalist but powerful deterministic denoising-diffusion model. While denoising diffusion has shown great success in many domains, its underlying theory remains largely inaccessible to non-expert users. Indeed, an understanding of graduate-level concepts such as Langevin dynamics or score matching appears to be required to grasp how it works. We propose an alternative approach that requires no more than undergrad calculus and probability. We consider two densities and observe what happens when random samples from these densities are blended (linearly interpolated). We show that iteratively blending and deblending samples produces random paths between the two densities that converge toward a deterministic mapping. This mapping can be evaluated with a neural network trained to deblend samples. We obtain a model that behaves like deterministic denoising diffusion: it iteratively maps samples from one density (e.g., Gaussian noise) to another (e.g., cat images). However, compared to the state-of-the-art alternative, our model is simpler to derive, simpler to implement, more numerically stable, achieves higher quality results in our experiments, and has interesting connections to computer graphics.},
booktitle = {ACM SIGGRAPH 2023 Conference Proceedings},
articleno = {34},
numpages = {8},
keywords = {diffusion models, mapping, sampling},
location = {Los Angeles, CA, USA},
series = {SIGGRAPH '23}
}

@inproceedings{Porter-Duff1984,
author = {Porter, Thomas and Duff, Tom},
title = {Compositing digital images},
year = {1984},
isbn = {0897911385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800031.808606},
doi = {10.1145/800031.808606},
abstract = {Most computer graphics pictures have been computed all at once, so that the rendering program takes care of all computations relating to the overlap of objects. There are several applications, however, where elements must be rendered separately, relying on compositing techniques for the anti-aliased accumulation of the full image. This paper presents the case for four-channel pictures, demonstrating that a matte component can be computed similarly to the color channels. The paper discusses guidelines for the generation of elements and the arithmetic for their arbitrary compositing.},
booktitle = {Proceedings of the 11th Annual Conference on Computer Graphics and Interactive Techniques},
pages = {253–259},
numpages = {7},
keywords = {Compositing, Graphics systems, Matte algebra, Matte channel, Visible surface algorithms},
series = {SIGGRAPH '84}
}

@misc{Oulhaj+2024,
      title={Differentiable Mapper For Topological Optimization Of Data Representation}, 
      author={Ziyad Oulhaj and Mathieu Carrière and Bertrand Michel},
      year={2024},
      eprint={2402.12854},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.12854}, 
}
@misc{Oulhaj+2024DeepMapper,
      title={Deep Mapper Graph and its Application to Visualize Plausible Pathways on High-Dimensional Distribution with Small Time-Complexity}, 
      author={Ziyad Oulhaj and Yoshiyuki Ishii and Kento Ohga and Kimihiro Yamazaki and Mutsuyo Wada and Yuhei Umeda and Takashi Kato and Yuichiro Wada and Hiroaki Kurihara},
      year={2024},
      eprint={2402.19177},
      archivePrefix={arXiv},
      primaryClass={q-bio.QM},
      url={https://arxiv.org/abs/2402.19177}, 
}
@misc{Esser+2024,
      title={Scaling Rectified Flow Transformers for High-Resolution Image Synthesis}, 
      author={Patrick Esser and Sumith Kulal and Andreas Blattmann and Rahim Entezari and Jonas Müller and Harry Saini and Yam Levi and Dominik Lorenz and Axel Sauer and Frederic Boesel and Dustin Podell and Tim Dockhorn and Zion English and Kyle Lacey and Alex Goodwin and Yannik Marek and Robin Rombach},
      year={2024},
      eprint={2403.03206},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.03206}, 
}
@inproceedings{Kingma-Gao2023,
 author = {Kingma, Diederik P. and Gao, Ruiqi},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {65484--65516},
 publisher = {Curran Associates, Inc.},
 title = {Understanding Diffusion Objectives as the ELBO with Simple Data Augmentation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/ce79fbf9baef726645bc2337abb0ade2-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}
@misc{Heng+2022,
      title={{Simulating Diffusion Bridges with Score Matching}}, 
      author={Jeremy Heng and Valentin De Bortoli and Arnaud Doucet and James Thornton},
      year={2022},
      eprint={2111.07243},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2111.07243}, 
}
@article{Tong+2024,
title={{Improving and Generalizing Flow-Based Generative Models with Minibatch Optimal Transport}},
author={Alexander Tong and Kilian Fatras and Nikolay Malkin and Guillaume Huguet and Yanlei Zhang and Jarrid Rector-Brooks and Guy Wolf and Yoshua Bengio},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://openreview.net/forum?id=CD9Snc73AW},
note={Expert Certification}
}
@article{Schrodinger1932,
author = {Schrödinger, E.},
journal = {Annales de l'institut Henri Poincaré},
keywords = {quantum theory},
language = {fre},
number = {4},
pages = {269-310},
publisher = {INSTITUT HENRI POINCARÉ ET LES PRESSES UNIVERSITAIRES DE FRANCE},
title = {Sur la théorie relativiste de l'électron et l'interprétation de la mécanique quantique},
url = {http://eudml.org/doc/78968},
volume = {2},
year = {1932},
}

@InProceedings{Pooladian+2023,
  title = 	 {Multisample Flow Matching: Straightening Flows with Minibatch Couplings},
  author =       {Pooladian, Aram-Alexandre and Ben-Hamu, Heli and Domingo-Enrich, Carles and Amos, Brandon and Lipman, Yaron and Chen, Ricky T. Q.},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {28100--28127},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/pooladian23a/pooladian23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/pooladian23a.html},
  abstract = 	 {Simulation-free methods for training continuous-time generative models construct probability paths that go between noise distributions and individual data samples. Recent works, such as Flow Matching, derived paths that are optimal for each data sample. However, these algorithms rely on independent data and noise samples, and do not exploit underlying structure in the data distribution for constructing probability paths. We propose Multisample Flow Matching, a more general framework that uses non-trivial couplings between data and noise samples while satisfying the correct marginal constraints. At small overhead costs, this generalization allows us to (i) reduce gradient variance during training, (ii) obtain straighter flows for the learned vector field, which allows us to generate high-quality samples using fewer function evaluations, and (iii) obtain transport maps with low cost in high dimensions, which has applications beyond generative modeling. Importantly, we do so in a completely simulation-free manner with a simple minimization objective. We show that our proposed methods improve sample consistency on downsampled ImageNet data sets, and lead to better low-cost sample generation.}
}
@misc{Isobe+2024,
      title={Extended Flow Matching: a Method of Conditional Generation with Generalized Continuity Equation}, 
      author={Noboru Isobe and Masanori Koyama and Jinzhe Zhang and Kohei Hayashi and Kenji Fukumizu},
      year={2024},
      eprint={2402.18839},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.18839}, 
}
@misc{Fatras+2021,
      title={Minibatch optimal transport distances; analysis and applications}, 
      author={Kilian Fatras and Younes Zine and Szymon Majewski and Rémi Flamary and Rémi Gribonval and Nicolas Courty},
      year={2021},
      eprint={2101.01792},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2101.01792}, 
}
@Inbook{Brenier2003,
author="Brenier, Yann",
title="Extended Monge-Kantorovich Theory",
bookTitle="Optimal Transportation and Applications: Lectures given at the C.I.M.E. Summer School, held in Martina Franca, Italy, September 2-8, 2001",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="91--121",
abstract="1 Abstract2 Generalized geodesics and the Monge-Kantorovich theory2.1 Generalized geodesics2.2 Extension to probability measures2.3 A decomposition result2.4 Relativistic MKT2.5 A relativistic heat equation2.6 Laplace's equation and Moser's lemma revisited3 Generalized Harmonic functions3.1 Classical harmonic functions3.2 Open problems4 Multiphasic MKT5 Generalized extremal surfaces5.1 MKT revisited as a subset of generalized surface theory5.2 Degenerate quadratic cost functions6 Generalized extremal surfaces in {\$}<math display='block'><mrow><msup><mi>{\&}{\#}x211D;</mi><mn>5</mn></msup></mrow></math>{\$}{\$}{\backslash}mathbb{\{}R{\}}^5{\$}and Electrodynamics6.1 Recovery of the Maxwell equations6.2 Derivation of a set of nonlinear Maxwell equations6.3 An Euler-Maxwell-type systemReferences",
isbn="978-3-540-44857-0",
doi="10.1007/978-3-540-44857-0_4",
url="https://doi.org/10.1007/978-3-540-44857-0_4"
}


@InProceedings{Kerrigan+2024,
  title = 	 {Functional Flow Matching},
  author =       {Kerrigan, Gavin and Migliorini, Giosue and Smyth, Padhraic},
  booktitle = 	 {Proceedings of The 27th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {3934--3942},
  year = 	 {2024},
  editor = 	 {Dasgupta, Sanjoy and Mandt, Stephan and Li, Yingzhen},
  volume = 	 {238},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {02--04 May},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v238/kerrigan24a/kerrigan24a.pdf},
  url = 	 {https://proceedings.mlr.press/v238/kerrigan24a.html},
  abstract = 	 {We propose Functional Flow Matching (FFM), a function-space generative model that generalizes the recently-introduced Flow Matching model to operate directly in infinite-dimensional spaces. Our approach works by first defining a path of probability measures that interpolates between a fixed Gaussian measure and the data distribution, followed by learning a vector field on the underlying space of functions that generates this path of measures. Our method does not rely on likelihoods or simulations, making it well-suited to the function space setting. We provide both a theoretical framework for building such models and an empirical evaluation of our techniques. We demonstrate through experiments on synthetic and real-world benchmarks that our proposed FFM method outperforms several recently proposed function-space generative models.}
}
@article{Lavenant2019,
title = {Harmonic mappings valued in the Wasserstein space},
journal = {Journal of Functional Analysis},
volume = {277},
number = {3},
pages = {688-785},
year = {2019},
issn = {0022-1236},
doi = {https://doi.org/10.1016/j.jfa.2019.05.003},
url = {https://www.sciencedirect.com/science/article/pii/S0022123619301478},
author = {Hugo Lavenant},
}
@article{Lavenant+2024,
author = {Hugo Lavenant and Stephen Zhang and Young-Heon Kim and Geoffrey Schiebinger},
title = {{Toward a mathematical theory of trajectory inference}},
volume = {34},
journal = {The Annals of Applied Probability},
number = {1A},
publisher = {Institute of Mathematical Statistics},
pages = {428 -- 500},
keywords = {Convex optimization, developmental biology, Optimal transport, single-cell RNA-sequencing, Stochastic processes, Trajectory inference},
year = {2024},
doi = {10.1214/23-AAP1969},
URL = {https://doi.org/10.1214/23-AAP1969}
}

@InProceedings{Hashimoto+2016,
  title = 	 {Learning Population-Level Diffusions with Generative RNNs},
  author = 	 {Hashimoto, Tatsunori and Gifford, David and Jaakkola, Tommi},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {2417--2426},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/hashimoto16.pdf},
  url = 	 {https://proceedings.mlr.press/v48/hashimoto16.html},
  abstract = 	 {We estimate stochastic processes that govern the dynamics of evolving populations such as cell differentiation. The problem is challenging since longitudinal trajectory measurements of individuals in a population are rarely available due to experimental cost and/or privacy. We show that cross-sectional samples from an evolving population suffice for recovery within a class of processes even if samples are available only at a few distinct time points. We provide a stratified analysis of recoverability conditions, and establish that reversibility is sufficient for recoverability. For estimation, we derive a natural loss and regularization, and parameterize the processes as diffusive recurrent neural networks. We demonstrate the approach in the context of uncovering complex cellular dynamics known as the ‘epigenetic landscape’ from existing biological assays.}
}

@misc{Dao+2023,
      title={Flow Matching in Latent Space}, 
      author={Quan Dao and Hao Phung and Binh Nguyen and Anh Tran},
      year={2023},
      eprint={2307.08698},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2307.08698}, 
}
@misc{Zheng+2023GuidedFlow,
      title={Guided Flows for Generative Modeling and Decision Making}, 
      author={Qinqing Zheng and Matt Le and Neta Shaul and Yaron Lipman and Aditya Grover and Ricky T. Q. Chen},
      year={2023},
      eprint={2311.13443},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2311.13443}, 
}
@inproceedings{Chen-Lipman2024,
title={Flow Matching on General Geometries},
author={Ricky T. Q. Chen and Yaron Lipman},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=g7ohDlTITL}
}
@misc{Habermann+2024,
      title={Amortized Bayesian Multilevel Models}, 
      author={Daniel Habermann and Marvin Schmitt and Lars Kühmichel and Andreas Bulling and Stefan T. Radev and Paul-Christian Bürkner},
      year={2024},
      eprint={2408.13230},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2408.13230}, 
}
@ARTICLE{Radev+2022,
  author={Radev, Stefan T. and Mertens, Ulf K. and Voss, Andreas and Ardizzone, Lynton and Köthe, Ullrich},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={BayesFlow: Learning Complex Stochastic Models With Invertible Neural Networks}, 
  year={2022},
  volume={33},
  number={4},
  pages={1452-1466},
  keywords={Data models;Bayes methods;Biological system modeling;Neural networks;Training;Numerical models;Estimation;Bayesian inference;computational and artificial intelligence;machine learning;neural networks;statistical learning},
  doi={10.1109/TNNLS.2020.3042395}}
@article{Cranmer+2020,
author = {Kyle Cranmer  and Johann Brehmer  and Gilles Louppe },
title = {The frontier of simulation-based inference},
journal = {Proceedings of the National Academy of Sciences},
volume = {117},
number = {48},
pages = {30055-30062},
year = {2020},
doi = {10.1073/pnas.1912789117},
URL = {https://www.pnas.org/doi/abs/10.1073/pnas.1912789117},
eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1912789117},
abstract = {Many domains of science have developed complex simulations to describe phenomena of interest. While these simulations provide high-fidelity models, they are poorly suited for inference and lead to challenging inverse problems. We review the rapidly developing field of simulation-based inference and identify the forces giving additional momentum to the field. Finally, we describe how the frontier is expanding so that a broad audience can appreciate the profound influence these developments may have on science.}}
@inproceedings{Koshizuka-Sato2023,
title={Neural Lagrangian Schr{\textbackslash}''\{o\}dinger Bridge: Diffusion Modeling for Population Dynamics},
author={Takeshi Koshizuka and Issei Sato},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=d3QNWD_pcFv}
}
@article{Efron2011,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/23239562},
 abstract = {We suppose that the statistician observes some large number of estimates zi, each with its own unobserved expectation parameter μi. The largest few of the zi's are likely to substantially overestimate their corresponding μi's, this being an example of selection bias, or regression to the mean. Tweedie's formula, first reported by Robbins in 1956, offers a simple empirical Bayes approach for correcting selection bias. This article investigates its merits and limitations. In addition to the methodology, Tweedie's formula raises more general questions concerning empirical Bayes theory, discussed here as "relevance" and "empirical Bayes information." There is a close connection between applications of the formula and James—Stein estimation.},
 author = {Bradley Efron},
 journal = {Journal of the American Statistical Association},
 number = {496},
 pages = {1602--1614},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Tweedie's Formula and Selection Bias},
 urldate = {2024-08-27},
 volume = {106},
 year = {2011}
}
@misc{Fjelde+2024,
  title   = "An Introduction to Flow Matching",
  author  = "Fjelde, Tor and Mathieu, Emile and Dutordoir, Vincent",
  journal = "https://mlg.eng.cam.ac.uk/blog/",
  year    = "2024",
  month   = "January",
  url     = "https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html"
}
@misc{Chen+2023SB,
  title={Schrodinger Bridges Beat Diffusion Models on Text-to-Speech Synthesis}, 
  author={Zehua Chen and Guande He and Kaiwen Zheng and Xu Tan and Jun Zhu},
  year={2023},
  url             = {https://bridge-tts.github.io/},
}
@unpublished{Dieleman2023,
	author = {Sander Dieleman},
	year   = {2023},
	title  = {Perspectives on diffusion},
	url    = {https://sander.ai/2023/07/20/perspectives.html},
	doi    = {}
}
@unpublished{Yuan2024,
	author = {Chenyang Yuan},
	year   = {2024},
	title  = {Diffusion Models from Scratch, from a New Theoretical Perspective},
	url    = {https://www.chenyang.co/diffusion.html},
	doi    = {}
}
@misc{Nakkiran+2024,
      title={Step-by-Step Diffusion: An Elementary Tutorial}, 
      author={Preetum Nakkiran and Arwen Bradley and Hattie Zhou and Madhu Advani},
      year={2024},
      eprint={2406.08929},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.08929}, 
}
@unpublished{Duan2023,
	author = {Tony Duan},
	year   = {2023},
	title  = {Diffusion Models from Scratch},
	url    = {https://www.tonyduan.com/diffusion/index.html},
	doi    = {}
}
@inproceedings{Das2024,
  author = {Das, Ayan},
  title = {Building Diffusion Model's theory from ground up},
  abstract = {Diffusion Models, a new generative model family, have taken the world by storm after the seminal paper by Ho et al. [2020]. While diffusion models are often described as a probabilistic Markov Chains, their underlying principle is based on the decade-old theory of Stochastic Differential Equations (SDE), as found out later by Song et al. [2021]. In this article, we will go back and revisit the 'fundamental ingredients' behind the SDE formulation and show how the idea can be 'shaped' to get to the modern form of Score-based Diffusion Models. We'll start from the very definition of the 'score', how it was used in the context of generative modeling, how we achieve the necessary theoretical guarantees and how the critical design choices were made to finally arrive at the more 'principled' framework of Score-based Diffusion. Throughout this article, we provide several intuitive illustrations for ease of understanding.},
  booktitle = {ICLR Blogposts 2024},
  year = {2024},
  date = {May 7, 2024},
  note = {https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/},
  url  = {https://iclr-blogposts.github.io/2024/blog/diffusion-theory-from-scratch/}
}

@InProceedings{Bunne+2022,
  title = 	 { Proximal Optimal Transport Modeling of Population Dynamics },
  author =       {Bunne, Charlotte and Papaxanthos, Laetitia and Krause, Andreas and Cuturi, Marco},
  booktitle = 	 {Proceedings of The 25th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {6511--6528},
  year = 	 {2022},
  editor = 	 {Camps-Valls, Gustau and Ruiz, Francisco J. R. and Valera, Isabel},
  volume = 	 {151},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {28--30 Mar},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v151/bunne22a/bunne22a.pdf},
  url = 	 {https://proceedings.mlr.press/v151/bunne22a.html},
  abstract = 	 { We propose a new approach to model the collective dynamics of a population of particles evolving with time. As is often the case in challenging scientific applications, notably single-cell genomics, measuring features for these particles requires destroying them. As a result, the population can only be monitored with periodic snapshots, obtained by sampling a few particles that are sacrificed in exchange for measurements. Given only access to these snapshots, can we reconstruct likely individual trajectories for all other particles? We propose to model these trajectories as collective realizations of a causal Jordan-Kinderlehrer-Otto (JKO) flow of measures: The JKO scheme posits that the new configuration taken by a population at time t+1 is one that trades off an improvement, in the sense that it decreases an energy, while remaining close (in Wasserstein distance) to the previous configuration observed at t. In order to learn such an energy using only snapshots, we propose JKOnet, a neural architecture that computes (in end-to-end differentiable fashion) the JKO flow given a parametric energy and initial configuration of points. We demonstrate the good performance and robustness of the JKOnet fitting procedure, compared to a more direct forward method. }
}
@article{Schiebinger+2019,
title = {Optimal-Transport Analysis of Single-Cell Gene Expression Identifies Developmental Trajectories in Reprogramming},
journal = {Cell},
volume = {176},
number = {4},
pages = {928-943.e22},
year = {2019},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S009286741930039X},
author = {Geoffrey Schiebinger and Jian Shu and Marcin Tabaka and Brian Cleary and Vidya Subramanian and Aryeh Solomon and Joshua Gould and Siyan Liu and Stacie Lin and Peter Berube and Lia Lee and Jenny Chen and Justin Brumbaugh and Philippe Rigollet and Konrad Hochedlinger and Rudolf Jaenisch and Aviv Regev and Eric S. Lander},
keywords = {optimal-transport, reprogramming, scRNA-seq, trajectories, ancestors, descendants, development, regulation, paracrine interactions, iPSCs},
abstract = {Summary
Understanding the molecular programs that guide differentiation during development is a major challenge. Here, we introduce Waddington-OT, an approach for studying developmental time courses to infer ancestor-descendant fates and model the regulatory programs that underlie them. We apply the method to reconstruct the landscape of reprogramming from 315,000 single-cell RNA sequencing (scRNA-seq) profiles, collected at half-day intervals across 18 days. The results reveal a wider range of developmental programs than previously characterized. Cells gradually adopt either a terminal stromal state or a mesenchymal-to-epithelial transition state. The latter gives rise to populations related to pluripotent, extra-embryonic, and neural cells, with each harboring multiple finer subpopulations. The analysis predicts transcription factors and paracrine signals that affect fates and experiments validate that the TF Obox6 and the cytokine GDF9 enhance reprogramming efficiency. Our approach sheds light on the process and outcome of reprogramming and provides a framework applicable to diverse temporal processes in biology.}
}

@InProceedings{Tong+2020,
  title = 	 {{T}rajectory{N}et: A Dynamic Optimal Transport Network for Modeling Cellular Dynamics},
  author =       {Tong, Alexander and Huang, Jessie and Wolf, Guy and Van Dijk, David and Krishnaswamy, Smita},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9526--9536},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/tong20a/tong20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/tong20a.html},
  abstract = 	 {It is increasingly common to encounter data in the form of cross-sectional population measurements over time, particularly in biomedical settings. Recent attempts to model individual trajectories from this data use optimal transport to create pairwise matchings between time points. However, these methods cannot model non-linear paths common in many underlying dynamic systems. We establish a link between continuous normalizing flows and dynamic optimal transport to model the expected paths of points over time. Continuous normalizing flows are generally under constrained, as they are allowed to take an arbitrary path from the source to the target distribution. We present \emph{TrajectoryNet}, which controls the continuous paths taken between distributions. We show how this is particularly applicable for studying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq) technologies, and that TrajectoryNet improves upon recently proposed static optimal transport-based models that can be used for interpolating cellular distributions.}
}

@InProceedings{Neklyudov+2023,
  title = 	 {Action Matching: Learning Stochastic Dynamics from Samples},
  author =       {Neklyudov, Kirill and Brekelmans, Rob and Severo, Daniel and Makhzani, Alireza},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {25858--25889},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/neklyudov23a/neklyudov23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/neklyudov23a.html},
  abstract = 	 {Learning the continuous dynamics of a system from snapshots of its temporal marginals is a problem which appears throughout natural sciences and machine learning, including in quantum systems, single-cell biological data, and generative modeling. In these settings, we assume access to cross-sectional samples that are uncorrelated over time, rather than full trajectories of samples. In order to better understand the systems under observation, we would like to learn a model of the underlying process that allows us to propagate samples in time and thereby simulate entire individual trajectories. In this work, we propose Action Matching, a method for learning a rich family of dynamics using only independent samples from its time evolution. We derive a tractable training objective, which does not rely on explicit assumptions about the underlying dynamics and does not require back-propagation through differential equations or optimal transport solvers. Inspired by connections with optimal transport, we derive extensions of Action Matching to learn stochastic differential equations and dynamics involving creation and destruction of probability mass. Finally, we showcase applications of Action Matching by achieving competitive performance in a diverse set of experiments from biology, physics, and generative modeling.}
}

@article{Eldan2013,
	abstract = {We consider the isoperimetric inequality on the class of high-dimensional isotropic convex bodies. We establish quantitative connections between two well-known open problems related to this inequality, namely, the thin shell conjecture, and the conjecture by Kannan, Lov{\'a}sz, and Simonovits, showing that the corresponding optimal bounds are equivalent up to logarithmic factors. In particular we prove that, up to logarithmic factors, the minimal possible ratio between surface area and volume is attained on ellipsoids. We also show that a positive answer to the thin shell conjecture would imply an optimal dependence on the dimension in a certain formulation of the Brunn--Minkowski inequality. Our results rely on the construction of a stochastic localization scheme for log-concave measures.},
	author = {Eldan, Ronen},
	date = {2013/04/01},
	date-added = {2024-08-28 20:57:55 +0900},
	date-modified = {2024-08-28 20:57:55 +0900},
	doi = {10.1007/s00039-013-0214-y},
	id = {Eldan2013},
	isbn = {1420-8970},
	journal = {Geometric and Functional Analysis},
	number = {2},
	pages = {532--569},
	title = {{Thin Shell Implies Spectral Gap Up to Polylog via a Stochastic Localization Scheme}},
	url = {https://doi.org/10.1007/s00039-013-0214-y},
	volume = {23},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s00039-013-0214-y}}
@misc{Montanari2023,
      title={Sampling, Diffusions, and Stochastic Localization}, 
      author={Andrea Montanari},
      year={2023},
      eprint={2305.10690},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2305.10690}, 
}
@INPROCEEDINGS{Alaoui+2022,
  author={Alaoui, Ahmed El and Montanari, Andrea and Sellke, Mark},
  booktitle={2022 IEEE 63rd Annual Symposium on Foundations of Computer Science (FOCS)}, 
  title={Sampling from the Sherrington-Kirkpatrick Gibbs measure via algorithmic stochastic localization}, 
  year={2022},
  volume={},
  number={},
  pages={323-334},
  keywords={Measurement;Location awareness;Couplings;Temperature distribution;Stochastic processes;Approximation algorithms;Sampling methods},
  doi={10.1109/FOCS54457.2022.00038}}
@misc{Benton+2024,
      title={Nearly $d$-Linear Convergence Bounds for Diffusion Models via Stochastic Localization}, 
      author={Joe Benton and Valentin De Bortoli and Arnaud Doucet and George Deligiannidis},
      year={2024},
      eprint={2308.03686},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2308.03686}, 
}
@article{Benton+2024Denoising,
    author = {Benton, Joe and Shi, Yuyang and De Bortoli, Valentin and Deligiannidis, George and Doucet, Arnaud},
    title = "{From denoising diffusions to denoising Markov models}",
    journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
    volume = {86},
    number = {2},
    pages = {286-301},
    year = {2024},
    month = {01},
    abstract = "{Denoising diffusions are state-of-the-art generative models exhibiting remarkable empirical performance. They work by diffusing the data distribution into a Gaussian distribution and then learning to reverse this noising process to obtain synthetic datapoints. The denoising diffusion relies on approximations of the logarithmic derivatives of the noised data densities using score matching. Such models can also be used to perform approximate posterior simulation when one can only sample from the prior and likelihood. We propose a unifying framework generalizing this approach to a wide class of spaces and leading to an original extension of score matching. We illustrate the resulting models on various applications.}",
    issn = {1369-7412},
    doi = {10.1093/jrsssb/qkae005},
    url = {https://doi.org/10.1093/jrsssb/qkae005},
    eprint = {https://academic.oup.com/jrsssb/article-pdf/86/2/286/57219053/qkae005.pdf},
}
@INPROCEEDINGS{Zhu-Mumford1998,
  author={Song Chun Zhu and Mumford, D.},
  booktitle={Sixth International Conference on Computer Vision (IEEE Cat. No.98CH36271)}, 
  title={GRADE: Gibbs reaction and diffusion equations}, 
  year={1998},
  volume={},
  number={},
  pages={847-854},
  keywords={Application software;Partial differential equations;Pattern formation;Nonlinear equations;Computer vision;Image processing;Markov random fields;Minimax techniques;Entropy;Differential equations},
  doi={10.1109/ICCV.1998.710816}}

@article{Zhu+1998,
	abstract = {This article presents a statistical theory for texture modeling. This theory combines filtering theory and Markov random field modeling through the maximum entropy principle, and interprets and clarifies many previous concepts and methods for texture analysis and synthesis from a unified point of view. Our theory characterizes the ensemble of images I with the same texture appearance by a probability distribution f(I) on a random field, and the objective of texture modeling is to make inference about f(I), given a set of observed texture examples.In our theory, texture modeling consists of two steps. (1) A set of filters is selected from a general filter bank to capture features of the texture, these filters are applied to observed texture images, and the histograms of the filtered images are extracted. These histograms are estimates of the marginal distributions of f( I). This step is called feature extraction. (2) The maximum entropy principle is employed to derive a distribution p(I), which is restricted to have the same marginal distributions as those in (1). This p(I) is considered as an estimate of f( I). This step is called feature fusion. A stepwise algorithm is proposed to choose filters from a general filter bank. The resulting model, called FRAME (Filters, Random fields And Maximum Entropy), is a Markov random field (MRF) model, but with a much enriched vocabulary and hence much stronger descriptive ability than the previous MRF models used for texture modeling. Gibbs sampler is adopted to synthesize texture images by drawing typical samples from p(I), thus the model is verified by seeing whether the synthesized texture images have similar visual appearances to the texture images being modeled. Experiments on a variety of 1D and 2D textures are described to illustrate our theory and to show the performance of our algorithms. These experiments demonstrate that many textures which are previously considered as from different categories can be modeled and synthesized in a common framework.},
	author = {Zhu, Song Chun and Wu, Yingnian and Mumford, David},
	date = {1998/03/01},
	date-added = {2024-09-02 13:15:46 +0900},
	date-modified = {2024-09-02 13:15:46 +0900},
	doi = {10.1023/A:1007925832420},
	id = {Zhu1998},
	isbn = {1573-1405},
	journal = {International Journal of Computer Vision},
	number = {2},
	pages = {107--126},
	title = {{Filters, Random Fields and Maximum Entropy (FRAME): Towards a Unified Theory for Texture Modeling}},
	url = {https://doi.org/10.1023/A:1007925832420},
	volume = {27},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1023/A:1007925832420}}
@article{Shibue-Komaki2020,
    doi = {10.1371/journal.pcbi.1007650},
    author = {Shibue, Ryohei AND Komaki, Fumiyasu},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Deconvolution of calcium imaging data using marked point processes},
    year = {2020},
    month = {03},
    volume = {16},
    url = {https://doi.org/10.1371/journal.pcbi.1007650},
    pages = {1-25},
    number = {3},
}

@article{Shibue-Komaki2017,
	abstract = {Neural decoding is a framework for reconstructing external stimuli from spike trains recorded by various neural recordings. Kloosterman et al. proposed a new decoding method using marked point processes (Kloosterman F, Layton SP, Chen Z, Wilson MA. J Neurophysiol 111: 217?227, 2014). This method does not require spike sorting and thereby improves decoding accuracy dramatically. In this method, they used kernel density estimation to estimate intensity functions of marked point processes. However, the use of kernel density estimation causes problems such as low decoding accuracy and high computational costs. To overcome these problems, we propose a new decoding method using infinite mixture models to estimate intensity. The proposed method improves decoding performance in terms of accuracy and computational speed. We apply the proposed method to simulation and experimental data to verify its performance. NEW \& NOTEWORTHY We propose a new neural decoding method using infinite mixture models and nonparametric Bayesian statistics. The proposed method improves decoding performance in terms of accuracy and computation speed. We have successfully applied the proposed method to position decoding from spike trains recorded in a rat hippocampus.},
	annote = {doi: 10.1152/jn.00818.2016},
	author = {Shibue, Ryohei and Komaki, Fumiyasu},
	date = {2017/11/01},
	date-added = {2024-09-02 13:56:48 +0900},
	date-modified = {2024-09-02 13:56:48 +0900},
	doi = {10.1152/jn.00818.2016},
	isbn = {0022-3077},
	journal = {Journal of Neurophysiology},
	journal1 = {Journal of Neurophysiology},
	month = {2024/09/01},
	n2 = {Neural decoding is a framework for reconstructing external stimuli from spike trains recorded by various neural recordings. Kloosterman et al. proposed a new decoding method using marked point processes (Kloosterman F, Layton SP, Chen Z, Wilson MA. J Neurophysiol 111: 217?227, 2014). This method does not require spike sorting and thereby improves decoding accuracy dramatically. In this method, they used kernel density estimation to estimate intensity functions of marked point processes. However, the use of kernel density estimation causes problems such as low decoding accuracy and high computational costs. To overcome these problems, we propose a new decoding method using infinite mixture models to estimate intensity. The proposed method improves decoding performance in terms of accuracy and computational speed. We apply the proposed method to simulation and experimental data to verify its performance. NEW \& NOTEWORTHY We propose a new neural decoding method using infinite mixture models and nonparametric Bayesian statistics. The proposed method improves decoding performance in terms of accuracy and computation speed. We have successfully applied the proposed method to position decoding from spike trains recorded in a rat hippocampus.},
	number = {5},
	pages = {2902--2913},
	publisher = {American Physiological Society},
	title = {Firing rate estimation using infinite mixture models and its application to neural decoding},
	type = {doi: 10.1152/jn.00818.2016},
	url = {https://doi.org/10.1152/jn.00818.2016},
	volume = {118},
	year = {2017},
	year1 = {2017},
	bdsk-url-1 = {https://doi.org/10.1152/jn.00818.2016}}
@book{Goldstine1980,
	author = {Herman H. Goldstine},
	year = {1980},
	title = {A History of the Calculus of Variations from the 17th through the 19th Century},
	series = {Studies in the History of Mathematics and Physical Sciences},
	volume = {5},
	edition = {},
	url = {https://doi.org/10.1007/978-1-4613-8106-8},
	doi = {10.1007/978-1-4613-8106-8},
	publisher = {Springer New York}
}
@article{Hamilton1834,
	author = {W. R. Hamilton},
	year = {1834},
	title = {On a General Method in Dynamics; by which the Study of the Motions of all free Systems of attracting or repelling Points is reduced to the Search and Differentiation of one central Relation, or characteristic Function.},
	journal = {Philosophical Transactions of the Royal Society},
	volume = {124},
	number = {},
	pages = {247-308},
	url = {https://www.jstor.org/stable/108066},
	doi = {}
}
@inproceedings{中根美知代2000,
	author = {中根美知代},
	year = {2000},
	title = {物理学から数学へ : Hamilton-Jacobi 理論の誕生 (数学史の研究)},
	booktitle = {数理解析研究所講究録},
	volume = {1130},
	pages = {58-71},
	url = {http://hdl.handle.net/2433/63679},
	doi = {}
}
@article{Fermat1657,
	author = {Pierre de Fermat},
	year = {1657},
	title = {Marin Cureau de la Chambre, La Lumière (Chez Iacqves D'Allin, Paris, 1657)},
	journal = {Personal correspondence},
	volume = {},
	number = {},
	pages = {},
	url = {https://gallica.bnf.fr/ark:/12148/bpt6k94859n.pdf},
	doi = {}
}
@book{Euler1744,
	author = {Leonhard Euler},
	year = {1744},
	title = {ethodus Inveniendi Lineas Curvas Maximi Minimive Proprietate Gaudentes sive Solutio Problematis Isoperimetrici Latissimo Sensu Accepti},
	series = {},
	volume = {},
	edition = {},
	url = {https://archive.org/details/methodusinvenie00eule},
	doi = {},
	publisher = {Lausannæ ; Genevæ : Apud Marcum-Michaelem Bousquet & Socios}
}
@article{Bernoulli1969,
	author = {John Bernoulli},
	year = {1696},
	title = {Poblema novum ad cujus solution em mathematici invitantur},
	journal = {Acta Eruditorum},
	volume = {1},
	number = {},
	pages = {269},
	url = {},
	doi = {}
}
@book{Lagrange1788,
	author = {Joseph-Louis Lagrange},
	year = {1788},
	title = {Mécanique Analytique},
	series = {},
	volume = {},
	edition = {},
	url = {https://gallica.bnf.fr/ark:/12148/bpt6k862625},
	doi = {},
	publisher = {}
}
@unpublished{Hanc2017,
	author = {Jozef Hanc},
	year = {2017},
	title = {The Original Euler's Calculus-of-Variations Method: Key to Lagrangian Mechanics for Beginners},
	url = {https://www.eftaylor.com/pub/HancEulerEJP.pdf},
	doi = {}
}
@book{Monge1781,
	author = {Gaspard Monge},
	year = {1781},
	title = {Mémoire sur la théorie des déblais et des remblais},
	series = {},
	volume = {},
	edition = {},
	url = {},
	doi = {},
	publisher = {Imprimerie Royale}
}

@article{Kantorovich1942,
    author          = {L. V. Kantorovich},
    year            = {1942},
    title           = {On the Translocation of Masses},
    language        = {Russian},
    journal         = {Doklady Akademii Nauk SSSR},
    volume          = {37},
    number          = {7-8},
    pages           = {227-229},
    url             = {https://link.springer.com/article/10.1007/s10958-006-0049-2}
}
@article{Kantorovich1940,
    author          = {L. V. Kantorovich},
    year            = {1940},
    title           = {On an Effective Method of Solving Certain Classes of Extremal Problems},
    language        = {Russian},
    journal         = {Doklady Akademii Nauk SSSR},
    volume          = {28},
	pages = {212-215},
}
@article{Kantorovich1948,
    author          = {L. V. Kantorovich},
    year            = {1948},
    title           = {On a Problem of Monge},
    language        = {Russian},
    journal         = {Uspekhi Matematicheskikh Nauk},
    volume          = {3},
    number          = {2},
    pages           = {225-226},
    url             = {https://doi.org/10.1007/s10958-006-0050-9}
}

@article{Vershik2013,
	author = {Vershik, A.  M. },
	date = {2013/12/01},
	date-added = {2024-09-04 13:28:11 +0900},
	date-modified = {2024-09-04 13:28:11 +0900},
	doi = {10.1007/s00283-013-9380-x},
	id = {Vershik2013},
	isbn = {1866-7414},
	journal = {The Mathematical Intelligencer},
	number = {4},
	pages = {1--9},
	title = {Long History of the Monge-Kantorovich Transportation Problem},
	url = {https://doi.org/10.1007/s00283-013-9380-x},
	volume = {35},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1007/s00283-013-9380-x}}
@Inbook{Ambrosio2024,
author="Ambrosio, Luigi
and Quarteroni, Alfio",
editor="Ambrosio, Luigi
and Quarteroni, Alfio",
title="Talking about Optimal Transport",
bookTitle="Conversations on Optimal Transport",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="1--19",
abstract="In this session, we have the pleasure of hosting Luigi Ambrosio, a professor at the Scuola Normale Superiore in Pisa, Italy, as our guest. Professor Ambrosio, who recently co-authored the new textbook, Lectures on Optimal Transport, with Elia Bru{\'e} and Daniele Semola, engages in a lively conversation with Alfio Quarteroni, a professor at Politecnico di Milano.",
isbn="978-3-031-51685-6",
doi="10.1007/978-3-031-51685-6_1",
url="https://doi.org/10.1007/978-3-031-51685-6_1"
}
@incollection{Levi2014,
	author = {Mark Levi},
	booktitle = {SIAM News},
	publisher = {SIAM},
	title = {Quick! Find a Solution to the Brachistochrone Problem},
	year = {2014}
}
@Inbook{Ambrosio2003,
author="Ambrosio, Luigi",
title="Lecture Notes on Optimal Transport Problems",
bookTitle="Mathematical Aspects of Evolving Interfaces: Lectures given at the C.I.M.-C.I.M.E. joint Euro-Summer School held in Madeira, Funchal, Portugal, July 3-9, 2000",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="1--52",
abstract="1 Some elementary examples2 Optimal transport plans: existence and regularity3 The one dimensional case4 The ODE version of the optimal transport problem5 The PDE version of the optimal transport problem and the p-laplacian approximation6 Existence of optimal transport maps7 Regularity and uniqueness of the transport density8 The Bouchitt{\'e}-Buttazzo mass optimization problem9 Appendix: some measure theoretic resultsReferences",
isbn="978-3-540-39189-0",
doi="10.1007/978-3-540-39189-0_1",
url="https://doi.org/10.1007/978-3-540-39189-0_1"
}
@book{Evans-Gangbo1999,
	author = {L. C. Evans and W. Gangbo},
	year = {1999},
	title = {Differential Equations Methods for the Monge-Kantorovich Mass Transfer Problem},
	series = {Memoirs of the American Mathematical Society},
	volume = {137},
	number = {653},
	edition = {},
	url = {https://doi.org/10.1090/memo/0653},
	doi = {},
	publisher = {American Mathematical Society}
}
@unpublished{Figalli2023,
	author = {Alessio Figalli},
	year = {2023},
	title = {An Introduction to Optimal Transport and Wasserstein Gradient Flows},
	url = {https://people.math.ethz.ch/~afigalli/lecture-notes},
	note = {Lecture Note},
	doi = {}
}
@book{Figalli-Glaudo2023,
  author = {Alessio Figalli and Federico Glaudo},
  year = {2023},
  title = {{An Invitation to Optimal Transport, Wasserstein Distances, and Gradient Flows}},
  series = {},
  volume = {},
  edition = {2},
  doi = {10.4171/ETB/25},
  publisher = {European Mathematical Society}
}
@book{佐藤竜馬2023,
	author = {佐藤竜馬},
	year = {2023},
	title = {最適輸送の理論とアルゴリズム},
	series = {機械学習プロフェッショナルシリーズ},
	volume = {},
	edition = {},
	url = {https://www.kspub.co.jp/book/detail/5305140.html},
	doi = {},
	publisher = {講談社サイエンティフィック}
}
@Inbook{Figalli-Ambrosio2024,
author="Figalli, Alessio
and Ambrosio, Luigi",
editor="Ambrosio, Luigi
and Quarteroni, Alfio",
title="Optimal Transport, Fields Medals and beyond",
bookTitle="Conversations on Optimal Transport",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="21--38",
abstract="Welcome to the Springer Math Podcast. This month, we're delighted to host Alessio Figalli, the Director of the Institute for Mathematical Research at ETH Zurich, Switzerland. A distinguished academic, Professor Figalli completed his PhD at the Scuola Normale Superiore of Pisa, Italy, and at the Ecole Normale Superieure of Lyon, France. His research has taken him across France, the United States, and Switzerland. His notable contributions to the theory of optimal transport have earned him numerous accolades, including the prestigious Fields Medal in 2018, and the European Mathematical Society Prize in 2012.",
isbn="978-3-031-51685-6",
doi="10.1007/978-3-031-51685-6_2",
url="https://doi.org/10.1007/978-3-031-51685-6_2"
}
@Inbook{Gigli-DeLellis2024,
author="Gigli, Nicola
and De Lellis, Camillo",
editor="Ambrosio, Luigi
and Quarteroni, Alfio",
title="From moving masses to bending spaces: an excursion in metric geometry",
bookTitle="Conversations on Optimal Transport",
year="2024",
publisher="Springer Nature Switzerland",
address="Cham",
pages="39--58",
abstract="Welcome to the Springer Math Podcast. In this month's podcast, Camillo De Lellis, a researcher at the Institute for Advanced Study in Princeton, converses with Nicola Gigli from the Scuola Internazionale Superiore di Studi Avanzati in Trieste, Italy. They delve into Nicola Gigli's personal journey in and out of mathematics, discussing his path to the topics of his research and his enthusiasm for them. In this conversation, they also explore the connection between the concepts of optimal transport and the curvature of space, a discovery that has given rise to a flourishing research field at the intersection of multiple areas of mathematics. Originally aired by the Springer Nature webinar series, this interview has been specifically adapted for the podcastformat.",
isbn="978-3-031-51685-6",
doi="10.1007/978-3-031-51685-6_3",
url="https://doi.org/10.1007/978-3-031-51685-6_3"
}
@article{久保川達也2006,
  title={線形混合モデルと小地域の推定},
  author={久保川達也},
  journal={応用統計学},
  volume={35},
  number={3},
  pages={139-161},
  year={2006},
  doi={10.5023/jappstat.35.139}
}
@article{Kubokawa2000,
  title={ESTIMATION OF VARIANCE AND COVARIANCE COMPONENTS IN ELLIPTICALLY CONTOURED DISTRIBUTIONS},
  author={Tatsuya Kubokawa},
  journal={JOURNAL OF THE JAPAN STATISTICAL SOCIETY},
  volume={30},
  number={2},
  pages={143-176},
  year={2000},
  doi={10.14490/jjss1995.30.143}
}
@article{Battese+1988,
author = {George E. Battese and Rachel M. Harter and Wayne A. Fuller},
title = {An Error-Components Model for Prediction of County Crop Areas Using Survey and Satellite Data},
journal = {Journal of the American Statistical Association},
volume = {83},
number = {401},
pages = {28--36},
year = {1988},
publisher = {ASA Website},
doi = {10.1080/01621459.1988.10478561},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478561
    

},
eprint = { 
    
    
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478561
    

}

}
@article{Gelman2014,
author = {Andrew Gelman},
title = {{How Bayesian Analysis Cracked the Red-State, Blue-State Problem}},
volume = {29},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {26 -- 35},
keywords = {Multilevel regression and poststratification (MRP), political science, sample surveys, sparse data, voting},
year = {2014},
doi = {10.1214/13-STS458},
URL = {https://doi.org/10.1214/13-STS458}
}
@article{Efron-Morris1975,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2285814},
 abstract = {In 1961, James and Stein exhibited an estimator of the mean of a multivariate normal distribution having uniformly lower mean squared error than the sample mean. This estimator is reviewed briefly in an empirical Bayes context. Stein's rule and its generalizations are then applied to predict baseball averages, to estimate toxomosis prevalence rates, and to estimate the exact size of Pearson's chi-square test with results from a computer simulation. In each of these examples, the mean square error of these rules is less than half that of the sample mean.},
 author = {Bradley Efron and Carl Morris},
 journal = {Journal of the American Statistical Association},
 number = {350},
 pages = {311--319},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Data Analysis Using Stein's Estimator and its Generalizations},
 urldate = {2024-09-11},
 volume = {70},
 year = {1975}
}
@article{Kubokawa-Srivastava1999,
author = {Tatsuya Kubokawa and M. S. Srivastava},
title = {{Improved nonnegative estimation of multivariate components of variance}},
volume = {27},
journal = {The Annals of Statistics},
number = {6},
publisher = {Institute of Mathematical Statistics},
pages = {2008 -- 2032},
keywords = {minimax and unbiased estimators, random effects model, restricted maximum likelihood estimator, Stein loss},
year = {1999},
doi = {10.1214/aos/1017939248},
URL = {https://doi.org/10.1214/aos/1017939248}
}
@article{丹後俊郎1988,
  title={死亡指標の経験的ベイズ推定量について},
  author={丹後俊郎},
  journal={応用統計学},
  volume={17},
  number={2},
  pages={81-96},
  year={1988},
  doi={10.5023/jappstat.17.81}
}
@article{Clayton-Kaldor1987,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2532003},
 abstract = {There have been many attempts in recent years to map incidence and mortality from diseases such as cancer. Such maps usually display either relative rates in each district, as measured by a standardized mortality ratio (SMR) or some similar index, or the statistical significance level for a test of the difference between the rates in that district and elsewhere. Neither of these approaches is fully satisfactory and we propose a new approach using empirical Bayes estimation. The resulting estimators represent a weighted compromise between the SMR, the overall mean relative rate, and a local mean of the relative rate in nearby areas. The compromise solution depends on the reliability of each individual SMR and on estimates of the overall amount of dispersion of relative rates over different districts.},
 author = {David Clayton and John Kaldor},
 journal = {Biometrics},
 number = {3},
 pages = {671--681},
 publisher = {[Wiley, International Biometric Society]},
 title = {Empirical Bayes Estimates of Age-Standardized Relative Risks for Use in Disease Mapping},
 urldate = {2024-09-11},
 volume = {43},
 year = {1987}
}
@article{西川正子2008,
  title={生存時間解析における競合リスクモデル},
  author={西川正子},
  journal={計量生物学},
  volume={29},
  number={2},
  pages={141-170},
  year={2008},
  doi={10.5691/jjb.29.141}
}
@article{Kaplan-Meier1958,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2281868},
 author = {E. L. Kaplan and Paul Meier},
 journal = {Journal of the American Statistical Association},
 number = {282},
 pages = {457--481},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Nonparametric Estimation from Incomplete Observations},
 urldate = {2024-09-12},
 volume = {53},
 year = {1958}
}
@misc{Hardcastle+2024,
      title={Averaging polyhazard models using Piecewise deterministic Monte Carlo with applications to data with long-term survivors}, 
      author={Luke Hardcastle and Samuel Livingstone and Gianluca Baio},
      year={2024},
      eprint={2406.14182},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2406.14182}, 
}
@article{森満2016,
  title={専門医に必要な統計の知識と研究デザイン},
  author={森満},
  journal={日本耳鼻咽喉科学会会報},
  volume={119},
  number={7},
  pages={989-992},
  year={2016},
  doi={10.3950/jibiinkoka.119.989}
}
@article{Cutler-Ederer1958,
title = {Maximum utilization of the life table method in analyzing survival},
journal = {Journal of Chronic Diseases},
volume = {8},
number = {6},
pages = {699-712},
year = {1958},
issn = {0021-9681},
doi = {https://doi.org/10.1016/0021-9681(58)90126-7},
url = {https://www.sciencedirect.com/science/article/pii/0021968158901267},
author = {Sidney J. Cutler and Fred Ederer},
abstract = {We have illustrated the life table method for computing survival rates with 5-year survival data for cancer patients, emphasizing the advantage gained by including survival information on cases which entered the series too late to have had the opportunity to survive a full 5 years. The advantage is measured in terms of reduction in standard error of the survival rate. For the five series of patients in this paper, the reduction in standard error ranged from one-third to two-thirds.}
}
@techreport{Latimer2011,
	author = {Nicholas R. Latimer},
	institution = {National Institute for Health and Care Excellence},
	title = {NICE DSU Technical Support Document 14: Undertaking Survival Analysis for Economic Evaluations alongside Clinical Trials--Estrapolation with Patient-level Data},
	year = {2011},
	url = {https://www.sheffield.ac.uk/nice-dsu/tsds/survival-analysis},
}
@article{Berger-Sun1993,
author = {James O. Berger and Dongchu Sun},
title = {Bayesian Analysis for the Poly-Weibull Distribution},
journal = {Journal of the American Statistical Association},
volume = {88},
number = {424},
pages = {1412--1418},
year = {1993},
publisher = {ASA Website},
doi = {10.1080/01621459.1993.10476426},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476426
    

},
eprint = { 
    
    
        https://www.tandfonline.com/doi/pdf/10.1080/01621459.1993.10476426
    

}

}
@article{Louzada-Neto1999,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2533756},
 abstract = {We propose a polyhazard model to deal with lifetime data associated with latent competing risks. The causes of failure are assumed unobserved and affecting individuals independently. The general framework allows a broad class of hazard models that includes the most common hazard-based models. The model accommodates bathtub and multimodal hazards, keeping enough flexibility for common lifetime data that cannot be accommodated by usual hazard-based models. Maximum likelihood estimation is discussed, and parametric simulation is used for hypothesis testing.},
 author = {Francisco Louzada-Neto},
 journal = {Biometrics},
 number = {4},
 pages = {1281--1285},
 publisher = {[Wiley, International Biometric Society]},
 title = {Polyhazard Models for Lifetime Data},
 urldate = {2024-09-12},
 volume = {55},
 year = {1999}
}






@article{Latimer2013,
author = {Nicholas R. Latimer},
title ={Survival Analysis for Economic Evaluations Alongside Clinical Trials—Extrapolation with Patient-Level Data: Inconsistencies, Limitations, and a Practical Guide},

journal = {Medical Decision Making},
volume = {33},
number = {6},
pages = {743-754},
year = {2013},
doi = {10.1177/0272989X12472398},
    note ={PMID: 23341049},

URL = { 
    
        https://doi.org/10.1177/0272989X12472398
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0272989X12472398
    
    

},
}
@article{齋藤-室谷2023,
  title={マルチステートモデルの理論とがん臨床研究への応用},
  author={齋藤哲雄 and 室谷健太},
  journal={日本統計学会誌},
  volume={52},
  number={2},
  pages={221-267},
  year={2023},
  doi={10.11329/jjssj.52.221}
}
@article{齋藤-室谷2024,
  title={Competing Risks and Multistate Modelsin Oncology Clinical Trials},
  author={Tetsuo Saito and Kenta Murotani},
  journal={Japanese Journal of Biometrics},
  volume={45},
  number={1},
  pages={37-65},
  year={2024},
  doi={10.5691/jjb.45.37}
}
@article{Negrin+2017,
author = {Miguel A. Negrín and Julian Nam and Andrew H. Briggs},
title ={Bayesian Solutions for Handling Uncertainty in Survival Extrapolation},

journal = {Medical Decision Making},
volume = {37},
number = {4},
pages = {367-376},
year = {2017},
doi = {10.1177/0272989X16650669},
    note ={PMID: 27281336},

URL = { 
    
        https://doi.org/10.1177/0272989X16650669
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0272989X16650669
    
    

}
,
}
@article{Demiris+2015,
author = {Nikolaos Demiris and David Lunn and Linda D Sharples},
title ={Survival extrapolation using the poly-Weibull model},

journal = {Statistical Methods in Medical Research},
volume = {24},
number = {2},
pages = {287-301},
year = {2015},
doi = {10.1177/0962280211419645},
    note ={PMID: 21937472},

URL = { 
    
        https://doi.org/10.1177/0962280211419645
    
    

},
eprint = { 
    
        https://doi.org/10.1177/0962280211419645
    
    

},
}
@article{Benaglia+2015,
author = {Benaglia, Tatiana and Jackson, Christopher H. and Sharples, Linda D.},
title = {Survival extrapolation in the presence of cause specific hazards},
journal = {Statistics in Medicine},
volume = {34},
number = {5},
pages = {796-811},
keywords = {survival analysis, survival extrapolation, poly-weibull, polyhazard, cause specific hazards},
doi = {https://doi.org/10.1002/sim.6375},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6375},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.6375},
abstract = {Health economic evaluations require estimates of expected survival from patients receiving different interventions, often over a lifetime. However, data on the patients of interest are typically only available for a much shorter follow-up time, from randomised trials or cohorts. Previous work showed how to use general population mortality to improve extrapolations of the short-term data, assuming a constant additive or multiplicative effect on the hazards for all-cause mortality for study patients relative to the general population. A more plausible assumption may be a constant effect on the hazard for the specific cause of death targeted by the treatments. To address this problem, we use independent parametric survival models for cause-specific mortality among the general population. Because causes of death are unobserved for the patients of interest, a polyhazard model is used to express their all-cause mortality as a sum of latent cause-specific hazards. Assuming proportional cause-specific hazards between the general and study populations then allows us to extrapolate mortality of the patients of interest to the long term. A Bayesian framework is used to jointly model all sources of data. By simulation, we show that ignoring cause-specific hazards leads to biased estimates of mean survival when the proportion of deaths due to the cause of interest changes through time. The methods are applied to an evaluation of implantable cardioverter defibrillators for the prevention of sudden cardiac death among patients with cardiac arrhythmia. After accounting for cause-specific mortality, substantial differences are seen in estimates of life years gained from implantable cardioverter defibrillators. © 2014 The Authors Statistics in Medicine Published by John Wiley \& Sons Ltd.},
year = {2015}
}
@article{Mitchell-Beauchamp1988,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2290129},
 abstract = {This article is concerned with the selection of subsets of predictor variables in a linear regression model for the prediction of a dependent variable. It is based on a Bayesian approach, intended to be as objective as possible. A probability distribution is first assigned to the dependent variable through the specification of a family of prior distributions for the unknown parameters in the regression model. The method is not fully Bayesian, however, because the ultimate choice of prior distribution from this family is affected by the data. It is assumed that the predictors represent distinct observables; the corresponding regression coefficients are assigned independent prior distributions. For each regression coefficient subject to deletion from the model, the prior distribution is a mixture of a point mass at 0 and a diffuse uniform distribution elsewhere, that is, a "spike and slab" distribution. The random error component is assigned a normal distribution with mean 0 and standard deviation σ, where ln(σ) has a locally uniform noninformative prior distribution. The appropriate posterior probabilities are derived for each submodel. If the regression coefficients have identical priors, the posterior distribution depends only on the data and the parameter γ, which is the height of the spike divided by the height of the slab for the common prior distribution. This parameter is not assigned a probability distribution; instead, it is considered a parameter that indexes the members of a class of Bayesian methods. Graphical methods are proposed as informal guides for choosing γ, assessing the complexity of the response function and the strength of the individual predictor variables, and assessing the degree of uncertainty about the best submodel. The following plots against γ are suggested: (a) posterior probability that a particular regression coefficient is 0; (b) posterior expected number of terms in the model; (c) posterior entropy of the submodel distribution; (d) posterior predictive error; and (e) posterior probability of goodness of fit. Plots (d) and (e) are suggested as ways to choose γ. The predictive error is determined using a Bayesian cross-validation approach that generates a predictive density for each observation, given all of the data except that observation, that is, a type of "leave one out" approach. The goodness-of-fit measure is the sum of the posterior probabilities of all submodels that pass a standard F test for goodness of fit relative to the full model, at a specified level of significance. The dependence of the results on the scaling of the variables is discussed, and some ways to choose the scaling constants are suggested. Examples based on a large data set arising from an energy-conservation study are given to demonstrate the application of the methods.},
 author = {T. J. Mitchell and J. J. Beauchamp},
 journal = {Journal of the American Statistical Association},
 number = {404},
 pages = {1023--1032},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Bayesian Variable Selection in Linear Regression},
 urldate = {2024-09-12},
 volume = {83},
 year = {1988}
}
@article{Ley-Steel2009,
author = {Ley, Eduardo and Steel, Mark F.J.},
title = {On the effect of prior assumptions in Bayesian model averaging with applications to growth regression},
journal = {Journal of Applied Econometrics},
volume = {24},
number = {4},
pages = {651-674},
doi = {https://doi.org/10.1002/jae.1057},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jae.1057},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/jae.1057},
abstract = {Abstract We consider the problem of variable selection in linear regression models. Bayesian model averaging has become an important tool in empirical settings with large numbers of potential regressors and relatively limited numbers of observations. We examine the effect of a variety of prior assumptions on the inference concerning model size, posterior inclusion probabilities of regressors and on predictive performance. We illustrate these issues in the context of cross-country growth regressions using three datasets with 41–67 potential drivers of growth and 72–93 observations. Finally, we recommend priors for use in this and related contexts. Copyright © 2009 John Wiley \& Sons, Ltd.},
year = {2009}
}
@article{Polson-Scott2012,
author = {Nicholas G. Polson and James G. Scott},
title = {{On the Half-Cauchy Prior for a Global Scale Parameter}},
volume = {7},
journal = {Bayesian Analysis},
number = {4},
publisher = {International Society for Bayesian Analysis},
pages = {887 -- 902},
keywords = {hierarchical models, normal scale mixtures, shrinkage},
year = {2012},
doi = {10.1214/12-BA730},
URL = {https://doi.org/10.1214/12-BA730}
}
@article{Gelman2006,
author = {Andrew Gelman},
title = {{Prior distributions for variance parameters in hierarchical models (comment on article by Browne and Draper)}},
volume = {1},
journal = {Bayesian Analysis},
number = {3},
publisher = {International Society for Bayesian Analysis},
pages = {515 -- 534},
keywords = {Bayesian inference, conditional conjugacy, folded-noncentral-$t$ distribution, half-$t$ distribution, hierarchical model, multilevel model, noninformative prior distribution, weakly informative prior distribution},
year = {2006},
doi = {10.1214/06-BA117A},
URL = {https://doi.org/10.1214/06-BA117A}
}
@article{Jasra+2005,
author = {A. Jasra and C. C. Holmes and D. A. Stephens},
title = {{Markov Chain Monte Carlo Methods and the Label Switching Problem in Bayesian Mixture Modeling}},
volume = {20},
journal = {Statistical Science},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {50 -- 67},
keywords = {Bayesian statistics, Identifiability, label switching, MCMC, mixture modeling, sensitivity analysis},
year = {2005},
doi = {10.1214/088342305000000016},
URL = {https://doi.org/10.1214/088342305000000016}
}
@article{山口一大2022,
  title={項目反応理論モデルのパラメタ推定法の展開},
  author={山口一大},
  journal={日本テスト学会誌},
  volume={18},
  number={1},
  pages={103-131},
  year={2022},
  doi={10.24690/jart.18.1_103}
}
@phdthesis{Wilson1984,
	author = {Mark R. Wilson},
	school = {University of Chicago},
	title = {A Psychometric Model of Hierarchical Development},
	year = {1984},

}

@article{Embretson1984,
	abstract = {The purpose of the current paper is to propose a general multicomponent latent trait model (GLTM) for response processes. The proposed model combines the linear logistic latent trait (LLTM) with the multicomponent latent trait model (MLTM). As with both LLTM and MLTM, the general multicomponent latent trait model can be used to (1) test hypotheses about the theoretical variables that underlie response difficulty and (2) estimate parameters that describe test items by basic substantive properties. However, GLTM contains both component outcomes and complexity factors in a single model and may be applied to data that neither LLTM nor MLTM can handle. Joint maximum likelihood estimators are presented for the parameters of GLTM and an application to cognitive test items is described.},
	author = {Embretson (Whitely), Susan},
	date = {1984/06/01},
	date-added = {2024-09-17 13:20:02 +0900},
	date-modified = {2024-09-17 13:20:02 +0900},
	doi = {10.1007/BF02294171},
	id = {Embretson (Whitely)1984},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {2},
	pages = {175--186},
	title = {A general latent trait model for response processes},
	url = {https://doi.org/10.1007/BF02294171},
	volume = {49},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1007/BF02294171}}
@misc{yuimadocs2024,
	author = {The YUIMA Project Team},
	title = {Package `yuima`},
	year = {2024},
	url = {https://cran.r-project.org/web/packages/yuima/index.html},
}
@article{Cox+1985,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1911242},
 abstract = {This paper uses an intertemporal general equilibrium asset pricing model to study the term structure of interest rates. In this model, anticipations, risk aversion, investment alternatives, and preferences about the timing of consumption all play a role in determining bond prices. Many of the factors traditionally mentioned as influencing the term structure are thus included in a way which is fully consistent with maximizing behavior and rational expectations. The model leads to specific formulas for bond prices which are well suited for empirical testing.},
 author = {John C. Cox and Jonathan E. Ingersoll and Stephen A. Ross},
 journal = {Econometrica},
 number = {2},
 pages = {385--407},
 publisher = {[Wiley, Econometric Society]},
 title = {A Theory of the Term Structure of Interest Rates},
 urldate = {2024-09-17},
 volume = {53},
 year = {1985}
}
@article{Hayashi-Yoshida2005,
author = {Takaki Hayashi and Nakahiro Yoshida},
title = {{On covariance estimation of non-synchronously observed diffusion processes}},
volume = {11},
journal = {Bernoulli},
number = {2},
publisher = {Bernoulli Society for Mathematical Statistics and Probability},
pages = {359 -- 379},
keywords = {Diffusions, Discrete-time observations, high-frequency data, mathematical finance, non-synchronous trading, Quadratic Variation, realized volatility},
year = {2005},
doi = {10.3150/bj/1116340299},
URL = {https://doi.org/10.3150/bj/1116340299}
}
@article{Gelman+2015,
author = {Andrew Gelman and Daniel Lee and Jiqiang Guo},
title ={Stan: A Probabilistic Programming Language for Bayesian Inference and Optimization},

journal = {Journal of Educational and Behavioral Statistics},
volume = {40},
number = {5},
pages = {530-543},
year = {2015},
doi = {10.3102/1076998615606113},

URL = { 
    
        https://doi.org/10.3102/1076998615606113
    
    

},
eprint = { 
    
        https://doi.org/10.3102/1076998615606113
    
    

}
,
}

@article{Lunn+2000,
	abstract = {WinBUGS is a fully extensible modular framework for constructing and analysing Bayesian full probability models. Models may be specified either textually via the BUGS language or pictorially using a graphical interface called DoodleBUGS. WinBUGS processes the model specification and constructs an object-oriented representation of the model. The software offers a user-interface, based on dialogue boxes and menu commands, through which the model may then be analysed using Markov chain Monte Carlo techniques. In this paper we discuss how and why various modern computing concepts, such as object-orientation and run-time linking, feature in the software's design. We also discuss how the framework may be extended. It is possible to write specific applications that form an apparently seamless interface with WinBUGS for users with specialized requirements. It is also possible to interface with WinBUGS at a lower level by incorporating new object types that may be used by WinBUGS without knowledge of the modules in which they are implemented. Neither of these types of extension require access to, or even recompilation of, the WinBUGS source-code.},
	author = {Lunn, David J. and Thomas, Andrew and Best, Nicky and Spiegelhalter, David},
	date = {2000/10/01},
	date-added = {2024-09-20 13:45:47 +0900},
	date-modified = {2024-09-20 13:45:47 +0900},
	doi = {10.1023/A:1008929526011},
	id = {Lunn2000},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {4},
	pages = {325--337},
	title = {WinBUGS - A Bayesian modelling framework: Concepts, structure, and extensibility},
	url = {https://doi.org/10.1023/A:1008929526011},
	volume = {10},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1023/A:1008929526011}}
@article{Green1995,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2337340},
 abstract = {Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.},
 author = {Peter J. Green},
 journal = {Biometrika},
 number = {4},
 pages = {711--732},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Reversible Jump Markov Chain Monte Carlo Computation and Bayesian Model Determination},
 urldate = {2024-09-22},
 volume = {82},
 year = {1995}
}

@article{McShane+2019,
	author = {Blakeley B. McShane, David Gal, Andrew Gelman, Christian Robert and Jennifer L. Tackett},
	doi = {10.1080/00031305.2018.1527253},
	eprint = {https://doi.org/10.1080/00031305.2018.1527253},
	journal = {The American Statistician},
	number = {sup1},
	pages = {235--245},
	publisher = {ASA Website},
	title = {Abandon Statistical Significance},
	url = {https://doi.org/10.1080/00031305.2018.1527253},
	volume = {73},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1080/00031305.2018.1527253}}

@article{Gelman-Stern2006,
	author = {Andrew Gelman and Hal Stern},
	doi = {10.1198/000313006X152649},
	eprint = {https://doi.org/10.1198/000313006X152649},
	journal = {The American Statistician},
	number = {4},
	pages = {328--331},
	publisher = {ASA Website},
	title = {The Difference Between ``Significant'' and ``Not Significant'' is not Itself Statistically Significant},
	url = {https://doi.org/10.1198/000313006X152649},
	volume = {60},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1198/000313006X152649}}

@article{Wagenmakers+2016,
	abstract = { The practical advantages of Bayesian inference are demonstrated here through two concrete examples. In the first example, we wish to learn about a criminal's IQ: a problem of parameter estimation. In the second example, we wish to quantify and track support in favor of the null hypothesis that Adam Sandler movies are profitable regardless of their quality: a problem of hypothesis testing. The Bayesian approach unifies both problems within a coherent predictive framework, in which parameters and models that predict the data successfully receive a boost in plausibility, whereas parameters and models that predict poorly suffer a decline. Our examples demonstrate how Bayesian analyses can be more informative, more elegant, and more flexible than the orthodox methodology that remains dominant within the field of psychology. },
	author = {Eric-Jan Wagenmakers and Richard D. Morey and Michael D. Lee},
	doi = {10.1177/0963721416643289},
	eprint = {https://doi.org/10.1177/0963721416643289},
	journal = {Current Directions in Psychological Science},
	number = {3},
	pages = {169-176},
	title = {Bayesian Benefits for the Pragmatic Researcher},
	url = {https://doi.org/10.1177/0963721416643289},
	volume = {25},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1177/0963721416643289}}

@article{Dienes-Mclatchie2018,
	abstract = {Inference using significance testing and Bayes factors is compared and contrasted in five case studies based on real research. The first study illustrates that the methods will often agree, both in motivating researchers to conclude that H1 is supported better than H0, and the other way round, that H0 is better supported than H1. The next four, however, show that the methods will also often disagree. In these cases, the aim of the paper will be to motivate the sensible evidential conclusion, and then see which approach matches those intuitions. Specifically, it is shown that a high-powered non-significant result is consistent with no evidence for H0 over H1 worth mentioning, which a Bayes factor can show, and, conversely, that a low-powered non-significant result is consistent with substantial evidence for H0 over H1, again indicated by Bayesian analyses. The fourth study illustrates that a high-powered significant result may not amount to any evidence for H1 over H0, matching the Bayesian conclusion. Finally, the fifth study illustrates that different theories can be evidentially supported to different degrees by the same data; a fact that P-values cannot reflect but Bayes factors can. It is argued that appropriate conclusions match the Bayesian inferences, but not those based on significance testing, where they disagree.},
	author = {Dienes, Zoltan and Mclatchie, Neil},
	date = {2018/02/01},
	date-added = {2024-09-23 15:12:32 +0900},
	date-modified = {2024-09-23 15:12:32 +0900},
	doi = {10.3758/s13423-017-1266-z},
	id = {Dienes2018},
	isbn = {1531-5320},
	journal = {Psychonomic Bulletin \& Review},
	number = {1},
	pages = {207--218},
	title = {Four reasons to prefer Bayesian analyses over significance testing},
	url = {https://doi.org/10.3758/s13423-017-1266-z},
	volume = {25},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3758/s13423-017-1266-z}}

@article{vandenBergh+2020,
  author = {Don van den Bergh and Johnny van Doorn and Maarten Marsman and Tim Draws and Erik-Jan van Kesteren and Koen Derks and Fabian Dablander and Quentin F. Gronau and Šimon Kucharský and Akash R. Komarlu Narendra Gupta and Alexandra Sarafoglou and Jan G. Voelkel and Angelika Stefan and Alexander Ly and Max Hinne and Dora Matzke and Eric-Jan Wagenmakers},
  year = {2020},
  title = {{A Tutorial on Conducting and Interpreting a Bayesian ANOVA in JASP}},
  journal = {L’Année psychologique},
  volume = {120},
  number = {},
  pages = {73-96},
  url = {https://doi.org/10.3917/anpsy1.201.0073}
}

@article{Rouder+2016,
	abstract = {Analysis of variance (ANOVA), the workhorse analysis of experimental designs, consists of F-tests of main effects and interactions. Yet, testing, including traditional ANOVA, has been recently critiqued on a number of theoretical and practical grounds. In light of these critiques, model comparison and model selection serve as an attractive alternative. Model comparison differs from testing in that one can support a null or nested model vis-a-vis a more general alternative by penalizing more flexible models. We argue this ability to support more simple models allows for more nuanced theoretical conclusions than provided by traditional ANOVA F-tests. We provide a model comparison strategy and show how ANOVA models may be reparameterized to better address substantive questions in data analysis.},
	author = {Rouder, Jeffrey N. and Engelhardt, Christopher R. and McCabe, Simon and Morey, Richard D.},
	date = {2016/12/01},
	date-added = {2024-09-23 17:04:20 +0900},
	date-modified = {2024-09-23 17:04:20 +0900},
	doi = {10.3758/s13423-016-1026-5},
	id = {Rouder2016},
	isbn = {1531-5320},
	journal = {Psychonomic Bulletin \& Review},
	number = {6},
	pages = {1779--1786},
	title = {Model comparison in ANOVA},
	url = {https://doi.org/10.3758/s13423-016-1026-5},
	volume = {23},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.3758/s13423-016-1026-5}}
@article{Shapiro-Wilk1965,
    author = {Shapiro, S. S. and Wilk, M. B.},
    title = "{An analysis of variance test for normality (complete samples)†}",
    journal = {Biometrika},
    volume = {52},
    number = {3-4},
    pages = {591-611},
    year = {1965},
    month = {12},
    issn = {0006-3444},
    doi = {10.1093/biomet/52.3-4.591},
    url = {https://doi.org/10.1093/biomet/52.3-4.591},
    eprint = {https://academic.oup.com/biomet/article-pdf/52/3-4/591/962907/52-3-4-591.pdf},
}

@article{Brown-Forsythe1974,
	author = {Morton B. Brown and Alan B. Forsythe},
	doi = {10.1080/01621459.1974.10482955},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1974.10482955},
	journal = {Journal of the American Statistical Association},
	number = {346},
	pages = {364--367},
	publisher = {ASA Website},
	title = {Robust Tests for the Equality of Variances},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1974.10482955},
	volume = {69},
	year = {1974},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1974.10482955},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1974.10482955}}
@article{Mauchly1940,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2235878},
 author = {John W. Mauchly},
 journal = {The Annals of Mathematical Statistics},
 number = {2},
 pages = {204--209},
 publisher = {Institute of Mathematical Statistics},
 title = {Significance Test for Sphericity of a Normal n-Variate Distribution},
 urldate = {2024-09-23},
 volume = {11},
 year = {1940}
}

@article{Tijmstra2018,
	abstract = {This article explores whether the null hypothesis significance testing (NHST) framework provides a sufficient basis for the evaluation of statistical model assumptions. It is argued that while NHST-based tests can provide some degree of confirmation for the model assumption that is evaluated---formulated as the null hypothesis---these tests do not inform us of the degree of support that the data provide for the null hypothesis and to what extent the null hypothesis should be considered to be plausible after having taken the data into account. Addressing the prior plausibility of the model assumption is unavoidable if the goal is to determine how plausible it is that the model assumption holds. Without assessing the prior plausibility of the model assumptions, it remains fully uncertain whether the model of interest gives an adequate description of the data and thus whether it can be considered valid for the application at hand. Although addressing the prior plausibility is difficult, ignoring the prior plausibility is not an option if we want to claim that the inferences of our statistical model can be relied upon.},
	author = {Tijmstra, Jesper},
	date = {2018/04/01},
	date-added = {2024-09-23 17:19:55 +0900},
	date-modified = {2024-09-23 17:19:55 +0900},
	doi = {10.3758/s13423-018-1447-4},
	id = {Tijmstra2018},
	isbn = {1531-5320},
	journal = {Psychonomic Bulletin \& Review},
	number = {2},
	pages = {548--559},
	title = {Why checking model assumptions using null hypothesis significance tests does not suffice: A plea for plausibility},
	url = {https://doi.org/10.3758/s13423-018-1447-4},
	volume = {25},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.3758/s13423-018-1447-4}}

@article{Rouder+2012,
	abstract = {Bayes factors have been advocated as superior to p-values for assessing statistical evidence in data. Despite the advantages of Bayes factors and the drawbacks of p-values, inference by p-values is still nearly ubiquitous. One impediment to the adoption of Bayes factors is a lack of practical development, particularly a lack of ready-to-use formulas and algorithms. In this paper, we discuss and expand a set of default Bayes factor tests for ANOVA designs. These tests are based on multivariate generalizations of Cauchy priors on standardized effects, and have the desirable properties of being invariant with respect to linear transformations of measurement units. Moreover, these Bayes factors are computationally convenient, and straightforward sampling algorithms are provided. We cover models with fixed, random, and mixed effects, including random interactions, and do so for within-subject, between-subject, and mixed designs. We extend the discussion to regression models with continuous covariates. We also discuss how these Bayes factors may be applied in nonlinear settings, and show how they are useful in differentiating between the power law and the exponential law of skill acquisition. In sum, the current development makes the computation of Bayes factors straightforward for the vast majority of designs in experimental psychology.},
	author = {Jeffrey N. Rouder and Richard D. Morey and Paul L. Speckman and Jordan M. Province},
	doi = {https://doi.org/10.1016/j.jmp.2012.08.001},
	issn = {0022-2496},
	journal = {Journal of Mathematical Psychology},
	keywords = {Bayes factor, Model selection, Bayesian statistics, Linear models},
	number = {5},
	pages = {356-374},
	title = {Default Bayes factors for ANOVA designs},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249612000806},
	volume = {56},
	year = {2012},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022249612000806},
	bdsk-url-2 = {https://doi.org/10.1016/j.jmp.2012.08.001}}

@article{Gelman-Shalizi2013,
	abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
	author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
	doi = {https://doi.org/10.1111/j.2044-8317.2011.02037.x},
	eprint = {https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044-8317.2011.02037.x},
	journal = {British Journal of Mathematical and Statistical Psychology},
	number = {1},
	pages = {8-38},
	title = {Philosophy and the practice of Bayesian statistics},
	url = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.2011.02037.x},
	volume = {66},
	year = {2013},
	bdsk-url-1 = {https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.2011.02037.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2044-8317.2011.02037.x}}
@misc{Gelman+2020,
      title={Bayesian Workflow}, 
      author={Andrew Gelman and Aki Vehtari and Daniel Simpson and Charles C. Margossian and Bob Carpenter and Yuling Yao and Lauren Kennedy and Jonah Gabry and Paul-Christian Bürkner and Martin Modrák},
      year={2020},
      eprint={2011.01808},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2011.01808}, 
}
@book{Jeffreys1961,
  author = {Harold Jeffreys},
  year = {1961},
  title = {Theory of Probability},
  series = {},
  volume = {},
  edition = {3},
  url = {https://doi.org/10.1093/oso/9780198503682.001.0001},
  publisher = {Oxford University Press}
}

@article{Robert+2009,
	author = {Christian P. Robert and Nicolas Chopin and Judith Rousseau},
	doi = {10.1214/09-STS284},
	journal = {Statistical Science},
	keywords = {Bayes factor, Bayesian foundations, goodness of fit, Jeffreys's prior, Kullback divergence, noninformative prior, P-values, tests, σ-finite measure},
	number = {2},
	pages = {141 -- 172},
	publisher = {Institute of Mathematical Statistics},
	title = {{Harold Jeffreys's Theory of Probability Revisited}},
	url = {https://doi.org/10.1214/09-STS284},
	volume = {24},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1214/09-STS284}}
@article{Bayarri-Darcia-Donato2007,
    author = {Bayarri, M.J. and García-Donato, Gonzalo},
    title = "{Extending conventional priors for testing general hypotheses in linear models}",
    journal = {Biometrika},
    volume = {94},
    number = {1},
    pages = {135-152},
    year = {2007},
    month = {03},
    abstract = "{We consider that observations come from a general normal linear model and that it is desirable to test a simplifying null hypothesis about the parameters. We approach this problem from an objective Bayesian, model-selection perspective. Crucial ingredients for this approach are ‘proper objective priors’ to be used for deriving the Bayes factors. Jeffreys-Zellner-Siow priors have good properties for testing null hypotheses defined by specific values of the parameters in full-rank linear models. We extend these priors to deal with general hypotheses in general linear models, not necessarily of full rank. The resulting priors, which we call ‘conventional priors’, are expressed as a generalization of recently introduced ‘partially informative distributions’. The corresponding Bayes factors are fully automatic, easily computed and very reasonable. The methodology is illustrated for the change-point problem and the equality of treatments effects problem. We compare the conventional priors derived for these problems with other objective Bayesian proposals like the intrinsic priors. It is concluded that both priors behave similarly although interesting subtle differences arise. We adapt the conventional priors to deal with nonnested model selection as well as multiple-model comparison. Finally, we briefly address a generalization of conventional priors to nonnormal scenarios.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/asm014},
    url = {https://doi.org/10.1093/biomet/asm014},
    eprint = {https://academic.oup.com/biomet/article-pdf/94/1/135/617595/asm014.pdf},
}

@article{Zellner-Siow1980,
	abstract = {Bayesian posterior odds ratios for frequently encountered hypotheses about parameters of the normal linear multiple regression model are derived and discussed. For the particular prior distributions utilized, it is found that the posterior odds ratios can be well approximated by functions that are monotonic in usual sampling theoryF statistics. Some implications of this finding and the relation of our work to the pioneering work of Jeffreys and others are considered. Tabulations of odds ratios are provided and discussed.},
	author = {Zellner, A. and Siow, A.},
	date = {1980/02/01},
	date-added = {2024-09-23 19:22:46 +0900},
	date-modified = {2024-09-23 19:22:46 +0900},
	doi = {10.1007/BF02888369},
	id = {Zellner1980},
	isbn = {0041-0241},
	journal = {Trabajos de Estadistica Y de Investigacion Operativa},
	number = {1},
	pages = {585--603},
	title = {Posterior odds ratios for selected regression hypotheses},
	url = {https://doi.org/10.1007/BF02888369},
	volume = {31},
	year = {1980},
	bdsk-url-1 = {https://doi.org/10.1007/BF02888369}}

@article{Wetzels+2011,
	abstract = { Statistical inference in psychology has traditionally relied heavily on p-value significance testing. This approach to drawing conclusions from data, however, has been widely criticized, and two types of remedies have been advocated. The first proposal is to supplement p values with complementary measures of evidence, such as effect sizes. The second is to replace inference with Bayesian measures of evidence, such as the Bayes factor. The authors provide a practical comparison of p values, effect sizes, and default Bayes factors as measures of statistical evidence, using 855 recently published t tests in psychology. The comparison yields two main results. First, although p values and default Bayes factors almost always agree about what hypothesis is better supported by the data, the measures often disagree about the strength of this support; for 70\% of the data sets for which the p value falls between .01 and .05, the default Bayes factor indicates that the evidence is only anecdotal. Second, effect sizes can provide additional evidence to p values and default Bayes factors. The authors conclude that the Bayesian approach is comparatively prudent, preventing researchers from overestimating the evidence in favor of an effect. },
	author = {Ruud Wetzels and Dora Matzke and Michael D. Lee and Jeffrey N. Rouder and Geoffrey J. Iverson and Eric-Jan Wagenmakers},
	doi = {10.1177/1745691611406923},
	eprint = {https://doi.org/10.1177/1745691611406923},
	journal = {Perspectives on Psychological Science},
	note = {PMID: 26168519},
	number = {3},
	pages = {291-298},
	title = {Statistical Evidence in Experimental Psychology: An Empirical Comparison Using 855 t Tests},
	url = {https://doi.org/10.1177/1745691611406923},
	volume = {6},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1177/1745691611406923}}

@article{Gelman2005,
	author = {Andrew Gelman},
	doi = {10.1214/009053604000001048},
	journal = {The Annals of Statistics},
	keywords = {ANOVA, Bayesian inference, fixed effects, hierarchical model, Linear regression, multilevel model, random effects, variance components},
	number = {1},
	pages = {1 -- 53},
	publisher = {Institute of Mathematical Statistics},
	title = {{Analysis of variance---why it is more important than ever}},
	url = {https://doi.org/10.1214/009053604000001048},
	volume = {33},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1214/009053604000001048}}

@inbook{Tweney2014,
	abstract = {Abstract First used by behavioral scientists in the 1930s, use of Analysis of Variance (ANOVA) grew quickly after World War II, and exactly paralleled incorporation of statistical significance testing in general. Detailed consideration of this history suggests several reasons for the pattern of slow growth, followed by rapid institutionalization. In particular, ANOVA stood duty as a warrant of scientific legitimacy among behavioral scientists, a fact that may also be relevant to understanding recent critiques by psychologists of its overuse and misuse.},
	author = {Tweney, Ryan D.},
	booktitle = {Wiley StatsRef: Statistics Reference Online},
	doi = {https://doi.org/10.1002/9781118445112.stat06304},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat06304},
	isbn = {9781118445112},
	keywords = {ANOVA, analysis of variance, Fisher, significance testing, history},
	publisher = {John Wiley & Sons, Ltd},
	title = {History of Analysis of Variance},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06304},
	year = {2014},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat06304},
	bdsk-url-2 = {https://doi.org/10.1002/9781118445112.stat06304}}
@article{Kelter2022,
  author = {Kelter, Riko},
  title = {bayesanova: An R package for Bayesian Inference in the Analysis of Variance via Markov Chain Monte Carlo in Gaussian Mixture Models},
  journal = {The R Journal},
  year = {2022},
  note = {https://rjournal.github.io/},
  volume = {14},
  issue = {1},
  issn = {2073-4859},
  pages = {54-78}
}

@article{Robert2016,
	abstract = {This note is a discussion commenting on the paper by Ly et al. on ``Harold Jeffreys's Default Bayes Factor Hypothesis Tests: Explanation, Extension, and Application in Psychology'' and on the perceived shortcomings of the classical Bayesian approach to testing, while reporting on an alternative approach advanced by Kamary et al. (2014) as a solution to this quintessential inference problem.},
	author = {Christian P. Robert},
	doi = {https://doi.org/10.1016/j.jmp.2015.08.002},
	issn = {0022-2496},
	journal = {Journal of Mathematical Psychology},
	keywords = {Testing of hypotheses, Bayesian inference, Bayes factor, Evidence, Decision theory, Loss function, Consistency, Mixtures of distributions},
	note = {Bayes Factors for Testing Hypotheses in Psychological Research: Practical Relevance and New Developments},
	pages = {33-37},
	title = {The expected demise of the Bayes factor},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249615000504},
	volume = {72},
	year = {2016},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0022249615000504},
	bdsk-url-2 = {https://doi.org/10.1016/j.jmp.2015.08.002}}
@misc{Kamary+2018,
      title={Testing hypotheses via a mixture estimation model}, 
      author={Kaniav Kamary and Kerrie Mengersen and Christian P. Robert and Judith Rousseau},
      year={2018},
      eprint={1412.2044},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1412.2044}, 
}

@article{Kruschke2018,
	abstract = { This article explains a decision rule that uses Bayesian posterior distributions as the basis for accepting or rejecting null values of parameters. This decision rule focuses on the range of plausible values indicated by the highest density interval of the posterior distribution and the relation between this range and a region of practical equivalence (ROPE) around the null value. The article also discusses considerations for setting the limits of a ROPE and emphasizes that analogous considerations apply to setting the decision thresholds for p values and Bayes factors. },
	author = {John K. Kruschke},
	doi = {10.1177/2515245918771304},
	eprint = {https://doi.org/10.1177/2515245918771304},
	journal = {Advances in Methods and Practices in Psychological Science},
	number = {2},
	pages = {270-280},
	title = {Rejecting or Accepting Parameter Values in Bayesian Estimation},
	url = {https://doi.org/10.1177/2515245918771304},
	volume = {1},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1177/2515245918771304}}

@article{Kruskal-Wallis1952,
	author = {William H. Kruskal and W. Allen Wallis},
	doi = {10.1080/01621459.1952.10483441},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483441},
	journal = {Journal of the American Statistical Association},
	number = {260},
	pages = {583--621},
	publisher = {ASA Website},
	title = {Use of Ranks in One-Criterion Variance Analysis},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441},
	volume = {47},
	year = {1952},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483441},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1952.10483441}}

@article{vandenBergh+2023,
	abstract = { Analysis of variance (ANOVA) is widely used to assess the influence of one or more experimental (or quasi-experimental) manipulations on a continuous outcome. Traditionally, ANOVA is carried out in a frequentist manner using p values, but a Bayesian alternative has been proposed. Assuming that the proposed Bayesian ANOVA is closely modeled after its frequentist counterpart, one may be surprised to find that the two can yield very different conclusions when the design involves multiple repeated-measures factors. We illustrate such a discrepancy with a real data set from a two-factorial within-subject experiment. For this data set, the results of a frequentist and Bayesian ANOVA are in a disagreement about which main effect accounts for the variance in the data. The reason for this disagreement is that frequentist and the proposed Bayesian ANOVA use different model specifications. As currently implemented, the proposed Bayesian ANOVA assumes that there are no individual differences in the magnitude of effects. We suspect that this assumption is neither obvious to nor desired by most analysts because it is untenable in most applications. We argue here that the Bayesian ANOVA should be revised to allow for individual differences. As a default, we suggest the standard frequentist model specification but discuss a recently proposed alternative and provide guidance on how to choose the appropriate model specification. We end by discussing the implications of the revised model specification for previously published results of Bayesian ANOVAs. },
	author = {Don van den Bergh and Eric-Jan Wagenmakers and Frederik Aust},
	doi = {10.1177/25152459231168024},
	eprint = {https://doi.org/10.1177/25152459231168024},
	journal = {Advances in Methods and Practices in Psychological Science},
	number = {2},
	pages = {25152459231168024},
	title = {Bayesian Repeated-Measures Analysis of Variance: An Updated Methodology Implemented in JASP},
	url = {https://doi.org/10.1177/25152459231168024},
	volume = {6},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1177/25152459231168024}}

@article{Spiegelhalter+1994,
	abstract = {SUMMARY Statistical issues in conducting randomized trials include the choice of a sample size, whether to stop a trial early and the appropriate analysis and interpretation of the trial results. At each of these stages, evidence external to the trial is useful, but generally such evidence is introduced in an unstructured and informal manner. We argue that a Bayesian approach allows a formal basis for using external evidence and in addition provides a rational way for dealing with issues such as the ethics of randomization, trials to show treatment equivalence, the monitoring of accumulating data and the prediction of the consequences of continuing a study. The motivation for using this methodology is practical rather than ideological.},
	author = {Spiegelhalter, David J. and Freedman, Laurence S. and Parmar, Mahesh K. B.},
	doi = {https://doi.org/10.2307/2983527},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.2307/2983527},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	keywords = {clinical trials, ethics, power, prediction, prior distribution, range of equivalence, shrinkage, stopping rules, subjective probabilities},
	number = {3},
	pages = {357-387},
	title = {Bayesian Approaches to Randomized Trials},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2983527},
	volume = {157},
	year = {1994},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2983527},
	bdsk-url-2 = {https://doi.org/10.2307/2983527}}
@article{Freedman+1984,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2530902},
 abstract = {A method is described of eliciting a `range of equivalence', i.e. a range of differences between two treatments over which a group of clinical trial participants would have no clear preference for either treatment. This range of equivalence is then incorporated into a formal stopping rule for the trial using an extension of the group sequential design. Tables for the implementation of the design are presented. The method is discussed in the context of other sequential-trial designs.},
 author = {L. S. Freedman and D. Lowe and P. Macaskill},
 journal = {Biometrics},
 number = {3},
 pages = {575--586},
 publisher = {International Biometric Society},
 title = {Stopping Rules for Clinical Trials Incorporating Clinical Opinion},
 urldate = {2024-09-24},
 volume = {40},
 year = {1984}
}
@book{Cohen1988,
  author = {Jacob Cohen},
  year = {1988},
  title = {Statistical Power Analysis for the Behavioral Sciences },
  series = {},
  volume = {},
  edition = {2},
  url = {https://doi.org/10.4324/9780203771587},
  publisher = {Routledge}
}
@misc{Kim2024,
      title={Statistics in Survey Sampling}, 
      author={Jae Kwang Kim},
      year={2024},
      eprint={2401.07625},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2401.07625}, 
}
@article{Rubin1976,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2335739},
 abstract = {When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
 author = {Donald B. Rubin},
 journal = {Biometrika},
 number = {3},
 pages = {581--592},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Inference and Missing Data},
 urldate = {2024-09-24},
 volume = {63},
 year = {1976}
}

@article{Horvitz-Thompson1952,
	author = {D. G. Horvitz and D. J. Thompson},
	doi = {10.1080/01621459.1952.10483446},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1952.10483446},
	journal = {Journal of the American Statistical Association},
	number = {260},
	pages = {663--685},
	publisher = {ASA Website},
	title = {A Generalization of Sampling Without Replacement from a Finite Universe},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483446},
	volume = {47},
	year = {1952},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1952.10483446},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1952.10483446}}
@article{Sen1953,
  author = {A. R. Sen},
  year = {1953},
  title = {On the estimate of the variance in sampling with varying probabilities},
  journal = {Journal of the Indian Society of Agricultural Statistics},
  volume = {5},
  number = {},
  pages = {119-127},
  url = {}
}

@article{Yates-Grundy1953,
	abstract = {SUMMARY In selection with probability proportional to size x from within strata without replacement, the usual method of selection gives rise to bias in the estimate of the total of a variate y derived by weighting the units by weights proportional to 1/x. By means of numerical examples it is shown that the amount of this bias is usually quite trivial. If, however, unbiased estimates are required, the true total probabilities of selection of the different units can be calculated easily for samples of 2, and with considerably more labour for samples of 3. The bias in the ordinary formula for the estimation of error is also investigated, and the formula is shown to be reasonably accurate. Horvitz and Thompson have given an unbiased estimator of the error variance, but this is shown to be inefficient and a new unbiased estimator is given. A method of revising the size measures so that with the usual method of selection the true total probabilities of selection are proportional to the original size measures is given for samples of 2. Horvitz and Thompson's solution of this problem does not appear to give satisfactory approximations in the cases met with in practice. The selection of successive members of a sample with arbitrary sets of probabilities chosen solely so that the total probabilities shall be proportional to the original size measures, which has been advocated in various quarters, is criticized.},
	author = {Yates, F. and Grundy, P. M.},
	doi = {https://doi.org/10.1111/j.2517-6161.1953.tb00140.x},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1953.tb00140.x},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	number = {2},
	pages = {253-261},
	title = {Selection Without Replacement from Within Strata with Probability Proportional to Size},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1953.tb00140.x},
	volume = {15},
	year = {1953},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1953.tb00140.x},
	bdsk-url-2 = {https://doi.org/10.1111/j.2517-6161.1953.tb00140.x}}

@book{Sarndal+1992,
  author = {Carl-Erik Särndal and Bengt Swensson and Jan Wretman},
  year = {1992},
  title = {Model Assisted Survey Sampling},
  series = {},
  volume = {},
  edition = {},
  url = {https://link.springer.com/book/9780387406206},
  publisher = {Springer New York}
}

@article{Deville-Sarndal1992,
	author = {Jean-Claude Deville and Carl-Erik S{\"a}rndal},
	doi = {10.1080/01621459.1992.10475217},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1992.10475217},
	journal = {Journal of the American Statistical Association},
	number = {418},
	pages = {376--382},
	publisher = {ASA Website},
	title = {Calibration Estimators in Survey Sampling},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475217},
	volume = {87},
	year = {1992},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1992.10475217},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1992.10475217}}
@article{Gelman-Little1997,
  author = {Andrew Gelman and Thomas Little},
  year = {1997},
  title = {Poststratification into Many Categories using Hierarchical Logistic Regression},
  journal = {Survey Methodology},
  volume = {},
  number = {1997002},
  pages = {},
  url = {https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X19970023616}
}
@article{Rubin1978,
 ISSN = {00905364, 21688966},
 URL = {http://www.jstor.org/stable/2958688},
 abstract = {Causal effects are comparisons among values that would have been observed under all possible assignments of treatments to experimental units. In an experiment, one assignment of treatments is chosen and only the values under that assignment can be observed. Bayesian inference for causal effects follows from finding the predictive distribution of the values under the other assignments of treatments. This perspective makes clear the role of mechanisms that sample experimental units, assign treatments and record data. Unless these mechanisms are ignorable (known probabilistic functions of recorded values), the Bayesian must model them in the data analysis and, consequently, confront inferences for causal effects that are sensitive to the specification of the prior distribution of the data. Moreover, not all ignorable mechanisms can yield data from which inferences for causal effects are insensitive to prior specifications. Classical randomized designs stand out as especially appealing assignment mechanisms designed to make inference for causal effects straightforward by limiting the sensitivity of a valid Bayesian analysis.},
 author = {Donald B. Rubin},
 journal = {The Annals of Statistics},
 number = {1},
 pages = {34--58},
 publisher = {Institute of Mathematical Statistics},
 title = {Bayesian Inference for Causal Effects: The Role of Randomization},
 urldate = {2024-09-24},
 volume = {6},
 year = {1978}
}
@article{Rubin1978MI,
  author = {Donald B. Rubin},
  year = {1978},
  title = {Multiple Imputations in Sample Surveys - A Phenomenological Bayesian Approach to Nonresponse},
  journal = {Proceedings of the Survey Research Methods Section, ASA},
  volume = {},
  number = {},
  pages = {20-28},
  url = {http://www.asasrms.org/Proceedings/y1978f.html}
}
@article{Aitken1936, title={IV.—On Least Squares and Linear Combination of Observations}, volume={55}, DOI={10.1017/S0370164600014346}, journal={Proceedings of the Royal Society of Edinburgh}, author={Aitken, A. C.}, year={1936}, pages={42–48}}
@article{Zieschang1990,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2289595},
 abstract = {The widely used Principal Person method of weighting households in federal government surveys uses external post-Censal information on population to improve survey sample weights by a form of poststratification. While the Principal Person Methodology can be viewed as part of a procedure to adjust for nonresponse and undercoverage, it is not oriented for efficiently incorporating ancillary information or combining information from multiple surveys into survey estimates of subdomain totals. In this article a generalized least squares adjustment algorithm is shown to incorporate ancillary information in a way that, in principle, reduces the design variance of estimated survey totals. The flexibility of the method is exploited in an application to the Consumer Expenditure Survey that makes use of its "weighting control" and "composition" features.},
 author = {Kimberly D. Zieschang},
 journal = {Journal of the American Statistical Association},
 number = {412},
 pages = {986--1001},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Sample Weighting Methods and Estimation of Totals in the Consumer Expenditure Survey},
 urldate = {2024-09-24},
 volume = {85},
 year = {1990}
}

@article{Deville-Sarndal1993,
	author = {Jean-Claude Deville, Carl-Erik S{\"a}rndal and Olivier Sautory},
	doi = {10.1080/01621459.1993.10476369},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1993.10476369},
	journal = {Journal of the American Statistical Association},
	number = {423},
	pages = {1013--1020},
	publisher = {ASA Website},
	title = {Generalized Raking Procedures in Survey Sampling},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476369},
	volume = {88},
	year = {1993},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1993.10476369},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1993.10476369}}

@article{Firth-Bennett1998,
	abstract = {In the estimation of a population mean or total from a random sample, certain methods based on linear models are known to be automatically design consistent, regardless of how well the underlying model describes the population. A sufficient condition is identified for this type of robustness to model failure; the condition, which we call `internal bias calibration', relates to the combination of a model and the method used to fit it. Included among the internally bias-calibrated models, in addition to the aforementioned linear models, are certain canonical link generalized linear models and nonparametric regressions constructed from them by a particular style of local likelihood fitting. Other models can often be made robust by using a suboptimal fitting method. Thus the class of model-based, but design consistent, analyses is enlarged to include more realistic models for certain types of survey variable such as binary indicators and counts. Particular applications discussed are the estimation of the size of a population subdomain, as arises in tax auditing for example, and the estimation of a bootstrap tail probability.},
	author = {Firth, D. and Bennett, K. E.},
	doi = {https://doi.org/10.1111/1467-9868.00105},
	eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00105},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	keywords = {Auditing, Bias calibration, Bootstrap acceleration, Control variate, Finite population, Generalized linear model, Importance sampling, Instrumental variable, Local likelihood, Logistic regression, Smoothing, Spline, Stratification, Survey sampling},
	number = {1},
	pages = {3-21},
	title = {Robust models in probability sampling},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00105},
	volume = {60},
	year = {1998},
	bdsk-url-1 = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00105},
	bdsk-url-2 = {https://doi.org/10.1111/1467-9868.00105}}
@article{Godambe-Joshi1965,
 ISSN = {00034851},
 URL = {http://www.jstor.org/stable/2239112},
 author = {V. P. Godambe and V. M. Joshi},
 journal = {The Annals of Mathematical Statistics},
 number = {6},
 pages = {1707--1722},
 publisher = {Institute of Mathematical Statistics},
 title = {Admissibility and Bayes Estimation in Sampling Finite Populations. I},
 urldate = {2024-09-24},
 volume = {36},
 year = {1965}
}

@article{Isaki-Fuller1982,
	author = {Cary T. Isaki and Wayne A. Fuller},
	doi = {10.1080/01621459.1982.10477770},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1982.10477770},
	journal = {Journal of the American Statistical Association},
	number = {377},
	pages = {89--96},
	publisher = {ASA Website},
	title = {Survey Design under the Regression Superpopulation Model},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1982.10477770},
	volume = {77},
	year = {1982},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1982.10477770},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1982.10477770}}

@article{Wu-Sitter2001,
	author = {Changbao Wu and Randy R Sitter},
	doi = {10.1198/016214501750333054},
	eprint = {https://doi.org/10.1198/016214501750333054},
	journal = {Journal of the American Statistical Association},
	number = {453},
	pages = {185--193},
	publisher = {ASA Website},
	title = {A Model-Calibration Approach to Using Complete Auxiliary Information From Survey Data},
	url = {https://doi.org/10.1198/016214501750333054},
	volume = {96},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1198/016214501750333054}}
@article{Kim-Haziza2014,
 ISSN = {10170405, 19968507},
 URL = {http://www.jstor.org/stable/26432548},
 abstract = {Statistical inference with missing data requires assumptions about the population or about the response probability. Doubly robust (DR) estimators use both relationships to estimate the parameters of interest, so that they are consistent even when one of the models is misspecified. In this paper, we propose a method of computing propensity scores that leads to DR estimation. In addition, we discuss DR variance estimation so that the resulting inference is doubly robust. Some asymptotic properties are discussed. Results from two limited simulation studies are also presented.},
 author = {Jae Kwang Kim and David Haziza},
 journal = {Statistica Sinica},
 number = {1},
 pages = {375--394},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {DOUBLY ROBUST INFERENCE WITH MISSING DATA IN SURVEY SAMPLING},
 urldate = {2024-09-24},
 volume = {24},
 year = {2014}
}
@article{Berg+2016,
    author = {Berg, Emily and Kim, Jae-Kwang and Skinner, Chris},
    title = "{Imputation Under Informative Sampling}",
    journal = {Journal of Survey Statistics and Methodology},
    volume = {4},
    number = {4},
    pages = {436-462},
    year = {2016},
    month = {12},
    abstract = "{Imputed values in surveys are often generated under the assumption that the sampling mechanism is non-informative (or ignorable) and the study variable is missing at random (MAR). When the sampling design is informative, the assumption of MAR in the population does not necessarily imply MAR in the sample. In this case, the classical method of imputation using a model fitted to the sample data does not in general lead to unbiased estimation. To overcome this problem, we consider alternative approaches to imputation assuming MAR in the population. We compare the alternative imputation procedures through simulation and an application to estimation of mean erosion using data from the Conservation Effects Assessment Project.}",
    issn = {2325-0984},
    doi = {10.1093/jssam/smw032},
    url = {https://doi.org/10.1093/jssam/smw032},
    eprint = {https://academic.oup.com/jssam/article-pdf/4/4/436/8721804/smw032.pdf},
}
@article{Sugden-Smith1984,
    author = {Sugden, R. A. and Smith, T. M. F.},
    title = "{Ignorable and informative designs in survey sampling inference}",
    journal = {Biometrika},
    volume = {71},
    number = {3},
    pages = {495-506},
    year = {1984},
    month = {12},
    abstract = "{The role of the sample selection mechanism in a model-based approach to finite population inference is examined. When the data analyst has only partial information on the sample design then a design which is ignorable when known fully may become informative. Conditions under which partially known designs can be ignored are established and examined for some standard designs. The results are illustrated by an example used by Scott (1977).}",
    issn = {0006-3444},
    doi = {10.1093/biomet/71.3.495},
    url = {https://doi.org/10.1093/biomet/71.3.495},
    eprint = {https://academic.oup.com/biomet/article-pdf/71/3/495/718101/71-3-495.pdf},
}

@article{Little1992,
	author = {Roderick J. A. Little},
	doi = {10.1080/01621459.1992.10476282},
	eprint = {https://doi.org/10.1080/01621459.1992.10476282},
	journal = {Journal of the American Statistical Association},
	number = {420},
	pages = {1227--1237},
	publisher = {ASA Website},
	title = {Regression with Missing X's: A Review},
	url = {https://doi.org/10.1080/01621459.1992.10476282},
	volume = {87},
	year = {1992},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.1992.10476282}}
@article{Enders+2001,
author = {Craig K. Enders and Deborah L. Bandalos},
title = {The Relative Performance of Full Information Maximum Likelihood Estimation for Missing Data in Structural Equation Models},
journal = {Structural Equation Modeling: A Multidisciplinary Journal},
volume = {8},
number = {3},
pages = {430--457},
year = {2001},
publisher = {Routledge},
doi = {10.1207/S15328007SEM0803\_5},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1207/S15328007SEM0803_5
    

}}


@article{Kalton-Kish1984,
	author = {Graham Kalton and Leslie Kish},
	doi = {10.1080/03610928408828805},
	eprint = {https://doi.org/10.1080/03610928408828805},
	journal = {Communications in Statistics - Theory and Methods},
	number = {16},
	pages = {1919--1939},
	publisher = {Taylor \& Francis},
	title = {Some efficient random imputation methods},
	url = {https://doi.org/10.1080/03610928408828805},
	volume = {13},
	year = {1984},
	bdsk-url-1 = {https://doi.org/10.1080/03610928408828805}}
@article{Kim-Fuller2004,
    author = {Kim, Jae Kwang and Fuller, Wayne},
    title = "{Fractional hot deck imputation}",
    journal = {Biometrika},
    volume = {91},
    number = {3},
    pages = {559-578},
    year = {2004},
    month = {09},
    abstract = "{To compensate for item nonresponse, hot deck imputation procedures replace missing values with values that occur in the sample. Fractional hot deck imputation replaces each missing observation with a set of imputed values and assigns a weight to each imputed value. Under the model in which observations in an imputation cell are independently and identically distributed, fractional hot deck imputation is shown to be an effective imputation procedure. A consistent replication variance estimation procedure for estimators computed with fractional imputation is suggested. Simulations show that fractional imputation and the suggested variance estimator are superior to multiple imputation estimators in general, and much superior to multiple imputation for estimating the variance of a domain mean.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/91.3.559},
    url = {https://doi.org/10.1093/biomet/91.3.559},
    eprint = {https://academic.oup.com/biomet/article-pdf/91/3/559/623932/910559.pdf},
}
@article{Enders+2020,
  author = {C. K. Enders and H. Du and B. T. Keller},
  year = {2020},
  title = {{A Model-based Imputation Procedure for Multilevel Regression Models with Random Coefficients, Interaction Effects, and Nonlinear Terms}},
  journal = {Psychological Methods},
  volume = {25},
  number = {1},
  pages = {88-112},
  url = {https://psycnet.apa.org/doi/10.1037/met0000228}
}


@book{vanBuuren2018,
 abstract = {},
 address = {Boca Raton, FL.},
 author = {van Buuren, Stef},
 editor = {},
 keywords = {},
 pages = {},
 publisher = {CRC Press},
 title = {{Flexible Imputation of Missing Data}},
 volume = {},
 edition = {2},
 year = {2018},
 url = {https://stefvanbuuren.name/fimd/},
}

@article{Royston-White2011,
 title={Multiple Imputation by Chained Equations (MICE): Implementation in Stata},
 volume={45},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v045i04},
 doi={10.18637/jss.v045.i04},
 abstract={Missing data are a common occurrence in real datasets. For epidemiological and prognostic factors studies in medicine, multiple imputation is becoming the standard route to estimating models with missing covariate data under a missing-at-random assumption. We describe &amp;lt;b&amp;gt;ice&amp;lt;/b&amp;gt;, an implementation in Stata of the MICE approach to multiple imputation. Real data from an observational study in ovarian cancer are used to illustrate the most important of the many options available with &amp;lt;b&amp;gt;ice&amp;lt;/b&amp;gt;. We remark briefly on the new database architecture and procedures for multiple imputation introduced in releases 11 and 12 of Stata.},
 number={4},
 journal={Journal of Statistical Software},
 author={Royston, Patrick and White, Ian R.},
 year={2011},
 pages={1–20}
}

@article{vanBuuren+2006,
	author = {Stef Van Buuren, J. P.L. Brand, C. G.M. Groothuis-Oudshoorn and D. B. Rubin},
	doi = {10.1080/10629360600810434},
	eprint = {https://doi.org/10.1080/10629360600810434},
	journal = {Journal of Statistical Computation and Simulation},
	number = {12},
	pages = {1049--1064},
	publisher = {Taylor \& Francis},
	title = {Fully conditional specification in multivariate imputation},
	url = {https://doi.org/10.1080/10629360600810434},
	volume = {76},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1080/10629360600810434}}
@article{vanBuuren-Groothuis-Oudshoorn2011,
 title={mice: Multivariate Imputation by Chained Equations in R},
 volume={45},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v045i03},
 doi={10.18637/jss.v045.i03},
 abstract={The R package &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; imputes incomplete multivariate data by chained equations. The software mice 1.0 appeared in the year 2000 as an S-PLUS library, and in 2001 as an R package. mice 1.0 introduced predictor selection, passive imputation and automatic pooling. This article documents mice, which extends the functionality of mice 1.0 in several ways. In &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt;, the analysis of imputed data is made completely general, whereas the range of models under which pooling works is substantially extended. &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; adds new functionality for imputing multilevel data, automatic predictor selection, data handling, post-processing imputed values, specialized pooling routines, model selection tools, and diagnostic graphs. Imputation of categorical data is improved in order to bypass problems caused by perfect prediction. Special attention is paid to transformations, sum scores, indices and interactions using passive imputation, and to the proper setup of the predictor matrix. &amp;lt;b&amp;gt;mice&amp;lt;/b&amp;gt; can be downloaded from the Comprehensive R Archive Network. This article provides a hands-on, stepwise approach to solve applied incomplete data problems.},
 number={3},
 journal={Journal of Statistical Software},
 author={van Buuren, Stef and Groothuis-Oudshoorn, Karin},
 year={2011},
 pages={1–67}
}

@article{Rezvan+2015,
	abstract = {Missing data are common in medical research, which can lead to a loss in statistical power and potentially biased results if not handled appropriately. Multiple imputation (MI) is a statistical method, widely adopted in practice, for dealing with missing data. Many academic journals now emphasise the importance of reporting information regarding missing data and proposed guidelines for documenting the application of MI have been published. This review evaluated the reporting of missing data, the application of MI including the details provided regarding the imputation model, and the frequency of sensitivity analyses within the MI framework in medical research articles.},
	author = {Hayati Rezvan, Panteha and Lee, Katherine J. and Simpson, Julie A.},
	date = {2015/04/07},
	date-added = {2024-09-25 15:49:14 +0900},
	date-modified = {2024-09-25 15:49:14 +0900},
	doi = {10.1186/s12874-015-0022-1},
	id = {Hayati Rezvan2015},
	isbn = {1471-2288},
	journal = {BMC Medical Research Methodology},
	number = {1},
	pages = {30},
	title = {The rise of multiple imputation: a review of the reporting and implementation of the method in medical research},
	url = {https://doi.org/10.1186/s12874-015-0022-1},
	volume = {15},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1186/s12874-015-0022-1}}

@article{野間久史2017,
	author = {野間 久史},
	doi = {10.5023/jappstat.46.67},
	journal = {応用統計学},
	number = {2},
	pages = {67-86},
	title = {連鎖方程式による多重代入法},
	volume = {46},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.5023/jappstat.46.67}}

@article{Meng1994,
	author = {Xiao-Li Meng},
	doi = {10.1214/ss/1177010269},
	journal = {Statistical Science},
	keywords = {Congeniality, importance sampling, incomplete data, missing data, nonresponse, Normalizing constants, public-use data file, Randomization, self-efficiency},
	number = {4},
	pages = {538 -- 558},
	publisher = {Institute of Mathematical Statistics},
	title = {{Multiple-Imputation Inferences with Uncongenial Sources of Input}},
	url = {https://doi.org/10.1214/ss/1177010269},
	volume = {9},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1214/ss/1177010269}}
@article{逸見昌之2014,
  author = {逸見昌之},
  year = {2014},
  title = {欠測データに対するセミパラメトリックな解析法――その理論的背景について――},
  journal = {統計数理},
  volume = {62},
  number = {1},
  pages = {103-122},
  url = {https://www.ism.ac.jp/editsec/toukei/pdf/62-1-103.pdf}
}
@article{Baker+2013,
    author = {Baker, Reg and Brick, J. Michael and Bates, Nancy A. and Battaglia, Mike and Couper, Mick P. and Dever, Jill A. and Gile, Krista J. and Tourangeau, Roger},
    title = "{Summary Report of the AAPOR Task Force on Non-probability Sampling}",
    journal = {Journal of Survey Statistics and Methodology},
    volume = {1},
    number = {2},
    pages = {90-143},
    year = {2013},
    month = {09},
    issn = {2325-0984},
    doi = {10.1093/jssam/smt008},
    url = {https://doi.org/10.1093/jssam/smt008},
    eprint = {https://academic.oup.com/jssam/article-pdf/1/2/90/2774724/smt008.pdf},
}
@techreport{AAOR2013,
  author = {AAOR},
  institution = {American Association for Public Opinion Research},
  title = {Report of the AAPOR Task Force on Non-Probability Sampling},
  year = {2013}
}

@article{Morikawa-Kim2021,
	author = {Kosuke Morikawa and Jae Kwang Kim},
	doi = {10.1214/21-AOS2070},
	journal = {The Annals of Statistics},
	keywords = {Estimating functions, Identification, incomplete data, not missing at random (NMAR), semiparametric efficient estimation},
	number = {5},
	pages = {2991 -- 3014},
	publisher = {Institute of Mathematical Statistics},
	title = {{Semiparametric optimal estimation with nonignorable nonresponse data}},
	url = {https://doi.org/10.1214/21-AOS2070},
	volume = {49},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/21-AOS2070}}
@article{Elliott-Valliant2017,
 ISSN = {08834237, 21688745},
 URL = {http://www.jstor.org/stable/26408228},
 abstract = {Although selecting a probability sample has been the standard for decades when making inferences from a sample to a finite population, incentives are increasing to use nonprobability samples. In a world of "big data", large amounts of data are available that are faster and easier to collect than are probability samples. Design-based inference, in which the distribution for inference is generated by the random mechanism used by the sampler, cannot be used for nonprobability samples. One alternative is quasi-randomization in which pseudo-inclusion probabilities are estimated based on covariates available for samples and nonsample units. Another is superpopulation modeling for the analytic variables collected on the sample units in which the model is used to predict values for the nonsample units. We discuss the pros and cons of each approach.},
 author = {Michael R. Elliott and Richard Valliant},
 journal = {Statistical Science},
 number = {2},
 pages = {249--264},
 publisher = {Institute of Mathematical Statistics},
 title = {Inference for Nonprobability Samples},
 urldate = {2024-09-25},
 volume = {32},
 year = {2017}
}
@article{Dorie+2019,
 ISSN = {08834237, 21688745},
 URL = {https://www.jstor.org/stable/26771031},
 abstract = {Statisticians have made great progress in creating methods that reduce our reliance on parametric assumptions. However, this explosion in research has resulted in a breadth of inferential strategies that both create opportunities for more reliable inference as well as complicate the choices that an applied researcher has to make and defend. Relatedly, researchers advocating for new methods typically compare their method to at best 2 or 3 other causal inference strategies and test using simulations that may or may not be designed to equally tease out flaws in all the competing methods. The causal inference data analysis challenge, "Is Your SATT Where It's At?", launched as part of the 2016 Atlantic Causal Inference Conference, sought to make progress with respect to both of these issues. The researchers creating the data testing grounds were distinct from the researchers submitting methods whose efficacy would be evaluated. Results from 30 competitors across the two versions of the competition (black-box algorithms and do-it-yourself analyses) are presented along with post-hoc analyses that reveal information about the characteristics of causal inference strategies and settings that affect performance. The most consistent conclusion was that methods that flexibly model the response surface perform better overall than methods that fail to do so. Finally new methods are proposed that combine features of several of the top-performing submitted methods.},
 author = {Vincent Dorie and Jennifer Hill and Uri Shalit and Marc Scott and Dan Cervone},
 journal = {Statistical Science},
 number = {1},
 pages = {pp. 43--68},
 publisher = {Institute of Mathematical Statistics},
 title = {Automated versus Do-It-Yourself Methods for Causal Inference: Lessons Learned from a Data Analysis Competition},
 urldate = {2024-09-25},
 volume = {34},
 year = {2019}
}

@article{Kim-Tam2021,
	abstract = {Summary The statistical challenges in using big data for making valid statistical inference in the finite population have been well documented in literature. These challenges are due primarily to statistical bias arising from under-coverage in the big data source to represent the population of interest and measurement errors in the variables available in the data set. By stratifying the population into a big data stratum and a missing data stratum, we can estimate the missing data stratum by using a fully responding probability sample and hence the population as a whole by using a data integration estimator. By expressing the data integration estimator as a regression estimator, we can handle measurement errors in the variables in big data and also in the probability sample. We also propose a fully nonparametric classification method for identifying the overlapping units and develop a bias-corrected data integration estimator under misclassification errors. Finally, we develop a two-step regression data integration estimator to deal with measurement errors in the probability sample. An advantage of the approach advocated in this paper is that we do not have to make unrealistic missing-at-random assumptions for the methods to work. The proposed method is applied to the real data example using 2015--2016 Australian Agricultural Census data.},
	author = {Kim, Jae-Kwang and Tam, Siu-Ming},
	doi = {https://doi.org/10.1111/insr.12434},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/insr.12434},
	journal = {International Statistical Review},
	keywords = {Calibration weighting, Measurement error, Non-response, Regression estimation, Selection bias},
	number = {2},
	pages = {382-401},
	title = {Data Integration by Combining Big Data and Survey Sample Data for Finite Population Inference},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12434},
	volume = {89},
	year = {2021},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12434},
	bdsk-url-2 = {https://doi.org/10.1111/insr.12434}}
@article{key,
  
  journal = {Proceedings of Social Statistics Section},
  volume = {},
  number = {},
  pages = {203-206},
  url = {}
}
@inproceedings{Hartley1962,
  author = {H. O. Hartley},
  year = {1962},
  title = {Multiple Frame Surveys},
  booktitle = {Proceedings of Social Statistics Section},
  volume = {},
  pages = {203-206},
  url = {}
}

@article{Skinner-Rao1996,
	author = {C. J. Skinner and J. N. K. Rao},
	doi = {10.1080/01621459.1996.10476695},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1996.10476695},
	journal = {Journal of the American Statistical Association},
	number = {433},
	pages = {349--356},
	publisher = {ASA Website},
	title = {Estimation in Dual Frame Surveys with Complex Designs},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476695},
	volume = {91},
	year = {1996},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1996.10476695},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1996.10476695}}

@article{Meng2018,
	author = {Xiao-Li Meng},
	doi = {10.1214/18-AOAS1161SF},
	journal = {The Annals of Applied Statistics},
	keywords = {Bias-variance tradeoff, data confidentiality and privacy, data defect correlation, data defect index (d.d.i.), data quality-quantity tradeoff, Euler identity, Monte Carlo and Quasi Monte Carlo (MCQMC), non-response bias},
	number = {2},
	pages = {685 -- 726},
	publisher = {Institute of Mathematical Statistics},
	title = {{Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election}},
	url = {https://doi.org/10.1214/18-AOAS1161SF},
	volume = {12},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1214/18-AOAS1161SF}}

@techreport{Athey+2019,
  author = {Susan Athey and Raj Chetty and Guido W. Imbens and Hyunseung Kang},
  institution = {National Bureau of Economic Research},
  title = {The Surrogate Index: Combining Short-Term Proxies to Estimate Long-Term Treatment Effects More Rapidly and Precisely},
  year = {2019},
  doi = {10.3386/w26463},
}
@misc{Athey+2020,
      title={Combining Experimental and Observational Data to Estimate Treatment Effects on Long Term Outcomes}, 
      author={Susan Athey and Raj Chetty and Guido Imbens},
      year={2020},
      eprint={2006.09676},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2006.09676}, 
}
@misc{Park-Sasaki2024,
      title={The Informativeness of Combined Experimental and Observational Data under Dynamic Selection}, 
      author={Yechan Park and Yuya Sasaki},
      year={2024},
      eprint={2403.16177},
      archivePrefix={arXiv},
      primaryClass={econ.EM},
      url={https://arxiv.org/abs/2403.16177}, 
}

@article{Lohr-Raghunathan2017,
	author = {Sharon L. Lohr and Trivellore E. Raghunathan},
	doi = {10.1214/16-STS584},
	journal = {Statistical Science},
	keywords = {hierarchical models, imputation, multiple frame survey, probability sample, record linkage, small area estimation},
	number = {2},
	pages = {293 -- 312},
	publisher = {Institute of Mathematical Statistics},
	title = {{Combining Survey Data with Other Data Sources}},
	url = {https://doi.org/10.1214/16-STS584},
	volume = {32},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1214/16-STS584}}
@article{Hand2018,
    author = {Hand, David J.},
    title = "{Statistical Challenges of Administrative and Transaction Data}",
    journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
    volume = {181},
    number = {3},
    pages = {555-605},
    year = {2018},
    month = {02},
    abstract = "{Administrative data are becoming increasingly important. They are typically the side effect of some operational exercise and are often seen as having significant advantages over alternative sources of data. Although it is true that such data have merits, statisticians should approach the analysis of such data with the same cautious and critical eye as they approach the analysis of data from any other source. The paper identifies some statistical challenges, with the aim of stimulating debate about and improving the analysis of administrative data, and encouraging methodology researchers to explore some of the important statistical problems which arise with such data.}",
    issn = {0964-1998},
    doi = {10.1111/rssa.12315},
    url = {https://doi.org/10.1111/rssa.12315},
    eprint = {https://academic.oup.com/jrsssa/article-pdf/181/3/555/49449402/jrsssa\_181\_3\_555.pdf},
}

@article{Rao2021,
	abstract = {Survey samplers have long been using probability samples from one or more sources in conjunction with census and administrative data to make valid and efficient inferences on finite population parameters. This topic has received a lot of attention more recently in the context of data from non-probability samples such as transaction data, web surveys and social media data. In this paper, I will provide a brief overview of probability sampling methods first and then discuss some recent methods, based on models for the non-probability samples, which could lead to useful inferences from a non-probability sample by itself or when combined with a probability sample. I will also explain how big data may be used as predictors in small area estimation, a topic of current interest because of the growing demand for reliable local area statistics.},
	author = {Rao, J.  N.  K. },
	date = {2021/05/01},
	date-added = {2024-09-27 13:37:43 +0900},
	date-modified = {2024-09-27 13:37:43 +0900},
	doi = {10.1007/s13571-020-00227-w},
	id = {Rao2021},
	isbn = {0976-8394},
	journal = {Sankhya B},
	number = {1},
	pages = {242--272},
	title = {On Making Valid Inferences by Integrating Data from Surveys and Other Sources},
	url = {https://doi.org/10.1007/s13571-020-00227-w},
	volume = {83},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1007/s13571-020-00227-w}}

@article{Golini-Righi2024,
	abstract = {This paper introduces the pseudo-calibration estimators, a novel method that integrates a non-probability sample of big size with a probability sample, assuming both samples contain relevant information for estimating the population parameter. The proposed estimators share a structural similarity with the adjusted projection estimators and the difference estimators but they adopt a different inferential approach and informative setup. The pseudo-calibration estimators can be employed when the target variable is observed in the probability sample and, in the non-probability sample, it is observed correctly, observed with error, or predicted. This paper also introduces an original application of the jackknife-type method for variance estimation. A simulation study shows that the proposed estimators are robust and efficient compared to the regression data integration estimators that use the same informative setup. Finally, a further evaluation using real data is carried out.},
	author = {Golini, Natalia and Righi, Paolo},
	date = {2024/04/01},
	date-added = {2024-09-27 13:38:28 +0900},
	date-modified = {2024-09-27 13:38:28 +0900},
	doi = {10.1007/s10260-023-00740-y},
	id = {Golini2024},
	isbn = {1613-981X},
	journal = {Statistical Methods \& Applications},
	number = {2},
	pages = {555--580},
	title = {Integrating probability and big non-probability samples data to produce Official Statistics},
	url = {https://doi.org/10.1007/s10260-023-00740-y},
	volume = {33},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s10260-023-00740-y}}
@article{Salvatore2024,
    author = {Salvatore, Camilla and Biffignandi, Silvia and Sakshaug, Joseph W and Wiśniowski, Arkadiusz and Struminskaya, Bella},
    title = "{Bayesian Integration of Probability and Nonprobability Samples for Logistic Regression}",
    journal = {Journal of Survey Statistics and Methodology},
    volume = {12},
    number = {2},
    pages = {458-492},
    year = {2024},
    month = {11},
    abstract = "{Probability sample (PS) surveys are considered the gold standard for population-based inference but face many challenges due to decreasing response rates, relatively small sample sizes, and increasing costs. In contrast, the use of nonprobability sample (NPS) surveys has increased significantly due to their convenience, large sample sizes, and relatively low costs, but they are susceptible to large selection biases and unknown selection mechanisms. Integrating both sample types in a way that exploits their strengths and overcomes their weaknesses is an ongoing area of methodological research. We build on previous work by proposing a method of supplementing PSs with NPSs to improve analytic inference for logistic regression coefficients and potentially reduce survey costs. Specifically, we use a Bayesian framework for inference. Inference relies on a probability survey with a small sample size, and through the prior structure we incorporate supplementary auxiliary information from a less-expensive (but potentially biased) NPS survey fielded in parallel. The performance of several strongly informative priors constructed from the NPS information is evaluated through a simulation study and real-data application. Overall, the proposed priors reduce the mean-squared error (MSE) of regression coefficients or, in the worst case, perform similarly to a weakly informative (baseline) prior that does not utilize any nonprobability information. Potential cost savings (of up to 68 percent) are evident compared to a probability-only sampling design with the same MSE for different informative priors under different sample sizes and cost scenarios. The algorithm, detailed results, and interactive cost analysis are provided through a Shiny web app as guidance for survey practitioners.}",
    issn = {2325-0992},
    doi = {10.1093/jssam/smad041},
    url = {https://doi.org/10.1093/jssam/smad041},
    eprint = {https://academic.oup.com/jssam/article-pdf/12/2/458/57170654/smad041.pdf},
}

@article{Angelopoulos2023,
	abstract = {Prediction-powered inference is a framework for performing valid statistical inference when an experimental dataset is supplemented with predictions from a machine-learning system. The framework yields simple algorithms for computing provably valid confidence intervals for quantities such as means, quantiles, and linear and logistic regression coefficients without making any assumptions about the machine-learning algorithm that supplies the predictions. Furthermore, more accurate predictions translate to smaller confidence intervals. Prediction-powered inference could enable researchers to draw valid and more data-efficient conclusions using machine learning. The benefits of prediction-powered inference were demonstrated with datasets from proteomics, astronomy, genomics, remote sensing, census analysis, and ecology. Over the past decade, there has been rapid progress in the development of large-scale machine learning (ML) systems that provide predictions related to various scientific phenomena. Unfortunately, the standard statistical approaches used to calculate confidence intervals and P values from gold standard data lose their statistical validity for ML-derived data. Angelopoulos et al. introduced ``prediction-powered inference,'' a standardized protocol for constructing valid confidence intervals and P values that enables the power and scale of ML systems to be used as predictors while ensuring responsible and reliable scientific inferences. The method has been demonstrated on a broad range of real datasets and offers a promising statistical approach for using ML to derive scientific conclusions responsibly. ---Yury Suleymanov A statistical protocol for valid scientific discovery using machine learning is presented.},
	author = {Anastasios N. Angelopoulos and Stephen Bates and Clara Fannjiang and Michael I. Jordan and Tijana Zrnic},
	doi = {10.1126/science.adi6000},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.adi6000},
	journal = {Science},
	number = {6671},
	pages = {669-674},
	title = {Prediction-powered inference},
	url = {https://www.science.org/doi/abs/10.1126/science.adi6000},
	volume = {382},
	year = {2023},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.adi6000},
	bdsk-url-2 = {https://doi.org/10.1126/science.adi6000}}
@article{Robbins+2020,
    author = {Robbins, Michael W and Ghosh-Dastidar, Bonnie and Ramchand, Rajeev},
    title = "{Blending Probability and Nonprobability Samples with Applications to a Survey of Military Caregivers}",
    journal = {Journal of Survey Statistics and Methodology},
    volume = {9},
    number = {5},
    pages = {1114-1145},
    year = {2020},
    month = {12},
    abstract = "{Probability samples are the preferred method for providing inferences that are generalizable to a larger population. However, in many cases, this approach is unlikely to yield a sample size large enough to produce precise inferences. Our goal here is to improve the efficiency of inferences from a probability sample by combining (or blending) it with a nonprobability sample, which is (by itself) potentially fraught with selection biases that would compromise the generalizability of results. We develop novel methods of statistical weighting that may be used for this purpose. Specifically, we make a distinction between weights that can be used to make the two samples representative of the population individually (disjoint blending) and those that make only the combined sample representative (simultaneous blending). Our focus is on weights constructed using propensity scores, but consideration is also given to calibration weighting. We include simulation studies that, among other illustrations, show the gain in precision provided by the convenience sample is lower in circumstances where the outcome is strongly related to the auxiliary variables used to align the samples. Motivating the exposition is a survey of military caregivers; our interest is focused on unpaid caregivers of wounded, ill, or injured US servicemembers and veterans who served following September 11, 2001. Our work serves not only to illustrate the proper execution of blending but also to caution the reader with respect to its dangers, as invoking a nonprobability sample may not yield substantial improvements in precision when assumptions are valid and may induce biases in the event that they are not.}",
    issn = {2325-0984},
    doi = {10.1093/jssam/smaa037},
    url = {https://doi.org/10.1093/jssam/smaa037},
    eprint = {https://academic.oup.com/jssam/article-pdf/9/5/1114/41727173/smaa037.pdf},
}
@article{Beaumont-Rao2021,
  author = {Jean-Francois Beaumont and J. N. K. Rao},
  year = {2021},
  title = {{Pitfalls of Making Inference from Non-probability Samples: Can Data Integration through Probability Samples Provide Remedies?}},
  journal = {The Survey Statistician},
  volume = {83},
  number = {},
  pages = {11-22},
  url = {https://isi-iass.org/home/wp-content/uploads/Survey_Statistician_2021_January_N83_02.pdf}
}
@article{Kim+2021,
    author = {Kim, Jae Kwang and Park, Seho and Chen, Yilin and Wu, Changbao},
    title = "{Combining Non-Probability and Probability Survey Samples Through Mass Imputation}",
    journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
    volume = {184},
    number = {3},
    pages = {941-963},
    year = {2021},
    month = {07},
    abstract = "{Analysis of non-probability survey samples requires auxiliary information at the population level. Such information may also be obtained from an existing probability survey sample from the same finite population. Mass imputation has been used in practice for combining non-probability and probability survey samples and making inferences on the parameters of interest using the information collected only in the non-probability sample for the study variables. Under the assumption that the conditional mean function from the non-probability sample can be transported to the probability sample, we establish the consistency of the mass imputation estimator and derive its asymptotic variance formula. Variance estimators are developed using either linearization or bootstrap. Finite sample performances of the mass imputation estimator are investigated through simulation studies. We also address important practical issues of the method through the analysis of a real-world non-probability survey sample collected by the Pew Research Centre.}",
    issn = {0964-1998},
    doi = {10.1111/rssa.12696},
    url = {https://doi.org/10.1111/rssa.12696},
    eprint = {https://academic.oup.com/jrsssa/article-pdf/184/3/941/49411783/jrsssa\_184\_3\_941.pdf},
}
@misc{Beresovsky+2024,
      title={Review of Quasi-Randomization Approaches for Estimation from Non-probability Samples}, 
      author={Vladislav Beresovsky and Julie Gershunskaya and Terrance D. Savitsky},
      year={2024},
      eprint={2312.05383},
      archivePrefix={arXiv},
      primaryClass={stat.AP},
      url={https://arxiv.org/abs/2312.05383}, 
}
@article{Elliot2009,
	author = {Elliot, Michael R},
	journal = {Survey Practice},
	doi = {10.29115/SP-2009-0025},
	number = {6},
	year = {2009},
	month = {sep 1},
	title = {Combining {Data} from {Probability} and {Non}- {Probability} {Samples} {Using} {Pseudo}-{Weights}},
	volume = {2},
}


@article{Shimodaira2000,
	abstract = {A class of predictive densities is derived by weighting the observed samples in maximizing the log-likelihood function. This approach is effective in cases such as sample surveys or design of experiments, where the observed covariate follows a different distribution than that in the whole population. Under misspecification of the parametric model, the optimal choice of the weight function is asymptotically shown to be the ratio of the density function of the covariate in the population to that in the observations. This is the pseudo-maximum likelihood estimation of sample surveys. The optimality is defined by the expected Kullback--Leibler loss, and the optimal weight is obtained by considering the importance sampling identity. Under correct specification of the model, however, the ordinary maximum likelihood estimate (i.e. the uniform weight) is shown to be optimal asymptotically. For moderate sample size, the situation is in between the two extreme cases, and the weight function is selected by minimizing a variant of the information criterion derived as an estimate of the expected loss. The method is also applied to a weighted version of the Bayesian predictive density. Numerical examples as well as Monte-Carlo simulations are shown for polynomial regression. A connection with the robust parametric estimation is discussed.},
	author = {Hidetoshi Shimodaira},
	doi = {https://doi.org/10.1016/S0378-3758(00)00115-4},
	issn = {0378-3758},
	journal = {Journal of Statistical Planning and Inference},
	keywords = {Akaike information criterion, Design of experiments, Importance sampling, Kullback--Leibler divergence, Misspecification, Sample surveys, Weighted least squares},
	number = {2},
	pages = {227-244},
	title = {Improving predictive inference under covariate shift by weighting the log-likelihood function},
	url = {https://www.sciencedirect.com/science/article/pii/S0378375800001154},
	volume = {90},
	year = {2000},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0378375800001154},
	bdsk-url-2 = {https://doi.org/10.1016/S0378-3758(00)00115-4}}
@article{Imai-Ratkovic2014,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/24772753},
 abstract = {The propensity score plays a central role in a variety of causal inference settings. In particular, matching and weighting methods based on the estimated propensity score have become increasingly common in the analysis of observational data. Despite their popularity and theoretical appeal, the main practical difficulty of these methods is that the propensity score must be estimated. Researchers have found that slight misspecification of the propensity score model can result in substantial bias of estimated treatment effects. We introduce covariate balancing propensity score (CBPS) methodology, which models treatment assignment while optimizing the covariate balance. The CBPS exploits the dual characteristics of the propensity score as a covariate balancing score and the conditional probability of treatment assignment. The estimation of the CBPS is done within the generalized method-of-moments or empirical likelihood framework. We find that the CBPS dramatically improves the poor empirical performance of propensity score matching and weighting methods reported in the literature. We also show that the CBPS can be extended to other important settings, including the estimation of the generalized propensity score for non-binary treatments and the generalization of experimental estimates to a target population. Open source software is available for implementing the methods proposed.},
 author = {Kosuke Imai and Marc Ratkovic},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {1},
 pages = {243--263},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Covariate balancing propensity score},
 urldate = {2024-09-27},
 volume = {76},
 year = {2014}
}
@article{Deng-Wu1987,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2289466},
 abstract = {The regression estimator and the ratio estimator are commonly used in survey practice. In the past more attention has been given to the ratio estimator because of its computational ease and applicability for general sampling designs. The ratio estimator is appropriate for populations whose regression line passes close to the origin. If the intercept of the regression line is significantly nonzero, however, it is much less efficient than the regression estimator (Deng 1984). In general, apart from n-2 terms, the mean squared error (MSE) of the former is bigger than that of the latter (Cochran 1977, p. 196). Given the present computing capacity, the computational advantage of the ratio estimator should be less of a concern and the regression estimator will gain wider popularity. The main purpose of this article is to provide a theoretical and empirical comparison of several variance estimators for the regression estimator in simple random sampling without replacement. The companion problem for the ratio estimator has been studied in the literature (see Wu and Deng 1983). Under comparison are several design-based and model-based estimators and a new class of estimators. Their second-order expressions and biases are derived and compared. Empirical results on the biases and MSE's of the variance estimators and the conditional and unconditional coverage probabilities of their associated t intervals are obtained. They lend support to the theoretical results and suggest questions for further investigation. Our empirical and theoretical study provides a guide to the use of these estimators in practice.},
 author = {Lih-Yuan Deng and C. F. J. Wu},
 journal = {Journal of the American Statistical Association},
 number = {398},
 pages = {568--576},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Estimation of Variance of the Regression Estimator},
 urldate = {2024-09-27},
 volume = {82},
 year = {1987}
}
@book{川口-澤田2024,
    author = {川口康平 and 澤田真行},
    year = {2024},
    title = {因果推論の計量経済学},
    series = {},
    volume = {},
    edition = {},
    url = {https://github.com/keisemi/EconometriciansGuide_CausalInference},
    doi = {},
    publisher = {日本評論社}
}
@book{Hernan-Robins2020,
    author = {M. A. Hernán and James M. Robins},
    year = {2020},
    title = {{Causal Inference: What If}},
    series = {},
    volume = {},
    edition = {https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/},
    doi = {},
    publisher = {Boca Raton: Chapman & Hall/CRC}
}
@book{Morgan-Winship2014,
    author = {Stephen L. Morgan and Christopher Winship},
    year = {2014},
    title = {{Counterfactuals and Causal Inference}},
    series = {Analytical Methods for Social Research},
    volume = {},
    edition = {2},
    url = {https://doi.org/10.1017/CBO9781107587991},
    doi = {},
    publisher = {Cambridge University Press}
}
@book{Rosenbaum2023,
    author = {Paul R. Rosenbaum},
    year = {2023},
    title = {Causal Inference},
    series = {MIT Press Essential Knowledge Series},
    volume = {},
    edition = {},
    url = {https://mitpress.mit.edu/9780262545198/causal-inference/},
    doi = {},
    publisher = {MIT Press}
}
@article{Pearl2015, title={TRYGVE HAAVELMO AND THE EMERGENCE OF CAUSAL CALCULUS}, volume={31}, DOI={10.1017/S0266466614000231}, number={1}, journal={Econometric Theory}, author={Pearl, Judea}, year={2015}, pages={152–179}}

@article{Haavelmo1943,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/1905714},
 author = {Trygve Haavelmo},
 journal = {Econometrica},
 number = {1},
 pages = {1--12},
 publisher = {[Wiley, Econometric Society]},
 title = {The Statistical Implications of a System of Simultaneous Equations},
 urldate = {2024-09-28},
 volume = {11},
 year = {1943}
}
@article{Leamer1983,
 ISSN = {00028282},
 URL = {http://www.jstor.org/stable/1803924},
 author = {Edward E. Leamer},
 journal = {The American Economic Review},
 number = {1},
 pages = {31--43},
 publisher = {American Economic Association},
 title = {Let's Take the Con Out of Econometrics},
 urldate = {2024-09-28},
 volume = {73},
 year = {1983}
}

@article{Bongers+2021,
	author = {Stephan Bongers and Patrick Forr{\'e} and Jonas Peters and Joris M. Mooij},
	date = {2021/10/1},
	date-added = {2024-09-28 17:56:02 +0900},
	date-modified = {2024-09-28 17:56:02 +0900},
	doi = {10.1214/21-AOS2064},
	journal = {The Annals of Statistics},
	journal1 = {The Annals of Statistics},
	journal2 = {The Annals of Statistics},
	month = {10},
	number = {5},
	pages = {2885--2915 },
	title = {Foundations of structural causal models with cycles and latent variables},
	url = {https://doi.org/10.1214/21-AOS2064},
	volume = {49},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1214/21-AOS2064}}
@article{Rubin1980,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2287653},
 author = {Donald B. Rubin},
 journal = {Journal of the American Statistical Association},
 number = {371},
 pages = {591--593},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Randomization Analysis of Experimental Data: The Fisher Randomization Test Comment},
 urldate = {2024-09-28},
 volume = {75},
 year = {1980}
}

@article{Runge2023,
	abstract = {Many research questions in Earth and environmental sciences are inherently causal, requiring robust analyses to establish whether and how changes in one variable cause changes in another. Causal inference provides the theoretical foundations to use data and qualitative domain knowledge to quantitatively answer these questions, complementing statistics and machine learning techniques. However, there is still a broad language gap between the methodological and domain science communities. In this Technical Review, we explain the use of causal inference frameworks with a focus on the challenges of time series data. Domain-adapted explanations, method guidance and practical case studies provide an accessible summary of methods for causal discovery and causal effect estimation. Examples from climate and biogeosciences illustrate typical challenges, such as contemporaneous causation, hidden confounding and non-stationarity, and some strategies to address these challenges. Integrating causal thinking into data-driven science will facilitate process understanding and more robust machine learning and statistical models for Earth and environmental sciences, enabling the tackling of many open problems with relevant environmental, economic and societal implications.},
	author = {Runge, Jakob and Gerhardus, Andreas and Varando, Gherardo and Eyring, Veronika and Camps-Valls, Gustau},
	date = {2023/07/01},
	date-added = {2024-09-28 18:33:14 +0900},
	date-modified = {2024-09-28 18:33:14 +0900},
	doi = {10.1038/s43017-023-00431-y},
	id = {Runge2023},
	isbn = {2662-138X},
	journal = {Nature Reviews Earth \& Environment},
	number = {7},
	pages = {487--505},
	title = {Causal inference for time series},
	url = {https://doi.org/10.1038/s43017-023-00431-y},
	volume = {4},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s43017-023-00431-y}}

@article{Cui-Athey2022,
	abstract = {Causal inference has recently attracted substantial attention in the machine learning and artificial intelligence community. It is usually positioned as a distinct strand of research that can broaden the scope of machine learning from predictive modelling to intervention and decision-making. In this Perspective, however, we argue that ideas from causality can also be used to improve the stronghold of machine learning, predictive modelling, if predictive stability, explainability and fairness are important. With the aim of bridging the gap between the tradition of precise modelling in causal inference and black-box approaches from machine learning, stable learning is proposed and developed as a source of common ground. This Perspective clarifies a source of risk for machine learning models and discusses the benefits of bringing causality into learning. We identify the fundamental problems addressed by stable learning, as well as the latest progress from both causal inference and learning perspectives, and we discuss relationships with explainability and fairness problems.},
	author = {Cui, Peng and Athey, Susan},
	date = {2022/02/01},
	date-added = {2024-09-28 19:20:10 +0900},
	date-modified = {2024-09-28 19:20:10 +0900},
	doi = {10.1038/s42256-022-00445-z},
	id = {Cui2022},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	number = {2},
	pages = {110--115},
	title = {Stable learning establishes some common ground between causal inference and machine learning},
	url = {https://doi.org/10.1038/s42256-022-00445-z},
	volume = {4},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1038/s42256-022-00445-z}}
@article{LaLonde1986,
 ISSN = {00028282},
 URL = {http://www.jstor.org/stable/1806062},
 abstract = {This paper compares the effect on trainee earnings of an employment program that was run as a field experiment where participants were randomly assigned to treatment and control groups with the estimates that would have been produced by an econometrician. This comparison shows that many of the econometric procedures do not replicate the experimentally determined results, and it suggests that researchers should be aware of the potential for specification errors in other nonexperimental evaluations.},
 author = {Robert J. LaLonde},
 journal = {The American Economic Review},
 number = {4},
 pages = {604--620},
 publisher = {American Economic Association},
 title = {Evaluating the Econometric Evaluations of Training Programs with Experimental Data},
 urldate = {2024-09-28},
 volume = {76},
 year = {1986}
}
@article{Heckman2010,
 ISSN = {00220515},
 URL = {http://www.jstor.org/stable/20778729},
 abstract = {This paper compares the structural approach to economic policy analysis with the program evaluation approach. It offers a third way to do policy analysis that combines the best features of both approaches. I illustrate the value of this alternative approach by making the implicit economics of LATE explicit, thereby extending the interpretability and range of policy questions that LATE can answer.},
 author = {James J. Heckman},
 journal = {Journal of Economic Literature},
 number = {2},
 pages = {356--398},
 publisher = {American Economic Association},
 title = {Building Bridges Between Structural and Program Evaluation Approaches to Evaluating Policy},
 urldate = {2024-09-28},
 volume = {48},
 year = {2010}
}

@article{黒木学2016,
	author = {黒木 学},
	doi = {10.20742/pbsj.44.0_124},
	journal = {日本行動計量学会大会抄録集},
	pages = {124-125},
	title = {R-6-2 構造的因果モデルから潜在反応モデルへ},
	volume = {44},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.20742/pbsj.44.0_124}}

@article{Lucas1976,
	author = {Robert E. Lucas},
	doi = {https://doi.org/10.1016/S0167-2231(76)80003-6},
	issn = {0167-2231},
	journal = {Carnegie-Rochester Conference Series on Public Policy},
	pages = {19-46},
	title = {Econometric policy evaluation: A critique},
	url = {https://www.sciencedirect.com/science/article/pii/S0167223176800036},
	volume = {1},
	year = {1976},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0167223176800036},
	bdsk-url-2 = {https://doi.org/10.1016/S0167-2231(76)80003-6}}
@book{Ding2024,
  author = {Peng Ding},
  year = {2024},
  title = {{A First Course in Causal Inference}},
  series = {},
  volume = {},
  edition = {},
  url = {https://doi.org/10.1201/9781003484080},
  publisher = {Chapman and Hall/CRC}
}
@book{Huber2023,
  author = {Martin Huber},
  year = {2023},
  title = {{Causal Analysis: Impact Evaluation and Causal Machine Learning with Applications in R}},
  series = {},
  volume = {},
  edition = {},
  url = {https://mitpress.mit.edu/9780262545914/causal-analysis/},
  publisher = {MIT Press}
}

@article{Brand+2023,
	abstract = {This article reviews recent advances in causal inference relevant to sociology. We focus on a selective subset of contributions aligning with four broad topics: causal effect identification and estimation in general, causal effect heterogeneity, causal effect mediation, and temporal and spatial interference. We describe how machine learning, as an estimation strategy, can be effectively combined with causal inference, which has been traditionally concerned with identification. The incorporation of machine learning in causal inference enables researchers to better address potential biases in estimating causal effects and uncover heterogeneous causal effects. Uncovering sources of effect heterogeneity is key for generalizing to populations beyond those under study. While sociology has long emphasized the importance of causal mechanisms, historical and life-cycle variation, and social contexts involving network interactions, recent conceptual and computational advances facilitate more principled estimation of causal effects under these settings. We encourage sociologists to incorporate these insights into their empirical research.},
	author = {Brand, Jennie E. and Zhou, Xiang and Xie, Yu},
	doi = {https://doi.org/10.1146/annurev-soc-030420-015345},
	issn = {1545-2115},
	journal = {Annual Review of Sociology},
	keywords = {external validity},
	number = {Volume 49, 2023},
	pages = {81-110},
	publisher = {Annual Reviews},
	title = {Recent Developments in Causal Inference and Machine Learning},
	type = {Journal Article},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-soc-030420-015345},
	volume = {49},
	year = {2023},
	bdsk-url-1 = {https://www.annualreviews.org/content/journals/10.1146/annurev-soc-030420-015345},
	bdsk-url-2 = {https://doi.org/10.1146/annurev-soc-030420-015345}}
@article{Fuller-Kim2005,
  author = {Wayne A. Fuller and Jae-Kwang Kim},
  year = {2005},
  title = {Hot Deck Imputation for the Response Model},
  journal = {Survey Methodology},
  volume = {31},
  number = {2},
  pages = {139-149},
  url = {https://www150.statcan.gc.ca/n1/pub/12-001-x/2005002/article/9041-eng.pdf}
}
@misc{Kwon+2024,
      title={Debiased calibration estimation using generalized entropy in survey sampling}, 
      author={Yonghyun Kwon and Jae Kwang Kim and Yumou Qiu},
      year={2024},
      eprint={2404.01076},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2404.01076}, 
}
@book{Hayashi2000,
  author = {Fumio Hayashi},
  year = {2000},
  title = {Econometrics},
  series = {},
  volume = {},
  edition = {},
  url = {https://press.princeton.edu/books/ebook/9781400823833/econometrics-1},
  publisher = {Princeton University Press}
}
@article{Pfanzagl1969,
	abstract = {The concept of minimum contrast (m.c.) estimates used in this paper covers maximum likelihood (m.l.) estimates as a special case. Section 1 contains sufficient conditions for the existence of measurable m.c. estimates and for their consistency.},
	author = {Pfanzagl, J. },
	date = {1969/12/01},
	date-added = {2024-09-29 14:40:43 +0900},
	date-modified = {2024-09-29 14:40:43 +0900},
	doi = {10.1007/BF02613654},
	id = {Pfanzagl1969},
	isbn = {1435-926X},
	journal = {Metrika},
	number = {1},
	pages = {249--272},
	title = {On the measurability and consistency of minimum contrast estimates},
	url = {https://doi.org/10.1007/BF02613654},
	volume = {14},
	year = {1969},
	bdsk-url-1 = {https://doi.org/10.1007/BF02613654}}

@article{Huber1964,
	author = {Peter J. Huber},
	doi = {10.1214/aoms/1177703732},
	journal = {The Annals of Mathematical Statistics},
	number = {1},
	pages = {73 -- 101},
	publisher = {Institute of Mathematical Statistics},
	title = {{Robust Estimation of a Location Parameter}},
	url = {https://doi.org/10.1214/aoms/1177703732},
	volume = {35},
	year = {1964},
	bdsk-url-1 = {https://doi.org/10.1214/aoms/1177703732}}
@book{Huber1981,
  author = {Peter J. Huber},
  year = {1981},
  title = {Robust Statistics},
  series = {Wiley Series in Probability and Statistics},
  volume = {},
  edition = {},
  url = {https://onlinelibrary.wiley.com/doi/book/10.1002/0471725250},
  publisher = {John Wiley & Sons}
}
@phdthesis{LeCam1952,
  author = {Lucien Marie Le{\ }Cam},
  school = {UC Berkeley},
  title = {On Some Asymptotic Properties of Maximum Likelihood Estimates and Related Bayes' Estimates},
  year = {1952},
  url = {https://math.berkeley.edu/publications/some-asymptotic-properties-maximum-likelihood-estimates-and-related-bayes-estimates},
}
@article{Godambe1997,
 ISSN = {07492170},
 URL = {http://www.jstor.org/stable/4356005},
 abstract = {The development of the modern theory of estimating functions is traced from its inception. It is shown that this development has brought about a synthesis of the two historically important methodologies of estimation namely, the 'least squares' and the 'maximum likelihood'.},
 author = {V. P. Godambe},
 journal = {Lecture Notes-Monograph Series},
 pages = {5--15},
 publisher = {Institute of Mathematical Statistics},
 title = {Estimating Functions: A Synthesis of Least Squares and Maximum Likelihood Methods},
 urldate = {2024-09-29},
 volume = {32},
 year = {1997}
}
@article{Owen1988,
    author = {Art B Owen},
    title = "{Empirical likelihood ratio confidence intervals for a single functional}",
    journal = {Biometrika},
    volume = {75},
    number = {2},
    pages = {237-249},
    year = {1988},
    month = {06},
    abstract = "{The empirical distribution function based on a sample is well known to be the maximum likelihood estimate of the distribution from which the sample was taken. In this paper the likelihood function for distributions is used to define a likelihood ratio function for distributions. It is shown that this empirical likelihood ratio function can be used to construct confidence intervals for the sample mean, for a class of M-estimates that includes quantiles, and for differentiable statistical functionals. The results are nonpara-metric extensions of Wilks's (1938) theorem for parametric likelihood ratios. The intervals are illustrated on some real data and compared in a simulation to some bootstrap confidence intervals and to intervals based on Student's t statistic. A hybrid method that uses the bootstrap to determine critical values of the likelihood ratio is introduced.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/75.2.237},
    url = {https://doi.org/10.1093/biomet/75.2.237},
    eprint = {https://academic.oup.com/biomet/article-pdf/75/2/237/827908/75-2-237.pdf},
}
@article{Wedderburn1974,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334725},
 abstract = {To define a likelihood we have to specify the form of distribution of the observations, but to define a quasi-likelihood function we need only specify a relation between the mean and variance of the observations and the quasi-likelihood can then be used for estimation. For a one-parameter exponential family the log likelihood is the same as the quasi-likelihood and it follows that assuming a one-parameter exponential family is the weakest sort of distributional assumption that can be made. The Gauss-Newton method for calculating nonlinear least squares estimates generalizes easily to deal with maximum quasi-likelihood estimates, and a rearrangement of this produces a generalization of the method described by Nelder & Wedderburn (1972).},
 author = {R. W. M. Wedderburn},
 journal = {Biometrika},
 number = {3},
 pages = {439--447},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Quasi-Likelihood Functions, Generalized Linear Models, and the Gauss-Newton Method},
 urldate = {2024-09-29},
 volume = {61},
 year = {1974}
}

@article{Qin-Lawless1994,
	author = {Jin Qin and Jerry Lawless},
	doi = {10.1214/aos/1176325370},
	journal = {The Annals of Statistics},
	keywords = {Asymptotic efficiency, Auxiliary information, empirical likelihood, estimating equations, parametric likelihood, semiparametric models, testing hypotheses, Wilks' theorem},
	number = {1},
	pages = {300 -- 325},
	publisher = {Institute of Mathematical Statistics},
	title = {{Empirical Likelihood and General Estimating Equations}},
	url = {https://doi.org/10.1214/aos/1176325370},
	volume = {22},
	year = {1994},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1176325370}}

@article{Robins-Greenland1992,
	author = {Robins, James M. and Greenland, Sander},
	date-added = {2024-09-29 19:47:26 +0900},
	date-modified = {2024-09-29 19:47:26 +0900},
	id = {00001648-199203000-00013},
	isbn = {1044-3983},
	journal = {Epidemiology},
	keywords = {causality; causal modeling; epidemiologic methods; risk},
	n2 = {We consider the problem of separating the direct effects of an exposure from effects relayed through an intermediate variable (indirect effects). We show that adjustment for the intermediate variable, which is the most common method of estimating direct effects, can be biased. We also show that, even in a randomized crossover trial of exposure, direct and indirect effects cannot be separated without special assumptions; in other words, direct and indirect effects are not separately identifiable when only exposure is randomized. If the exposure and intermediate never interact to cause disease and if intermediate effects can be controlled, that is, blocked by a suitable intervention, then a trial randomizing both exposure and the intervention can separate direct from indirect effects. Nonetheless, the estimation must be carried out using the G-computation algorithm. Conventional adjustment methods remain biased. When exposure and the intermediate interact to cause disease, direct and indirect effects will not be separable even in a trial in which both the exposure and the intervention blocking intermediate effects are randomly assigned. Nonetheless, in such a trial, one can still estimate the fraction of exposure-induced disease that could be prevented by control of the intermediate. Even in the absence of an intervention blocking the intermediate effect, the fraction of exposure-induced disease that could be prevented by control of the intermediate can be estimated with the G-computation algorithm if data are obtained on additional confounding variables. (Epidemiology 1992;3:143--155)  {\copyright} Lippincott-Raven Publishers.},
	number = {2},
	title = {Identifiability and Exchangeability for Direct and Indirect Effects},
	url = {https://journals.lww.com/epidem/fulltext/1992/03000/identifiability_and_exchangeability_for_direct_and.13.aspx},
	volume = {3},
	year = {1992},
	bdsk-url-1 = {https://journals.lww.com/epidem/fulltext/1992/03000/identifiability_and_exchangeability_for_direct_and.13.aspx}}
@article{Nguyen+2021,
  author = {T. Q. Nguyen and I. Schmid and E. A. Stuart},
  year = {2021},
  title = {Clarifying causal mediation analysis for the applied researcher: Defining effects based on what we want to learn},
  journal = {Psychological Methods},
  volume = {26},
  number = {2},
  pages = {255-271},
  url = {https://doi.org/10.1037/met0000299}
}

@inbook{Pearl2012,
	abstract = {Summary This chapter contains sections titled: Mediation: Direct and indirect effects The mediation formula: A simple solution to a thorny problem Relation to other methods Conclusions Acknowledgments References},
	author = {Pearl, Judea},
	booktitle = {Causality},
	chapter = {12},
	doi = {https://doi.org/10.1002/9781119945710.ch12},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119945710.ch12},
	isbn = {9781119945710},
	keywords = {mediation formula, causal pathways in nonlinear models, direct effects, in legal disputes over hiring race/sex, and the discrimination, structural equation models, for analyzing path-specific effects, controlled direct effect, in linear systems, to path coefficient of link, identification of direct effects, not `ignorability' or `sequential ignorability', hiring context, path-specific effects in policy making, mediation effects in nonparametric, nonlinear models, mediation effects in linear, logistic, and probit models, analytical `sensitivity analysis' in statistics for parameter estimation, mediation formulas, causal pathway assessment in the sciences},
	pages = {151-179},
	publisher = {John Wiley & Sons, Ltd},
	title = {The Mediation Formula: A Guide to the Assessment of Causal Pathways in Nonlinear Models},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119945710.ch12},
	year = {2012},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781119945710.ch12},
	bdsk-url-2 = {https://doi.org/10.1002/9781119945710.ch12}}

@article{Fujii+2022,
	author = {Ryosuke Fujii, Shuntaro Sato, Yoshiki Tsuboi, Andres Cardenas and Koji Suzuki},
	doi = {10.1080/15592294.2021.1959736},
	eprint = {https://doi.org/10.1080/15592294.2021.1959736},
	journal = {Epigenetics},
	note = {PMID: 34384035},
	number = {7},
	pages = {759--785},
	publisher = {Taylor \& Francis},
	title = {DNA methylation as a mediator of associations between the environment and chronic diseases: A scoping review on application of mediation analysis},
	url = {https://doi.org/10.1080/15592294.2021.1959736},
	volume = {17},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1080/15592294.2021.1959736}}

@article{Robins1986,
	abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.},
	author = {James M. Robins},
	doi = {https://doi.org/10.1016/0270-0255(86)90088-6},
	issn = {0270-0255},
	journal = {Mathematical Modelling},
	number = {9},
	pages = {1393-1512},
	title = {A new approach to causal inference in mortality studies with a sustained exposure period---application to control of the healthy worker survivor effect},
	url = {https://www.sciencedirect.com/science/article/pii/0270025586900886},
	volume = {7},
	year = {1986},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0270025586900886},
	bdsk-url-2 = {https://doi.org/10.1016/0270-0255(86)90088-6}}
@article{Robins+1992,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2532304},
 abstract = {In order to estimate the causal effects of one or more exposures or treatments on an outcome of interest, one has to account for the effect of "confounding factors" which both covary with the exposures or treatments and are independent predictors of the outcome. In this paper we present regression methods which, in contrast to standard methods, adjust for the confounding effect of multiple continuous or discrete covariates by modelling the conditional expectation of the exposures or treatments given the confounders. In the special case of a univariate dichotomous exposure or treatment, this conditional expectation is identical to what Rosenbaum and Rubin have called the propensity score. They have also proposed methods to estimate causal effects by modelling the propensity score. Our methods generalize those of Rosenbaum and Rubin in several ways. First, our approach straightforwardly allows for multivariate exposures or treatments, each of which may be continuous, ordinal, or discrete. Second, even in the case of a single dichotomous exposure, our approach does not require subclassification or matching on the propensity score so that the potential for "residual confounding," i.e., bias, due to incomplete matching is avoided. Third, our approach allows a rather general formalization of the idea that it is better to use the "estimated propensity score" than the true propensity score even when the true score is known. The additional power of our approach derives from the fact that we assume the causal effects of the exposures or treatments can be described by the parametric component of a semiparametric regression model. To illustrate our methods, we reanalyze the effect of current cigarette smoking on the level of forced expiratory volume in one second in a cohort of 2,713 adult white males. We compare the results with those obtained using standard methods.},
 author = {James M. Robins and Steven D. Mark and Whitney K. Newey},
 journal = {Biometrics},
 number = {2},
 pages = {479--495},
 publisher = {International Biometric Society},
 title = {Estimating Exposure Effects by Modelling the Expectation of Exposure Conditional on Confounders},
 urldate = {2024-09-29},
 volume = {48},
 year = {1992}
}

@article{Robins+2000,
	author = {Robins, James M. and Hern{\'a}n, Miguel {\'A}ngel and Brumback, Babette},
	date-added = {2024-09-29 20:22:32 +0900},
	date-modified = {2024-09-29 20:22:32 +0900},
	id = {00001648-200009000-00011},
	isbn = {1044-3983},
	journal = {Epidemiology},
	keywords = {causality; counterfactuals; epidemiologic methods; longitudinal data; structural models; confounding; intermediate variables},
	n2 = {In observational studies with exposures or treatments that vary over time, standard approaches for adjustment of confounding are biased when there exist time-dependent confounders that are also affected by previous treatment. This paper introduces marginal structural models, a new class of causal models that allow for improved adjustment of confounding in those situations. The parameters of a marginal structural model can be consistently estimated using a new class of estimators, the inverse-probability-of-treatment weighted estimators.},
	number = {5},
	title = {Marginal Structural Models and Causal Inference in Epidemiology},
	url = {https://journals.lww.com/epidem/fulltext/2000/09000/marginal_structural_models_and_causal_inference_in.11.aspx},
	volume = {11},
	year = {2000},
	bdsk-url-1 = {https://journals.lww.com/epidem/fulltext/2000/09000/marginal_structural_models_and_causal_inference_in.11.aspx}}

@inproceedings{Robins2000,
	abstract = {Robins (1993, 1994, 1997, 1998ab) has developed a set of causal or counterfactual models, the structural nested models (SNMs). This paper describes an alternative new class of causal models --- the (non-nested) marginal structural models (MSMs). We will then describe a class of semiparametric estimators for the parameters of these new models under a sequential randomization (i.e., ignorability) assumption. We then compare the strengths and weaknesses of MSMs versus SNMs for causal inference from complex longitudinal data with time-dependent treatments and confounders. Our results provide an extension to continuous treatments of propensity score estimators of an average treatment effect.},
	address = {New York, NY},
	author = {Robins, James M.},
	booktitle = {Statistical Models in Epidemiology, the Environment, and Clinical Trials},
	editor = {Halloran, M. Elizabeth and Berry, Donald},
	isbn = {978-1-4612-1284-3},
	pages = {95--133},
	publisher = {Springer New York},
	title = {Marginal Structural Models versus Structural nested Models as Tools for Causal inference},
	year = {2000}}

@article{Vamsteelandt-Joffe2014,
	author = {Stijn Vansteelandt and Marshall Joffe},
	doi = {10.1214/14-STS493},
	journal = {Statistical Science},
	keywords = {causal effect, confounding, direct effect, instrumental variable, mediation, time-varying confounding},
	number = {4},
	pages = {707 -- 731},
	publisher = {Institute of Mathematical Statistics},
	title = {{Structural Nested Models and G-estimation: The Partially Realized Promise}},
	url = {https://doi.org/10.1214/14-STS493},
	volume = {29},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1214/14-STS493}}
@article{Baron-Kenny1986,
  author = {R. M. Baron and D. A. Kenny},
  year = {1986},
  title = {The moderator–mediator variable distinction in social psychological research: Conceptual, strategic, and statistical considerations},
  journal = {Journal of Personality and Social Psychology},
  volume = {51},
  number = {6},
  pages = {1173-1182},
  url = {https://doi.org/10.1037/0022-3514.51.6.1173}
}
@article{Alwin-Hauser1975,
 ISSN = {00031224},
 URL = {http://www.jstor.org/stable/2094445},
 abstract = {This paper is about the logic of interpreting recursive causal theories in sociology. We review the distinction between associations and effects and discuss the decomposition of effects into direct and indirect components. We then describe a general method for decomposing effects into their components by the systematic application of ordinary least squares regression. The method involves successive computation of reduced-form equations, beginning with an equation containing only exogenous variables, then computing equations which add intervening variables in sequence from cause to effect. This generates all the information required to decompose effects into their various direct and indirect parts. This method is a substitute for the often more cumbersome computation of indirect effects from the structural coefficients (direct effects) of the causal model. Finally, we present a way of summarizing this information in tabular form and illustrate the procedures using an empirical example.},
 author = {Duane F. Alwin and Robert M. Hauser},
 journal = {American Sociological Review},
 number = {1},
 pages = {37--47},
 publisher = {[American Sociological Association, Sage Publications, Inc.]},
 title = {The Decomposition of Effects in Path Analysis},
 urldate = {2024-09-29},
 volume = {40},
 year = {1975}
}
@article{Cheng1994,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2291203},
 abstract = {This article considers a distribution-free estimation procedure for a basic pattern of missing data that often arises from the well-known double sampling in survey methodology. Without parametric modeling of the missing mechanism or the joint distribution, kernel regression estimators are used to estimate mean functionals through empirical estimation of the missing pattern. A generalization of the method of Cheng and Wei is verified under the assumption of missing at random. Asymptotic distributions are derived for estimating the mean of the incomplete data and for estimating the mean treatment difference in a nonrandomized observational study. The nonparametric method is compared with a naive pairwise deletion method and a linear regression method via the asymptotic relative efficiencies and a simulation study. The comparison shows that the proposed nonparametric estimators attain reliable performances in general.},
 author = {Philip E. Cheng},
 journal = {Journal of the American Statistical Association},
 number = {425},
 pages = {81--87},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Nonparametric Estimation of Mean Functionals with Data Missing at Random},
 urldate = {2024-09-29},
 volume = {89},
 year = {1994}
}
@article{Bertrand+2004,
    author = {Bertrand, Marianne and Duflo, Esther and Mullainathan, Sendhil},
    title = "{How Much Should We Trust Differences-In-Differences Estimates?*}",
    journal = {The Quarterly Journal of Economics},
    volume = {119},
    number = {1},
    pages = {249-275},
    year = {2004},
    month = {02},
    abstract = "{Most papers that employ Differences-in-Differences estimation (DD) use many years of data and focus on serially correlated outcomes but ignore that the resulting standard errors are inconsistent. To illustrate the severity of this issue, we randomly generate placebo laws in state-level data on female wages from the Current Population Survey. For each law, we use OLS to compute the DD estimate of its “effect” as well as the standard error of this estimate. These conventional DD standard errors severely understate the standard deviation of the estimators: we find an “effect” significant at the 5 percent level for up to 45 percent of the placebo interventions. We use Monte Carlo simulations to investigate how well existing methods help solve this problem. Econometric corrections that place a specific parametric form on the time-series process do not perform well. Bootstrap (taking into account the autocorrelation of the data) works well when the number of states is large enough. Two corrections based on asymptotic approximation of the variance-covariance matrix work well for moderate numbers of states and one correction that collapses the time series information into a “pre”- and “post”-period and explicitly takes into account the effective sample size works well even for small numbers of states.}",
    issn = {0033-5533},
    doi = {10.1162/003355304772839588},
    url = {https://doi.org/10.1162/003355304772839588},
    eprint = {https://academic.oup.com/qje/article-pdf/119/1/249/5304584/119-1-249.pdf},
}
@article{Poole-Rosenthal1991,
 ISSN = {00925853, 15405907},
 URL = {http://www.jstor.org/stable/2111445},
 abstract = {Congressional roll call voting has been highly structured for most of U.S. history. The structure is revealed by a dynamic, spatial analysis of the entire roll call voting record from 1789 to 1985. The space is characterized by a predominant major dimension with, at times, a significant, but less important second dimension. In the modern era, spatial positions are very stable. This stability is such that, under certain conditions, short run forecasting of roll call votes is possible. Since the end of World War II, changes in congressional voting patterns have occurred almost entirely through the process of replacement of retiring or defeated legislators with new members. Politically, selection is far more important than adaptation.},
 author = {Keith T. Poole and Howard Rosenthal},
 journal = {American Journal of Political Science},
 number = {1},
 pages = {228--278},
 publisher = {[Midwest Political Science Association, Wiley]},
 title = {Patterns of Congressional Voting},
 urldate = {2024-09-29},
 volume = {35},
 year = {1991}
}
@book{浅古泰史2016,
  author = {浅古泰史},
  year = {2016},
  title = {政治の数理分析入門},
  series = {},
  volume = {},
  edition = {},
  url = {http://www.yasushiasako.com/book.htm},
  publisher = {木鐸社}
}

@article{稗田健志2015,
	author = {稗田健志},
	doi = {10.7218/nenpouseijigaku.66.1_13},
	journal = {年報政治学},
	number = {1},
	pages = {1_13-1_36},
	title = {政治理論と実証研究をつなぐ環},
	volume = {66},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.7218/nenpouseijigaku.66.1_13}}

@article{細野助博1981,
	author = {細野助博},
	doi = {10.11228/pcs1981.1981.65},
	journal = {公共選択の研究},
	number = {1},
	pages = {65-73},
	title = {政治競争モデル設計の試み},
	volume = {1981},
	year = {1981},
	bdsk-url-1 = {https://doi.org/10.11228/pcs1981.1981.65}}
@article{Davis+1970,
 ISSN = {00030554, 15375943},
 URL = {http://www.jstor.org/stable/1953842},
 author = {Otto A. Davis and Melvin J. Hinich and Peter C. Ordeshook},
 journal = {The American Political Science Review},
 number = {2},
 pages = {426--448},
 publisher = {[American Political Science Association, Cambridge University Press]},
 title = {An Expository Development of a Mathematical Model of the Electoral Process},
 urldate = {2024-09-30},
 volume = {64},
 year = {1970}
}

@article{Converse2006,
	author = {Philip E. Converse},
	doi = {10.1080/08913810608443650},
	eprint = {https://doi.org/10.1080/08913810608443650},
	journal = {Critical Review},
	number = {1-3},
	pages = {1--74},
	publisher = {Routledge},
	title = {The nature of belief systems in mass publics (1964)},
	url = {https://doi.org/10.1080/08913810608443650},
	volume = {18},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1080/08913810608443650}}
@article{Hinich-Pollard1981,
 ISSN = {00925853, 15405907},
 URL = {http://www.jstor.org/stable/2110856},
 abstract = {In his unidimensional model of electoral competition, Downs argues that voters use party ideology as an informational short cut for forecasting the policies that a party will pursue if elected. Parties are perceived by voters as points on an ideological axis. In the Davis-Hinich multidimensional model, on the other hand, the axes are real issues, and the principal actors are politicians who are modeled as points in the multi-issue space. This paper reformulates spatial voting theory in terms of a model that connects what we call predictive dimensions with political issues that are salient during a given election campaign. This model is both a synthesis and an extension of the Downs and Davis-Hinich spatial models. We obtain a median voter result for one predictive dimension that is similar to the Downs result but with important differences. We also obtain results showing the electoral advantage of incumbency and the tendency for incremental change when there is a great deal of heterogeneity in voter perceptions about the candidates.},
 author = {Melvin J. Hinich and Walker Pollard},
 journal = {American Journal of Political Science},
 number = {2},
 pages = {323--341},
 publisher = {[Midwest Political Science Association, Wiley]},
 title = {A New Approach to the Spatial Theory of Electoral Competition},
 urldate = {2024-10-01},
 volume = {25},
 year = {1981}
}
@Article{Imai+2016,
  author={Imai, Kosuke and Lo, James and Olmsted, Jonathan},
  title={{Fast Estimation of Ideal Points with Massive Data}},
  journal={American Political Science Review},
  year=2016,
  volume={110},
  number={4},
  pages={631-656},
  month={November},
  keywords={},
  doi={},
  abstract={Estimation of ideological positions among voters, legislators, and other actors is central to many subfields of political science. Recent applications include large data sets of various types including roll calls, surveys, and textual and social media data. To overcome the resulting computational challenges, we propose fast estimation methods for ideal points with massive data. We derive the expectation-maximization (EM) algorithms to estimate the standard ideal point model with binary, ordinal, and continuous outcome variables. We then extend this methodology to dynamic and hierarchical ideal point models by developing variational EM algorithms for approximate inference. We demonstrate the computational efficiency and scalability of our methodology through a variety of real and simulated data. In cases where a standard Markov chain Monte Carlo algorithm would require several days to compute ideal points, the proposed algorithm can produce essentially identical estimates within minutes. Open-source software is available for implementing the proposed methods.},
  url={https://ideas.repec.org/a/cup/apsrev/v110y2016i04p631-656_00.html}
}
@article{Heckman-Snyder1997,
 ISSN = {07416261},
 URL = {http://www.jstor.org/stable/3087459},
 abstract = {We formulate and estimate a rigorously justified linear probability model of binary choices over alternatives characterized by unobserved attributes. We apply the model to estimate preferences of congressmen as expressed in their votes on bills. The effective dimension of the attribute space characterizing votes is larger than what has been estimated in recent influential studies of congressional voting by Poole and Rosenthal. Congressmen vote on more than ideology. Issue-specific attributes are an important determinant of congressional voting patterns. The estimated dimension is too large for the median voter model to describe congressional voting.},
 author = {James J. Heckman and James M. Snyder},
 journal = {The RAND Journal of Economics},
 pages = {S142--S189},
 publisher = {[RAND Corporation, Wiley]},
 title = {Linear Probability Models of the Demand for Attributes with an Empirical Application to Estimating the Preferences of Legislators},
 urldate = {2024-10-01},
 volume = {28},
 year = {1997}
}
@InCollection{McFadden1976,
  author={Daniel L. McFadden},
  title={{Quantal Choice Analysis: A Survey}},
  booktitle={{Annals of Economic and Social Measurement, Volume 5, number 4}},
  publisher={National Bureau of Economic Research, Inc},
  year={1976},
  month={},
  volume={},
  number={},
  series={NBER Chapters},
  edition={},
  chapter={},
  pages={363-390},
  doi={},
  keywords={},
  abstract={No abstract is available for this item.},
  url={https://ideas.repec.org/h/nbr/nberch/10488.html}
}
@Article{Zeileis+2008,
  title = {Regression Models for Count Data in {R}},
  author = {Achim Zeileis and Christian Kleiber and Simon Jackman},
  journal = {Journal of Statistical Software},
  year = {2008},
  volume = {27},
  number = {8},
  url = {https://www.jstatsoft.org/v27/i08/},
}
@article{Clinton-Meirowitz2003, title={Integrating Voting Theory and Roll Call Analysis: A Framework}, volume={11}, DOI={10.1093/pan/mpg023}, number={4}, journal={Political Analysis}, author={Clinton, Joshua D. and Meirowitz, Adam}, year={2017}, pages={381–396}}
@article{Clinton-Meirowitz2001,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/25791647},
 abstract = {Existing preference estimation procedures do not incorporate the full structure of the spatial model of voting, as they fail to use the sequential natural of the agenda. In the maximum likelihood framework, the consequences of this omission may be far-reaching. First, information useful for the identification of the model is neglected. Specifically, information that identifies the proposal locations is ignored. Second, the dimensionality of the policy space may be incorrectly estimated. Third, preference and proposal location estimates are incorrect and difficult to interpret in terms of the spatial model. We also show that the Bayesian simulation approach to ideal point estimation (Clinton et al. 2000; Jackman 2000) may be improved through the use of information about the legislative agenda. This point is illustrated by comparing several preference estimators of the first U.S. House (1789–1791).},
 author = {Joshua D. Clinton and Adam Meirowitz},
 journal = {Political Analysis},
 number = {3},
 pages = {242--259},
 publisher = {[Oxford University Press, Society for Political Methodology]},
 title = {Agenda Constrained Legislator Ideal Points and the Spatial Voting Model},
 urldate = {2024-10-01},
 volume = {9},
 year = {2001}
}


@article{加藤拓巳2021,
	author = {加藤拓巳},
	doi = {10.7222/marketing.2021.001},
	journal = {マーケティングジャーナル},
	number = {3},
	pages = {78-88},
	title = {選択における文脈効果の出現要因とその方向性},
	volume = {40},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.7222/marketing.2021.001}}
@article{Simonson1989,
 ISSN = {00935301, 15375277},
 URL = {http://www.jstor.org/stable/2489315},
 abstract = {Building on previous research, this article proposes that choice behavior under preference uncertainty may be easier to explain by assuming that consumers select the alternative supported by the best reasons. This approach provides an explanation for the so-called attraction effect and leads to the prediction of a compromise effect. Consistent with the hypotheses, the results indicate that (1) brands tend to gain share when they become compromise alternatives in a choice set; (2) attraction and compromise effects tend to be stronger among subjects who expect to justify their decisions to others; and (3) selections of dominating and compromise brands are associated with more elaborate and difficult decisions.},
 author = {Itamar Simonson},
 journal = {Journal of Consumer Research},
 number = {2},
 pages = {158--174},
 publisher = {Oxford University Press},
 title = {Choice Based on Reasons: The Case of Attraction and Compromise Effects},
 urldate = {2024-10-01},
 volume = {16},
 year = {1989}
}
@book{Embretson-Reise2000,
  author = {Susan E. Embretson and Steven P. Reise},
  year = {2000},
  title = {Item Response Theory for Psychologists},
  series = {},
  volume = {},
  edition = {},
  url = {https://doi.org/10.4324/9781410605269},
  publisher = {Psychology Press}
}
@book{Lord+1968,
  author = {F. M. Lord and M. R. Novick and A. Birnbaum},
  year = {1968},
  title = {{Statistical Theories of Mental Test Scores}},
  series = {},
  volume = {},
  edition = {},
  url = {},
  publisher = {Addison-Wesley}
}

@article{Bock-Aitkin1981,
	abstract = {Maximum likelihood estimation of item parameters in the marginal distribution, integrating over the distribution of ability, becomes practical when computing procedures based on an EM algorithm are used. By characterizing the ability distribution empirically, arbitrary assumptions about its form are avoided. The Em procedure is shown to apply to general item-response models lacking simple sufficient statistics for ability. This includes models with more than one latent dimension.},
	author = {Bock, R. Darrell and Aitkin, Murray},
	date = {1981/12/01},
	date-added = {2024-10-01 16:33:15 +0900},
	date-modified = {2024-10-01 16:33:15 +0900},
	doi = {10.1007/BF02293801},
	id = {Bock1981},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {4},
	pages = {443--459},
	title = {Marginal maximum likelihood estimation of item parameters: Application of an EM algorithm},
	url = {https://doi.org/10.1007/BF02293801},
	volume = {46},
	year = {1981},
	bdsk-url-1 = {https://doi.org/10.1007/BF02293801}}
@book{Rasch1960,
  author = {Georg W. Rasch},
  year = {1960},
  title = {Studies in Mathematical Psychology: I. Probabilistic Models for Some Intelligence and Attainment Tests},
  series = {},
  volume = {},
  edition = {},
  url = {},
  publisher = {Nielsen & Lydiche}
}

@article{Fischer1973,
	abstract = {The present paper consists of a theoretical and an empirical part: First Rasch's test model for items with two answer categories is considered under the assumption of linear constraints on the item parameters (`linear logistic model'). It is shown that this model is appropriate for the analysis of subject areas in instructional research if the subject area comprises tasks or items which are solved by the pupil by combination of a certain number of cognitive operations or rules. An empirical investigation was made which showed that the psychological complexity of problems in elementary differential calculus, as taught in secondary school mathematics, can be approximately explained through the assumption of seven psychologically meaningful operations. The psychological contribution of this analysis does not lie in a mere statistical description of item difficulties, but rather in the testing of hypotheses as to which steps (operations) in solving a problem are to be viewed as psychological units. It was seen, for instance, that differentiation of a polynomial is to be considered a single operation psychologically, which is mastered and correctly combined with the other operations or not, and that the complexity of a task is primarily determined by the combination of different operations and is not increased significantly when the same operation occurs repeatedly within the problem.},
	author = {Gerhard H. Fischer},
	doi = {https://doi.org/10.1016/0001-6918(73)90003-6},
	issn = {0001-6918},
	journal = {Acta Psychologica},
	number = {6},
	pages = {359-374},
	title = {The linear logistic test model as an instrument in educational research},
	url = {https://www.sciencedirect.com/science/article/pii/0001691873900036},
	volume = {37},
	year = {1973},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0001691873900036},
	bdsk-url-2 = {https://doi.org/10.1016/0001-6918(73)90003-6}}
@book{Fox2010,
  author = {Jean-Paul Fox},
  year = {2010},
  title = {{Bayesian Item Response Modeling}},
  series = {},
  volume = {},
  edition = {},
  url = {https://doi.org/10.1007/978-1-4419-0742-4},
  publisher = {Springer New York}
}

@article{Bock-Lieberman1970,
	abstract = {A method of estimating the parameters of the normal ogive model for dichotomously scored item-responses by maximum likelihood is demonstrated. Although the procedure requires numerical integration in order to evaluate the likelihood equations, a computer implemented Newton-Raphson solution is shown to be straightforward in other respects. Empirical tests of the procedure show that the resulting estimates are very similar to those based on a conventional analysis of item ``difficulties''and first factor loadings obtained from the matrix of tetrachoric correlation coefficients. Problems of testing the fit of the model, and of obtaining invariant parameters are discussed.},
	author = {Bock, R. Darrell and Lieberman, Marcus},
	date = {1970/06/01},
	date-added = {2024-10-01 19:21:24 +0900},
	date-modified = {2024-10-01 19:21:24 +0900},
	doi = {10.1007/BF02291262},
	id = {Darrell Bock1970},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {2},
	pages = {179--197},
	title = {Fitting a response model forn dichotomously scored items},
	url = {https://doi.org/10.1007/BF02291262},
	volume = {35},
	year = {1970},
	bdsk-url-1 = {https://doi.org/10.1007/BF02291262}}
@book{Hambleton+1991,
  author = {Hambleton, R. K., Swaminathan, H., & Rogers, H. J.},
  year = {1991},
  title = {Fundamentals of item response theory},
  series = {},
  volume = {},
  edition = {},
  url = {},
  publisher = {Sage Publications}
}

@article{前川眞一2023,
	author = {前川眞一},
	doi = {10.24690/jart.19.1_35},
	journal = {日本テスト学会誌},
	number = {1},
	pages = {35-58},
	title = {項目反応理論におけるモデル変換},
	volume = {19},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.24690/jart.19.1_35}}

@article{Masters1982,
	abstract = {A unidimensional latent trait model for responses scored in two or more ordered categories is developed. This ``Partial Credit''model is a member of the family of latent trait models which share the property of parameter separability and so permit ``specifically objective''comparisons of persons and items. The model can be viewed as an extension of Andrich's Rating Scale model to situations in which ordered response alternatives are free to vary in number and structure from item to item. The difference between the parameters in this model and the ``category boundaries''in Samejima's Graded Response model is demonstrated. An unconditional maximum likelihood procedure for estimating the model parameters is developed.},
	author = {Masters, Geoff N. },
	date = {1982/06/01},
	date-added = {2024-10-01 19:43:34 +0900},
	date-modified = {2024-10-01 19:43:34 +0900},
	doi = {10.1007/BF02296272},
	id = {Masters1982},
	isbn = {1860-0980},
	journal = {Psychometrika},
	number = {2},
	pages = {149--174},
	title = {A rasch model for partial credit scoring},
	url = {https://doi.org/10.1007/BF02296272},
	volume = {47},
	year = {1982},
	bdsk-url-1 = {https://doi.org/10.1007/BF02296272}}

@article{Muraki1992,
	abstract = {ABSTRACT The Partial Credit model with a varying slope parameter has been developed, and it is called the Generalized Partial Credit model. The item step parameter of this model is decomposed to a location and a threshold parameter, following Andrich's Rating Scale formulation. The EM algorithm for estimating the model parameters was derived. The performance of this generalized model is compared with a Rasch family of polytomous item response models based on both simulated and real data. Simulated data were generated and then analyzed by the various polytomous item response models. The results obtained demonstrate that the rating formulation of the Generalized Partial Credit model is quite adaptable to the analysis of polytomous item responses. The real data used in this study consisted of NAEP Mathematics data which was made up of both dichotomous and polytomous item types. The Partial Credit model was applied to this data using both constant and varying slope parameters. The Generalized Partial Credit model, which provides for varying slope parameters, yielded better fit to data than the Partial Credit model without such a provision. Index terms: item response model polytomous item response model the Partial Credit model the Rating Scale model the Nominal Response model NAEP},
	author = {Muraki, Eiji},
	doi = {https://doi.org/10.1002/j.2333-8504.1992.tb01436.x},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/j.2333-8504.1992.tb01436.x},
	journal = {ETS Research Report Series},
	number = {1},
	pages = {i-30},
	title = {{A Generalized Partial Credit Model: Application of an EM Algorithm}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1992.tb01436.x},
	volume = {1992},
	year = {1992},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/j.2333-8504.1992.tb01436.x},
	bdsk-url-2 = {https://doi.org/10.1002/j.2333-8504.1992.tb01436.x}}

@inbook{Samejima1997,
	abstract = {The graded response model represents a family of mathematical models that deals with ordered polytomous categories. These ordered categories include rating such as letter grading, A, B, C, D, and F, used in the evaluation of students' performance; strongly disagree, disagree, agree, and strongly agree, used in attitude surveys; or partial credit given in accordance with an examinee's degree of attainment in solving a problem.},
	address = {New York, NY},
	author = {Samejima, Fumiko},
	booktitle = {Handbook of Modern Item Response Theory},
	doi = {10.1007/978-1-4757-2691-6_5},
	editor = {van der Linden, Wim J. and Hambleton, Ronald K.},
	isbn = {978-1-4757-2691-6},
	pages = {85--100},
	publisher = {Springer New York},
	title = {Graded Response Model},
	url = {https://doi.org/10.1007/978-1-4757-2691-6_5},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1007/978-1-4757-2691-6_5}}
@inproceedings{井澤廣行2008,
  author = {井澤廣行},
  year = {2008},
  title = {Raschの理論と理念、及び、WrightのRasch測定展開},
  booktitle = {},
  volume = {20},
  number = {2},
  pages = {},
  url = {https://ryuka.repo.nii.ac.jp/records/176}
}

@article{Wright-Panchapakesan1969,
	author = {Benjamin Wright and Nargis Panchapakesan},
	doi = {10.1177/001316446902900102},
	eprint = {https://doi.org/10.1177/001316446902900102},
	journal = {Educational and Psychological Measurement},
	number = {1},
	pages = {23-48},
	title = {A Procedure for Sample-Free Item Analysis},
	url = {https://doi.org/10.1177/001316446902900102},
	volume = {29},
	year = {1969},
	bdsk-url-1 = {https://doi.org/10.1177/001316446902900102}}

@article{Kruschke2011,
	abstract = { Psychologists have been trained to do data analysis by asking whether null values can be rejected. Is the difference between groups nonzero? Is choice accuracy not at chance level? These questions have been traditionally addressed by null hypothesis significance testing (NHST). NHST has deep problems that are solved by Bayesian data analysis. As psychologists transition to Bayesian data analysis, it is natural to ask how Bayesian analysis assesses null values. The article explains and evaluates two different Bayesian approaches. One method involves Bayesian model comparison (and uses Bayes factors). The second method involves Bayesian parameter estimation and assesses whether the null value falls among the most credible values. Which method to use depends on the specific question that the analyst wants to answer, but typically the estimation approach (not using Bayes factors) provides richer information than the model comparison approach. },
	author = {John K. Kruschke},
	doi = {10.1177/1745691611406925},
	eprint = {https://doi.org/10.1177/1745691611406925},
	journal = {Perspectives on Psychological Science},
	note = {PMID: 26168520},
	number = {3},
	pages = {299-312},
	title = {Bayesian Assessment of Null Values Via Parameter Estimation and Model Comparison},
	url = {https://doi.org/10.1177/1745691611406925},
	volume = {6},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1177/1745691611406925}}
@misc{ONeill-Kypraios2016,
      title={Bayesian model choice via mixture distributions with application to epidemics and population process models}, 
      author={Philip D. O'Neill and Theodore Kypraios},
      year={2016},
      eprint={1411.7888},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1411.7888}, 
}
@article{Jackman2000, title={Estimation and Inference Are Missing Data Problems: Unifying Social Science Statistics via Bayesian Simulation}, volume={8}, DOI={10.1093/oxfordjournals.pan.a029818}, number={4}, journal={Political Analysis}, author={Jackman, Simon}, year={2000}, pages={307–332}}

@article{Bliss1934,
	author = {C. I. Bliss},
	doi = {10.1126/science.79.2037.38},
	eprint = {https://www.science.org/doi/pdf/10.1126/science.79.2037.38},
	journal = {Science},
	number = {2037},
	pages = {38-39},
	title = {The Method of Probits},
	url = {https://www.science.org/doi/abs/10.1126/science.79.2037.38},
	volume = {79},
	year = {1934},
	bdsk-url-1 = {https://www.science.org/doi/abs/10.1126/science.79.2037.38},
	bdsk-url-2 = {https://doi.org/10.1126/science.79.2037.38}}
@article{Klein-Spady1993,
 ISSN = {00129682, 14680262},
 URL = {http://www.jstor.org/stable/2951556},
 abstract = {This paper proposes an estimator for discrete choice models that makes no assumption concerning the functional form of the choice probability function, where this function can be characterized by an index. The estimator is shown to be consistent, asymptotically normally distributed, and to achieve the semiparametric efficiency bound. Monte-Carlo evidence indicates that there may be only modest efficiency losses relative to maximum likelihood estimation when the distribution of the disturbances is known, and that the small-sample behavior of the estimator in other cases is good.},
 author = {Roger W. Klein and Richard H. Spady},
 journal = {Econometrica},
 number = {2},
 pages = {387--421},
 publisher = {[Wiley, Econometric Society]},
 title = {An Efficient Semiparametric Estimator for Binary Response Models},
 urldate = {2024-10-01},
 volume = {61},
 year = {1993}
}
@article{Albert1992,
 ISSN = {03629791},
 URL = {http://www.jstor.org/stable/1165149},
 abstract = {The problem of estimating item parameters from a two-parameter normal ogive model is considered. Gibbs sampling (Gelfand & Smith, 1990) is used to simulate draws from the joint posterior distribution of the ability and item parameters. This method gives marginal posterior density estimates for any parameter of interest; these density estimates can be used to judge the accuracy of normal approximations based on maximum likelihood estimates. This simulation technique is illustrated using data from a mathematics placement exam.},
 author = {James H. Albert},
 journal = {Journal of Educational Statistics},
 number = {3},
 pages = {251--269},
 publisher = {[Sage Publications, Inc., American Educational Research Association, American Statistical Association]},
 title = {Bayesian Estimation of Normal Ogive Item Response Curves Using Gibbs Sampling},
 urldate = {2024-10-01},
 volume = {17},
 year = {1992}
}
@article{Bafumi+2005, title={{Practical Issues in Implementing and Understanding Bayesian Ideal Point Estimation}}, volume={13}, DOI={10.1093/pan/mpi010}, number={2}, journal={Political Analysis}, author={Bafumi, Joseph and Gelman, Andrew and Park, David K. and Kaplan, Noah}, year={2005}, pages={171–187}}
@article{Lewis-Poole2004, title={Measuring Bias and Uncertainty in Ideal Point Estimates via the Parametric Bootstrap}, volume={12}, DOI={10.1093/pan/mph015}, number={2}, journal={Political Analysis}, author={Lewis, Jeffrey B. and Poole, Keith T.}, year={2004}, pages={105–127}}
@article{Carroll+2009,
 ISSN = {10471987, 14764989},
 URL = {http://www.jstor.org/stable/25791974},
 abstract = {DW-NOMINATE scores for the U.S. Congress are widely used measures of legislators' ideological locations over time. These scores have been used in a large number of studies in political science and closely related fields. In this paper, we extend the work of Lewis and Poole (2004) on the parametric bootstrap to DW-NOMINATE and obtain standard errors for the legislator ideal points. These standard errors are in the range of 1%–4% of the range of DW-NOMINATE coordinates.},
 author = {Royce Carroll and Jeffrey B. Lewis and James Lo and Keith T. Poole and Howard Rosenthal},
 journal = {Political Analysis},
 number = {3},
 pages = {261--275},
 publisher = {[Cambridge University Press, Oxford University Press, Society for Political Methodology]},
 title = {Measuring Bias and Uncertainty in DW-NOMINATE Ideal Point Estimates via the Parametric Bootstrap},
 urldate = {2024-10-01},
 volume = {17},
 year = {2009}
}
@article{Martin+2011,
  author = {Andrew D. Martin and Kevin M. Quinn and Jong Hee Park},
  year = {2011},
  title = {{MCMCpack: Markov chain Monte Carlo in R}},
  journal = {Journal of Statistical Software},
  volume = {42},
  number = {9},
  pages = {1-21},
  url = {https://doi.org/10.18637/jss.v042.i09}
}
@book{Arnold2018,
  author = {Jeffrey B. Arnold},
  year = {2018},
  title = {{Simon Jackman’s Bayesian Model Examples in Stan}},
  series = {},
  volume = {},
  edition = {},
  url = {https://jrnold.github.io/bugs-examples-in-stan/},
  publisher = {}
}

@article{坂本-柴山2017,
	author = {坂本佑太朗 and 柴山直},
	doi = {10.32146/bdajcs.6.31},
	journal = {データ分析の理論と応用},
	number = {1},
	pages = {31-45},
	title = {学力テストの下位領域に関する多次元IRT分析},
	volume = {6},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.32146/bdajcs.6.31}}

@article{宇佐美+2018,
	author = {宇佐美慧 and 荘島宏二郎 and 光永悠彦 and 登藤直弥},
	doi = {10.20587/pamjaep.60.0_24},
	journal = {日本教育心理学会総会発表論文集},
	pages = {24-25},
	title = {項目反応理論（IRT）の考え方と実践},
	volume = {60},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.20587/pamjaep.60.0_24},
  note = {スライドも公開されている: http://usami-lab.com/教育心理学会チュートリアル2018資料.pdf},
  }
@book{樋口知之*2011,
  author = {樋口知之 and 上野玄太 and 中野慎也 and 中村和幸 and 吉田亮},
  year = {2011},
  title = {データ同化入門―次世代のシミュレーション技術},
  series = {予測と発見の科学},
  volume = {6},
  edition = {},
  url = {https://www.asakura.co.jp/detail.php?book_code=12786},
  publisher = {朝倉書店}
}
@article{Hoerl-Kennard1970,
 ISSN = {00401706},
 URL = {http://www.jstor.org/stable/1267351},
 abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X′X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X′X to obtain biased estimates with smaller mean square error.},
 author = {Arthur E. Hoerl and Robert W. Kennard},
 journal = {Technometrics},
 number = {1},
 pages = {55--67},
 publisher = {[Taylor & Francis, Ltd., American Statistical Association, American Society for Quality]},
 title = {Ridge Regression: Biased Estimation for Nonorthogonal Problems},
 urldate = {2024-10-02},
 volume = {12},
 year = {1970}
}
@incollection{Nickl2017,
  author = {Richard Nickl},
  booktitle = {Bernoulli News},
  publisher = {Bernoulli Society},
  title = {{On Bayesian Inference for Some Statistical Inverse Problems with Partial Differential Equations}},
  year = {2017},
  volume = {24},
  number = {2},
  url = {https://www.bernoullisociety.org/files/BernoulliNews2017-2.pdf},
}

@article{岡本-本間2002,
	author = {岡本良夫 and 本間生夫},
	doi = {10.1541/ieejeiss1987.122.9_1417},
	journal = {電気学会論文誌. C, 電子・情報・システム部門誌},
	number = {9},
	pages = {1417-1425},
	title = {脳波とその逆問題解析},
	volume = {122},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1541/ieejeiss1987.122.9_1417}}

@article{Ter-Pogossian1975,
	abstract = { An apparatus was developed for obtaining emission transaxial images of sections of organs containing positron-emitting radiopharmaceuticals. The detection system is a hexagonal array of 24 NaI (Tl) detectors connected to coincidence circuits to achieve the ``electronic'' collimation of annihilation photons. The image is formed by a computer-applied algorithm which provides quantitative reconstruction of the distribution of activity. Computer simulations, phantom and animal studies show that this approach is capable of providing images of better contrast and resolution than are obtained with scintillation cameras. Advantages of positron vs. single photon reconstruction tomography are discussed. },
	author = {Ter-Pogossian, Michel M. and Phelps, Michael E. and Hoffman, Edward J. and Mullani, Nizar A.},
	doi = {10.1148/114.1.89},
	eprint = {https://doi.org/10.1148/114.1.89},
	journal = {Radiology},
	note = {PMID: 1208874},
	number = {1},
	pages = {89-98},
	title = {A Positron-Emission Transaxial Tomograph for Nuclear Imaging (PETT)},
	url = {https://doi.org/10.1148/114.1.89},
	volume = {114},
	year = {1975},
	bdsk-url-1 = {https://doi.org/10.1148/114.1.89}}

@book{Nakamura-Potthast2015,
	abstract = {This book provides a comprehensive introduction to the techniques, tools and methods for
        inverse problems and data assimilation, and is written at the interface between mathematics
        and applications for students, researchers and developers in mathematics, physics,
        engineering, acoustics, electromagnetics, meteorology, biology, environmental and other
        applied sciences. Basic analytic questions and tools are introduced, as well as a wide
        variety of concepts, methods and approaches to formulate and solve inverse problems. OCTAVE
        /MATLAB codes are included, which serve as a first step towards simulations and more
        sophisticated inversion or data assimilation algorithms.},
	author = {Nakamura, Gen and Potthast, Roland},
	doi = {10.1088/978-0-7503-1218-9},
	isbn = {978-0-7503-1218-9},
	publisher = {IOP Publishing},
	series = {2053-2563},
	title = {Inverse Modeling: An introduction to the theory and methods of inverse problems and data assimilation},
	url = {https://dx.doi.org/10.1088/978-0-7503-1218-9},
	year = {2015},
	bdsk-url-1 = {https://dx.doi.org/10.1088/978-0-7503-1218-9}}

@article{vanLeeuwen+2019,
	abstract = {Particle filters contain the promise of fully nonlinear data assimilation. They have been applied in numerous science areas, including the geosciences, but their application to high-dimensional geoscience systems has been limited due to their inefficiency in high-dimensional systems in standard settings. However, huge progress has been made, and this limitation is disappearing fast due to recent developments in proposal densities, the use of ideas from (optimal) transportation, the use of localization and intelligent adaptive resampling strategies. Furthermore, powerful hybrids between particle filters and ensemble Kalman filters and variational methods have been developed. We present a state-of-the-art discussion of present efforts of developing particle filters for high-dimensional nonlinear geoscience state-estimation problems, with an emphasis on atmospheric and oceanic applications, including many new ideas, derivations and unifications, highlighting hidden connections, including pseudo-code, and generating a valuable tool and guide for the community. Initial experiments show that particle filters can be competitive with present-day methods for numerical weather prediction, suggesting that they will become mainstream soon.},
	author = {van Leeuwen, Peter Jan and K{\"u}nsch, Hans R. and Nerger, Lars and Potthast, Roland and Reich, Sebastian},
	doi = {https://doi.org/10.1002/qj.3551},
	eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.3551},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	keywords = {hybrids, localization, nonlinear data assimilation, particle filters, proposal densities},
	number = {723},
	pages = {2335-2365},
	title = {Particle filters for high-dimensional geoscience applications: A review},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3551},
	volume = {145},
	year = {2019},
	bdsk-url-1 = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.3551},
	bdsk-url-2 = {https://doi.org/10.1002/qj.3551}}

@article{Kac1966,
	author = {Mark Kac},
	doi = {10.1080/00029890.1966.11970915},
	eprint = {https://doi.org/10.1080/00029890.1966.11970915},
	journal = {The American Mathematical Monthly},
	number = {4P2},
	pages = {1--23},
	publisher = {Taylor \& Francis},
	title = {Can One Hear the Shape of a Drum?},
	url = {https://doi.org/10.1080/00029890.1966.11970915},
	volume = {73},
	year = {1966},
	bdsk-url-1 = {https://doi.org/10.1080/00029890.1966.11970915}}

@article{佐々木1958,
	author = {Y. Sasaki},
	doi = {10.2151/jmsj1923.36.3_77},
	journal = {氣象集誌. 第2輯},
	number = {3},
	pages = {77-88},
	title = {An ObJective Analysis Based on the Variational Method},
	volume = {36},
	year = {1958},
	bdsk-url-1 = {https://doi.org/10.2151/jmsj1923.36.3_77}}

@article{浅井冨雄1967,
	author = {浅井冨雄},
	doi = {10.2151/jmsj1965.45.3_251},
	journal = {Journal of the Meteorological Society of Japan. Ser. II},
	number = {3},
	pages = {251-260},
	title = {細胞状積雲対流の特性について},
	volume = {45},
	year = {1967},
	bdsk-url-1 = {https://doi.org/10.2151/jmsj1965.45.3_251}}

@article{Yano+2018,
	address = {Boston MA, USA},
	author = {Jun-Ichi Yano and Micha{\l} Z. Ziemia{\'n}ski and Mike Cullen and Piet Termonia and Jeanette Onvlee and Lisa Bengtsson and Alberto Carrassi and Richard Davy and Anna Deluca and Suzanne L. Gray and V{\'\i}ctor Homar and Martin K{\"o}hler and Simon Krichak and Silas Michaelides and Vaughan T. J. Phillips and Pedro M. M. Soares and Andrzej A. Wyszogrodzki},
	doi = {10.1175/BAMS-D-17-0125.1},
	journal = {Bulletin of the American Meteorological Society},
	number = {4},
	pages = {699 - 710},
	publisher = {American Meteorological Society},
	title = {Scientific Challenges of Convective-Scale Numerical Weather Prediction},
	url = {https://journals.ametsoc.org/view/journals/bams/99/4/bams-d-17-0125.1.xml},
	volume = {99},
	year = {2018},
	bdsk-url-1 = {https://journals.ametsoc.org/view/journals/bams/99/4/bams-d-17-0125.1.xml},
	bdsk-url-2 = {https://doi.org/10.1175/BAMS-D-17-0125.1}}
@incollection{Lorenz1995,
  author = {Edward N. Lorenz},
  booktitle = {Seminar on Predictability, 4-8 September 1995},
  publisher = {ECMWF},
  title = {Predictability: A Problem Partly Solved},
  year = {1995},
  url = {https://www.ecmwf.int/en/elibrary/75462-predictability-problem-partly-solved},
}

@software{Milan2021,
	author = {Milan K},
	doi = {10.5281/zenodo.5121430},
	month = jul,
	publisher = {Zenodo},
	title = {milankl/Lorenz96.jl: v0.3.0},
	url = {https://doi.org/10.5281/zenodo.5121430},
	version = {v0.3.0},
	year = 2021,
	bdsk-url-1 = {https://doi.org/10.5281/zenodo.5121430}}
@book{Balwada+2023,
  author = {Balwada, D., Abernathey, R., Acharya, S., Adcroft, A., Brener, J., Balaji, V. and Zanna, L.},
  year = {2023},
  title = {{Learning Machine Learning with Lorenz-96}},
  series = {},
  volume = {},
  edition = {},
  url     = {https://m2lines.github.io/L96_demo/intro.html},
  publisher = {Authorea Preprints}
}
@phdthesis{vanKekem2018,
  author = {Dirk Leendert van{/ }Kekem},
  school = {University of Groningen},
  title = {Dynamics of the Lorenz-96 model: Bifurcations, symmetries and waves},
  year = {2018},
  url  = {https://research.rug.nl/en/publications/dynamics-of-the-lorenz-96-model-bifurcations-symmetries-and-waves}
}
@articleInfo{Kerin-Engler2022,
title = {On the Lorenz '96 model and some generalizations},
journal = {Discrete and Continuous Dynamical Systems - B},
volume = {27},
number = {2},
pages = {769-797},
year = {2022},
issn = {1531-3492},
doi = {10.3934/dcdsb.2021064},
url = {https://www.aimsciences.org/article/id/a8ebee41-67f0-484c-abc6-99061059ace6},
author = {John Kerin and Hans Engler},
keywords = {Lorenz-96 model, equivariant dynamical system, Hopf bifurcation, normal form, Neimark-Sacker bifurcation}
}

@article{Kantas+2014,
	abstract = { We consider the inverse problem of estimating the initial condition of a partial differential equation, which is observed only through noisy measurements at discrete time intervals. In particular, we focus on the case where Eulerian measurements are obtained from the time and space evolving vector field, whose evolution obeys the two-dimensional Navier--Stokes equations defined on a torus. This context is particularly relevant to the area of numerical weather forecasting and data assimilation. We will adopt a Bayesian formulation resulting from a particular regularization that ensures the problem is well posed. In the context of Monte Carlo--based inference, it is a challenging task to obtain samples from the resulting high-dimensional posterior on the initial condition. In real data assimilation applications it is common for computational methods to invoke the use of heuristics and Gaussian approximations. As a result, the resulting inferences are biased and not well justified in the presence of nonlinear dynamics and observations. On the other hand, Monte Carlo methods can be used to assimilate data in a principled manner, but they are often perceived as inefficient in this context due to the high dimensionality of the problem. In this work we will propose a generic sequential Monte Carlo (SMC) sampling approach for high-dimensional inverse problems that overcomes these difficulties. The method builds upon ``state of the art'' Markov chain Monte Carlo (MCMC) techniques, which are currently considered as benchmarks for evaluating data assimilation algorithms used in practice. SMC samplers can improve in terms of efficiency, as they possess greater flexibility and one can include steps like sequential tempering, adaptation, and parallelization with a relatively low number of extra computations. We will illustrate this using numerical examples, where our proposed SMC approach can achieve the same accuracy as MCMC but in a much more efficient manner. },
	author = {Kantas, Nikolas and Beskos, Alexandros and Jasra, Ajay},
	doi = {10.1137/130930364},
	eprint = {https://doi.org/10.1137/130930364},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	number = {1},
	pages = {464-489},
	title = {Sequential Monte Carlo Methods for High-Dimensional Inverse Problems: A Case Study for the Navier--Stokes Equations},
	url = {https://doi.org/10.1137/130930364},
	volume = {2},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1137/130930364}}

@article{Chorin1967,
	author = {Alexandre Joel Chorin},
	journal = {Bulletin of the American Mathematical Society},
	number = {6},
	pages = {928 -- 931},
	publisher = {American Mathematical Society},
	title = {{The numerical solution of the Navier-Stokes equations for an incompressible fluid}},
	volume = {73},
	year = {1967},
  url = {https://projecteuclid.org/journals/bulletin-of-the-american-mathematical-society-new-series/volume-73/issue-6/The-numerical-solution-of-the-Navier-Stokes-equations-for-an/bams/1183529112.full},
  }
@article{Chorin1968,
 ISSN = {00255718, 10886842},
 URL = {http://www.jstor.org/stable/2004575},
 abstract = {A finite-difference method for solving the time-dependent Navier Stokes equations for an incompressible fluid is introduced. This method uses the primitive variables, i.e. the velocities and the pressure, and is equally applicable to problems in two and three space dimensions. Test problems are solved, and an application to a three-dimensional convection problem is presented.},
 author = {Alexandre Joel Chorin},
 journal = {Mathematics of Computation},
 number = {104},
 pages = {745--762},
 publisher = {American Mathematical Society},
 title = {Numerical Solution of the Navier-Stokes Equations},
 urldate = {2024-10-05},
 volume = {22},
 year = {1968}
}

@article{Lorenz1963,
	address = {Boston MA, USA},
	author = {Edward N. Lorenz},
	doi = {10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2},
	journal = {Journal of Atmospheric Sciences},
	number = {2},
	pages = {130 - 141},
	publisher = {American Meteorological Society},
	title = {Deterministic Nonperiodic Flow},
	url = {https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml},
	volume = {20},
	year = {1963},
	bdsk-url-1 = {https://journals.ametsoc.org/view/journals/atsc/20/2/1520-0469_1963_020_0130_dnf_2_0_co_2.xml},
	bdsk-url-2 = {https://doi.org/10.1175/1520-0469(1963)020%3C0130:DNF%3E2.0.CO;2}}
@book{Temam1995,
  author = {Roger Meyer Temam},
  year = {1995},
  title = {Navier–Stokes Equations and Nonlinear Functional Analysis},
  series = {CBMS-NSF Regional Conference Series in Applied Mathematics},
  volume = {},
  edition = {2},
  doi = {10.1137/1.9781611970050},
  publisher = {Society for Industrial and Applied Mathematics}
}
@book{Tsai2018,
  author = {Tai-Peng Tsai},
  year = {2018},
  title = {Lectures on Navier-Stokes Equations},
  series = {Graduate Studies in Mathematics},
  volume = {192},
  edition = {},
  doi = {10.1090/gsm/192},
  publisher = {American Mathematical Society}
}

@article{Cox-Matthews2002,
	abstract = {We develop a class of numerical methods for stiff systems, based on the method of exponential time differencing. We describe schemes with second- and higher-order accuracy, introduce new Runge--Kutta versions of these schemes, and extend the method to show how it may be applied to systems whose linear part is nondiagonal. We test the method against other common schemes, including integrating factor and linearly implicit methods, and show how it is more accurate in a number of applications. We apply the method to both dissipative and dispersive partial differential equations, after illustrating its behavior using forced ordinary differential equations with stiff linear parts.},
	author = {S.M. Cox and P.C. Matthews},
	doi = {https://doi.org/10.1006/jcph.2002.6995},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	number = {2},
	pages = {430-455},
	title = {Exponential Time Differencing for Stiff Systems},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999102969950},
	volume = {176},
	year = {2002},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0021999102969950},
	bdsk-url-2 = {https://doi.org/10.1006/jcph.2002.6995}}

@article{Sanderse-Koren2012,
	abstract = {This paper investigates the temporal accuracy of the velocity and pressure when explicit Runge--Kutta methods are applied to the incompressible Navier--Stokes equations. It is shown that, at least up to and including fourth order, the velocity attains the classical order of accuracy without further constraints. However, in case of a time-dependent gradient operator, which can appear in case of time-varying meshes, additional order conditions need to be satisfied to ensure the correct order of accuracy. Furthermore, the pressure is only first-order accurate unless additional order conditions are satisfied. Two new methods that lead to a second-order accurate pressure are proposed, which are applicable to a certain class of three- and four-stage methods. A special case appears when the boundary conditions for the continuity equation are independent of time, since in that case the pressure can be computed to the same accuracy as the velocity field, without additional cost. Relevant computations of decaying vortices and of an actuator disk in a time-dependent inflow support the analysis and the proposed methods.},
	author = {B. Sanderse and B. Koren},
	doi = {https://doi.org/10.1016/j.jcp.2011.11.028},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	keywords = {Differential--algebraic equations, Incompressible Navier--Stokes equations, Temporal accuracy, Time integration, Runge--Kutta method, Moving meshes},
	number = {8},
	pages = {3041-3063},
	title = {Accuracy analysis of explicit Runge--Kutta methods applied to the incompressible Navier--Stokes equations},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999111006838},
	volume = {231},
	year = {2012},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0021999111006838},
	bdsk-url-2 = {https://doi.org/10.1016/j.jcp.2011.11.028}}

@article{Tsitouras2011,
	abstract = {Among the most popular methods for the solution of the Initial Value Problem are the Runge--Kutta pairs of orders 5 and 4. These methods can be derived solving a system of nonlinear equations for its coefficients. To achieve this, we usually admit various simplifying assumptions. The most common of them are the so-called row simplifying assumptions. Here we neglect them and present an algorithm for the construction of Runge--Kutta pairs of orders 5 and 4 based only in the first column simplifying assumption. The result is a pair that outperforms other known pairs in the bibliography when tested to the standard set of problems of DETEST. A cost free fourth order formula is also derived for handling dense output.},
	author = {Ch. Tsitouras},
	doi = {https://doi.org/10.1016/j.camwa.2011.06.002},
	issn = {0898-1221},
	journal = {Computers & Mathematics with Applications},
	keywords = {Runge--Kutta, Truncation error, Non-linear algebraic systems, Free parameters, Dense output},
	number = {2},
	pages = {770-775},
	title = {Runge--Kutta pairs of order 5(4) satisfying only the first column simplifying assumption},
	url = {https://www.sciencedirect.com/science/article/pii/S0898122111004706},
	volume = {62},
	year = {2011},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0898122111004706},
	bdsk-url-2 = {https://doi.org/10.1016/j.camwa.2011.06.002}}
@article{Schrodinger1931,
  author = {E. Schrödinger},
  year = {1931},
  title = {Über die Umkehrung der Naturgesetze},
  journal = {Sitzungsberichte der preussischen Akademie der Wissenschaften, physikalische mathematische Klasse},
  volume = {8},
  number = {9},
  pages = {144-153},
  url = {}
}

@article{Chetrite+2021,
	abstract = {We present an English translation of Erwin Schr{\"o}dinger's paper on ``On the Reversal of the Laws of Nature`'. In this paper, Schr{\"o}dinger analyses the idea of time reversal of a diffusion process. Schr{\"o}dinger's paper acted as a prominent source of inspiration for the works of Bernstein on reciprocal processes and of Kolmogorov on time reversal properties of Markov processes and detailed balance. The ideas outlined by Schr{\"o}dinger also inspired the development of probabilistic interpretations of quantum mechanics by F{\'e}nyes, Nelson and others as well as the notion of ``Euclidean Quantum Mechanics''as probabilistic analogue of quantization. In the second part of the paper, Schr{\"o}dinger discusses the relation between time reversal and statistical laws of physics. We emphasize in our commentary the relevance of Schr{\"o}dinger's intuitions for contemporary developments in statistical nano-physics.},
	author = {Chetrite, Rapha{\"e}l and Muratore-Ginanneschi, Paolo and Schwieger, Kay},
	date = {2021/11/22},
	date-added = {2024-10-06 14:58:11 +0900},
	date-modified = {2024-10-06 14:58:11 +0900},
	doi = {10.1140/epjh/s13129-021-00032-7},
	id = {Chetrite2021},
	isbn = {2102-6467},
	journal = {The European Physical Journal H},
	number = {1},
	pages = {28},
	title = {E. Schr{\"o}dinger's 1931 paper ``On the Reversal of the Laws of Nature''{$[$}``{\"U}ber die Umkehrung der Naturgesetze'', Sitzungsberichte der preussischen Akademie der Wissenschaften, physikalisch-mathematische Klasse, 8 N9 144--153{$]$}},
	url = {https://doi.org/10.1140/epjh/s13129-021-00032-7},
	volume = {46},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1140/epjh/s13129-021-00032-7}}

@InProceedings{Sharrock+2024,
  title = 	 {Sequential Neural Score Estimation: Likelihood-Free Inference with Conditional Score Based Diffusion Models},
  author =       {Sharrock, Louis and Simons, Jack and Liu, Song and Beaumont, Mark},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {44565--44602},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/sharrock24a/sharrock24a.pdf},
  url = 	 {https://proceedings.mlr.press/v235/sharrock24a.html},
  abstract = 	 {We introduce Sequential Neural Posterior Score Estimation (SNPSE), a score-based method for Bayesian inference in simulator-based models. Our method, inspired by the remarkable success of score-based methods in generative modelling, leverages conditional score-based diffusion models to generate samples from the posterior distribution of interest. The model is trained using an objective function which directly estimates the score of the posterior. We embed the model into a sequential training procedure, which guides simulations using the current approximation of the posterior at the observation of interest, thereby reducing the simulation cost. We also introduce several alternative sequential approaches, and discuss their relative merits. We then validate our method, as well as its amortised, non-sequential, variant on several numerical examples, demonstrating comparable or superior performance to existing state-of-the-art methods such as Sequential Neural Posterior Estimation (SNPE).}
}

@article{Neal2003,
	author = {Radford M. Neal},
	doi = {10.1214/aos/1056562461},
	journal = {The Annals of Statistics},
	keywords = {Adaptive methods, auxiliary variables, dynamical methods, Gibbs sampling, Markov chain Monte Carlo, Metropolis algorithm, overrelaxation},
	number = {3},
	pages = {705 -- 767},
	publisher = {Institute of Mathematical Statistics},
	title = {{Slice sampling}},
	url = {https://doi.org/10.1214/aos/1056562461},
	volume = {31},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1056562461}}
@articleInfo{Leonard2014,
title = {A survey of the Schrödinger problem and some of its  connections with optimal transport},
journal = {Discrete and Continuous Dynamical Systems},
volume = {34},
number = {4},
pages = {1533-1574},
year = {2014},
issn = {1078-0947},
doi = {10.3934/dcds.2014.34.1533},
url = {https://www.aimsciences.org/article/id/d5bcf817-901d-4104-b7da-eade7847c53e},
author = {Christian Léonard},
keywords = {Schrödinger problem, optimal transport, displacement interpolations, Markov measures, relative entropy, large deviations}
}

@article{Eldan+2020,
	author = {Ronen Eldan and Joseph Lehec and Yair Shenfeld},
	doi = {10.1214/19-AIHP1038},
	journal = {Annales de l'Institut Henri Poincar{\'e}, Probabilit{\'e}s et Statistiques},
	keywords = {Quantitative functional inequalities, Stochastic methods},
	number = {3},
	pages = {2253 -- 2269},
	publisher = {Institut Henri Poincar{\'e}},
	title = {{Stability of the logarithmic Sobolev inequality via the F{\"o}llmer process}},
	url = {https://doi.org/10.1214/19-AIHP1038},
	volume = {56},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1214/19-AIHP1038}}

@article{Jamison1975,
	author = {Jamison, Benton},
	date = {1975/12/01},
	date-added = {2024-10-06 21:47:06 +0900},
	date-modified = {2024-10-06 21:47:06 +0900},
	doi = {10.1007/BF00535844},
	id = {Jamison1975},
	isbn = {1432-2064},
	journal = {Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte Gebiete},
	number = {4},
	pages = {323--331},
	title = {The Markov processes of Schr{\"o}dinger},
	url = {https://doi.org/10.1007/BF00535844},
	volume = {32},
	year = {1975},
	bdsk-url-1 = {https://doi.org/10.1007/BF00535844}}

@article{DaiPra1991,
	abstract = {The problem of forcing a nondegenerate diffusion process to a given final configuration is considered. Using the logarithmic transformation approach developed by Fleming, it is shown that the perturbation of the drift suggested by Jamison solves an optimal stochastic control problem. Such perturbation happens to have minimum energy between all controls that ``bring''the diffusion to the desired final distribution. A special property of the change of measure on the path-space that corresponds to the aforesaid perturbation of the drift is also shown.},
	author = {Dai{\ }Pra, Paolo},
	date = {1991/01/01},
	date-added = {2024-10-06 21:48:11 +0900},
	date-modified = {2024-10-06 21:48:11 +0900},
	doi = {10.1007/BF01442404},
	id = {Dai Pra1991},
	isbn = {1432-0606},
	journal = {Applied Mathematics and Optimization},
	number = {1},
	pages = {313--329},
	title = {A stochastic control approach to reciprocal diffusion processes},
	url = {https://doi.org/10.1007/BF01442404},
	volume = {23},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1007/BF01442404}}
@misc{Huang+2021SF,
      title={{Schr{\"o}dinger-F{\"o}llmer Sampler: Sampling without Ergodicity}}, 
      author={Jian Huang and Yuling Jiao and Lican Kang and Xu Liao and Jin Liu and Yanyan Liu},
      year={2021},
      eprint={2106.10880},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2106.10880}, 
}
@misc{Jiao+2021,
      title={Convergence Analysis of Schr{\"o}dinger-F{\"o}llmer Sampler without Convexity}, 
      author={Yuling Jiao and Lican Kang and Yanyan Liu and Youzhou Zhou},
      year={2021},
      eprint={2107.04766},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2107.04766}, 
}

@article{Chopin-Ridgway2017,
	author = {Nicolas Chopin and James Ridgway},
	doi = {10.1214/16-STS581},
	journal = {Statistical Science},
	keywords = {Bayesian computation, expectation propagation, Markov chain Monte Carlo, sequential Monte Carlo, variational inference},
	number = {1},
	pages = {64 -- 87},
	publisher = {Institute of Mathematical Statistics},
	title = {{Leave Pima Indians Alone: Binary Regression as a Benchmark for Bayesian Computation}},
	url = {https://doi.org/10.1214/16-STS581},
	volume = {32},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1214/16-STS581}}

@article{Breiman2001DM,
	author = {Leo Breiman},
	doi = {10.1214/ss/1009213726},
	journal = {Statistical Science},
	number = {3},
	pages = {199 -- 231},
	publisher = {Institute of Mathematical Statistics},
	title = {{Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)}},
	url = {https://doi.org/10.1214/ss/1009213726},
	volume = {16},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1214/ss/1009213726}}
@article{Firth1993,
 ISSN = {00063444},
 URL = {http://www.jstor.org/stable/2336755},
 abstract = {It is shown how, in regular parametric problems, the first-order term is removed from the asymptotic bias of maximum likelihood estimates by a suitable modification of the score function. In exponential families with canonical parameterization the effect is to penalize the likelihood by the Jeffreys invariant prior. In binomial logistic models, Poisson log linear models and certain other generalized linear models, the Jeffreys prior penalty function can be imposed in standard regression software using a scheme of iterative adjustments to the data.},
 author = {David Firth},
 journal = {Biometrika},
 number = {1},
 pages = {27--38},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Bias Reduction of Maximum Likelihood Estimates},
 urldate = {2024-10-07},
 volume = {80},
 year = {1993}
}

@article{Gelman+2008,
	author = {Andrew Gelman and Aleks Jakulin and Maria Grazia Pittau and Yu-Sung Su},
	doi = {10.1214/08-AOAS191},
	journal = {The Annals of Applied Statistics},
	keywords = {Bayesian inference, generalized linear model, hierarchical model, least squares, Linear regression, logistic regression, multilevel model, noninformative prior distribution, weakly informative prior distribution},
	number = {4},
	pages = {1360 -- 1383},
	publisher = {Institute of Mathematical Statistics},
	title = {{A weakly informative default prior distribution for logistic and other regression models}},
	url = {https://doi.org/10.1214/08-AOAS191},
	volume = {2},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1214/08-AOAS191}}

@article{武冨-山本2023,
	author = {武冨奈菜美 and 山本和嬉},
	doi = {10.11329/jjssj.52.69},
	journal = {日本統計学会誌},
	number = {2},
	pages = {69-112},
	title = {生存時間解析・信頼性解析のための統計モデル},
	volume = {52},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.11329/jjssj.52.69}}
@article{Rosin-Rammler1933,
  author = {P. Rosin and E. Rammler},
  year = {1933},
  title = {The law governing the fineness of powdered coal},
  journal = {Journal of the Institute of Fuel},
  volume = {7},
  number = {},
  pages = {29-36},
  url = {}
}

@article{Wei1992,
	abstract = {Abstract For the past two decades the Cox proportional hazards model has been used extensively to examine the covariate effects on the hazard function for the failure time variable. On the other hand, the accelerated failure time model, which simply regresses the logarithm of the survival time over the covariates, has seldom been utilized in the analysis of censored survival data. In this article, we review some newly developed linear regression methods for analysing failure time observations. These procedures have sound theoretical justification and can be implemented with an efficient numerical method. The accelerated failure time model has an intuitive physical interpretation and would be a useful alternative to the Cox model in survival analysis.},
	author = {Wei, L. J.},
	doi = {https://doi.org/10.1002/sim.4780111409},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.4780111409},
	journal = {Statistics in Medicine},
	number = {14-15},
	pages = {1871-1879},
	title = {The accelerated failure time model: A useful alternative to the cox regression model in survival analysis},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780111409},
	volume = {11},
	year = {1992},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780111409},
	bdsk-url-2 = {https://doi.org/10.1002/sim.4780111409}}
@book{Kalnay2002, place={Cambridge}, title={Atmospheric Modeling, Data Assimilation and Predictability}, publisher={Cambridge University Press}, author={Kalnay, Eugenia}, year={2002}}

@inbook{Ghil+1981,
	abstract = {Numerical weather prediction (NWP) is an initial-value problem for a system of nonlinear partial differential equations (PDEs) in which the initial values are known only incompletely and inaccurately. Data at initial time can be supplemented, however, by observations of the system distributed over a time interval preceding it. Estimation theory has been successful in approaching such problems for models governed by systems of ordinary differential equations and of linear PDEs. We develop methods of sequential estimation for NWP.},
	address = {New York, NY},
	author = {Ghil, M. and Cohn, S. and Tavantzis, J. and Bube, K. and Isaacson, E.},
	booktitle = {Dynamic Meteorology: Data Assimilation Methods},
	doi = {10.1007/978-1-4612-5970-1_5},
	editor = {Bengtsson, Lennart and Ghil, Michael and K{\"a}ll{\'e}n, Erland},
	isbn = {978-1-4612-5970-1},
	pages = {139--224},
	publisher = {Springer New York},
	title = {Applications of Estimation Theory to Numerical Weather Prediction},
	url = {https://doi.org/10.1007/978-1-4612-5970-1_5},
	year = {1981},
	bdsk-url-1 = {https://doi.org/10.1007/978-1-4612-5970-1_5}}
@book{Lions1971,
  author = {Jacques Louis Lions},
  year = {1971},
  title = {Optimal Control of Systems Governed by Partial Differential Equations },
  series = {Grundlehren der mathematischen Wissenschaften},
  volume = {170},
  edition = {},
  url = {https://link.springer.com/book/9783642650260},
  publisher = {Springer Berlin, Heidelberg}
}

@article{Talagrand-Coutier1987,
	abstract = {Abstract The following variational approach is taken to the problem of assimilation of meteorological observations: find the solution of the assimilating model which minimizes a given scalar function measuring the `distance' between a model solution and the available observations. It is shown how the `adjoint equations' of the model can be used to compute explicitly the `gradient' of the distance function with respect to the model's initial conditions. the computation of one gradient requires one forward integration of the full model equations over the time interval on which the observations are available, followed by one backward integration of the adjoint equations. Successive gradients thus computed are introduced into a descent algorithm in order to determine the initial conditions which define the minimizing model solution. The theory is applied to the vorticity equation. Successful numerical experiments performed on a Haurwitz wave are described.},
	author = {Talagrand, Olivier and Courtier, Philippe},
	doi = {https://doi.org/10.1002/qj.49711347812},
	eprint = {https://rmets.onlinelibrary.wiley.com/doi/pdf/10.1002/qj.49711347812},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	number = {478},
	pages = {1311-1328},
	title = {Variational Assimilation of Meteorological Observations With the Adjoint Vorticity Equation. I: Theory},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.49711347812},
	volume = {113},
	year = {1987},
	bdsk-url-1 = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/qj.49711347812},
	bdsk-url-2 = {https://doi.org/10.1002/qj.49711347812}}
@article{Brenier1987,
    author = {Yann Brenier},
    year = {1987},
    title = {Decomposition polaire et réarrangement monotone des champs de vecteurs},
    journal = {Comptes Rendus de l'Académie des Sciences - Series I - Mathematics},
    volume = {305},
    number = {19},
    pages = {805-808},
    url = {https://gallica.bnf.fr/ark:/12148/bpt6k57465590/f13.item}
}
@article{Brenier1991,
	abstract = {Abstract Given a probability space (X, μ) and a bounded domain Ω in ℝd equipped with the Lebesgue measure |·| (normalized so that |Ω| = 1), it is shown (under additional technical assumptions on X and Ω) that for every vector-valued function u ∈ Lp (X, μ; ℝd) there is a unique ``polar factorization'' u = ∇Ψs, where Ψ is a convex function defined on Ω and s is a measure-preserving mapping from (X, μ) into (Ω, |·|), provided that u is nondegenerate, in the sense that μ(u−1(E)) = 0 for each Lebesgue negligible subset E of ℝd. Through this result, the concepts of polar factorization of real matrices, Helmholtz decomposition of vector fields, and nondecreasing rearrangements of real-valued functions are unified. The Monge-Amp{\`e}re equation is involved in the polar factorization and the proof relies on the study of an appropriate ``Monge-Kantorovich'' problem.},
	author = {Brenier, Yann},
	doi = {https://doi.org/10.1002/cpa.3160440402},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpa.3160440402},
	journal = {Communications on Pure and Applied Mathematics},
	number = {4},
	pages = {375-417},
	title = {Polar factorization and monotone rearrangement of vector-valued functions},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160440402},
	volume = {44},
	year = {1991},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpa.3160440402},
	bdsk-url-2 = {https://doi.org/10.1002/cpa.3160440402}}

@article{Jordan+1998,
	abstract = { The Fokker--Planck equation, or forward Kolmogorov equation, describes the evolution of the probability density for a stochastic process associated with an Ito stochastic differential equation. It pertains to a wide variety of time-dependent systems in which randomness plays a role. In this paper, we are concerned with Fokker--Planck equations for which the drift term is given by the gradient of a potential. For a broad class of potentials, we construct a time discrete, iterative variational scheme whose solutions converge to the solution of the Fokker--Planck equation. The major novelty of this iterative scheme is that the time-step is governed by the Wasserstein metric on probability measures. This formulation enables us to reveal an appealing, and previously unexplored, relationship between the Fokker--Planck equation and the associated free energy functional. Namely, we demonstrate that the dynamics may be regarded as a gradient flux, or a steepest descent, for the free energy with respect to the Wasserstein metric. },
	author = {Jordan, Richard and Kinderlehrer, David and Otto, Felix},
	doi = {10.1137/S0036141096303359},
	eprint = {https://doi.org/10.1137/S0036141096303359},
	journal = {SIAM Journal on Mathematical Analysis},
	number = {1},
	pages = {1-17},
	title = {The Variational Formulation of the Fokker--Planck Equation},
	url = {https://doi.org/10.1137/S0036141096303359},
	volume = {29},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1137/S0036141096303359}}
@misc{Bernton+2019,
      title={Schr\"odinger Bridge Samplers}, 
      author={Espen Bernton and Jeremy Heng and Arnaud Doucet and Pierre E. Jacob},
      year={2019},
      eprint={1912.13170},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/1912.13170}, 
}
@misc{Andral-Kamatani2024,
      title={Automated Techniques for Efficient Sampling of Piecewise-Deterministic Markov Processes}, 
      author={Charly Andral and Kengo Kamatani},
      year={2024},
      eprint={2408.03682},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/2408.03682}, 
}
@misc{Revels+2016,
      title={Forward-Mode Automatic Differentiation in Julia}, 
      author={Jarrett Revels and Miles Lubin and Theodore Papamarkou},
      year={2016},
      eprint={1607.07892},
      archivePrefix={arXiv},
      primaryClass={cs.MS},
      url={https://arxiv.org/abs/1607.07892}, 
}

@article{Baydin+2017,
author = {Baydin, At\i{}l\i{}m G\"{u}nes and Pearlmutter, Barak A. and Radul, Alexey Andreyevich and Siskind, Jeffrey Mark},
title = {Automatic differentiation in machine learning: a survey},
year = {2017},
issue_date = {January 2017},
publisher = {JMLR.org},
volume = {18},
number = {1},
issn = {1532-4435},
abstract = {Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply "auto-diff", is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational uid dynamics, atmospheric sciences, and engineering design optimization. Until very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other's results. Despite its relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names "dynamic computational graphs" and "differentiable programming". We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main implementation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms "autodiff", "automatic differentiation", and "symbolic differentiation" as these are encountered more and more in machine learning settings.},
journal = {J. Mach. Learn. Res.},
month = jan,
pages = {5595–5637},
numpages = {43},
keywords = {backpropagation, differentiable programming},
url = {http://www.jmlr.org/papers/v18/17-468.html},
}

@article{Giles+2024,
	author = {Giles, D. and Graham, M. M. and Giordano, M. and Koskela, T. and Beskos, A. and Guillas, S.},
	doi = {10.5194/gmd-17-2427-2024},
	journal = {Geoscientific Model Development},
	number = {6},
	pages = {2427--2445},
	title = {ParticleDA.jl v.1.0: a distributed particle-filtering data assimilation package},
	url = {https://gmd.copernicus.org/articles/17/2427/2024/},
	volume = {17},
	year = {2024},
	bdsk-url-1 = {https://gmd.copernicus.org/articles/17/2427/2024/},
	bdsk-url-2 = {https://doi.org/10.5194/gmd-17-2427-2024}}

@article{Zanella2020,
author = {Giacomo Zanella},
title = {Informed Proposals for Local MCMC in Discrete Spaces},
journal = {Journal of the American Statistical Association},
volume = {115},
number = {530},
pages = {852--865},
year = {2020},
publisher = {ASA Website},
doi = {10.1080/01621459.2019.1585255},


URL = { 
    
        https://doi.org/10.1080/01621459.2019.1585255
    
    

},
eprint = { 
    
        https://doi.org/10.1080/01621459.2019.1585255
    
    

}

}
@Article{Liang+2023,
AUTHOR = {Liang, Xitong and Livingstone, Samuel and Griffin, Jim},
TITLE = {Adaptive MCMC for Bayesian Variable Selection in Generalised Linear Models and Survival Models},
JOURNAL = {Entropy},
VOLUME = {25},
YEAR = {2023},
NUMBER = {9},
ARTICLE-NUMBER = {1310},
URL = {https://www.mdpi.com/1099-4300/25/9/1310},
DOI = {10.3390/e25091310}
}
@article{Chevallier+2023,
author = {Augustin Chevallier and Paul Fearnhead and Matthew Sutton},
title = {Reversible Jump PDMP Samplers for Variable Selection},
journal = {Journal of the American Statistical Association},
volume = {118},
number = {544},
pages = {2915--2927},
year = {2023},
publisher = {ASA Website},
doi = {10.1080/01621459.2022.2099402},


URL = { 
    
        https://doi.org/10.1080/01621459.2022.2099402
    
    

},
eprint = { 
    
        https://doi.org/10.1080/01621459.2022.2099402
    
    

}

}
@article{Patz-Junker1999,
 ISSN = {10769986, 19351054},
 URL = {http://www.jstor.org/stable/1165199},
 abstract = {This paper demonstrates Markov chain Monte Carlo (MCMC) techniques that are particularly well-suited to complex models with item response theory (IRT) assumptions. MCMC may be thought of as a successor to the standard practice of first calibrating the items using E-M methods and then taking the item parameters to be known and fixed at their calibrated values when proceeding with inference regarding the latent trait. In contrast to this two-stage E-M approach, MCMC methods treat item and subject parameters at the same time; this allows us to incorporate standard errors of item estimates into trait inferences, and vice versa. We develop a MCMC methodology, based on Metropolis-Hastings sampling, that can be routinely implemented to fit novel IRT models, and we compare the algorithmic features of the Metropolis-Hastings approach to other approaches based on Gibbs sampling. For concreteness we illustrate the methodology using the familiar two-parameter logistic (2PL) IRT model; more complex models are treated in a subsequent paper (Patz & Junker, in press).},
 author = {Richard J. Patz and Brian W. Junker},
 journal = {Journal of Educational and Behavioral Statistics},
 number = {2},
 pages = {146--178},
 publisher = {[American Educational Research Association, Sage Publications, Inc., American Statistical Association]},
 title = {A Straightforward Approach to Markov Chain Monte Carlo Methods for Item Response Models},
 urldate = {2024-11-22},
 volume = {24},
 year = {1999}
}
@article{Bailey2007,
author = {Bailey, Michael A.},
title = {Comparable Preference Estimates across Time and Institutions for the Court, Congress, and Presidency},
journal = {American Journal of Political Science},
volume = {51},
number = {3},
pages = {433-448},
doi = {https://doi.org/10.1111/j.1540-5907.2007.00260.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1540-5907.2007.00260.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-5907.2007.00260.x},
abstract = {Empirically oriented scholars often struggle with how to measure preferences across time and institutional contexts. This article characterizes these difficulties and provides a measurement approach that incorporates information that bridges time and institutions in a Bayesian Markov Chain Monte Carlo approach to ideal point measurement. The resulting preference estimates for presidents, senators, representatives, and Supreme Court justices are comparable across time and institutions. These estimates are useful in a variety of important research projects, including research on statutory interpretation, executive influence on the Supreme Court, and Senate influence on court appointments.},
year = {2007}
}
@article{Chib1998,
title = {Estimation and comparison of multiple change-point models},
journal = {Journal of Econometrics},
volume = {86},
number = {2},
pages = {221-241},
year = {1998},
issn = {0304-4076},
doi = {https://doi.org/10.1016/S0304-4076(97)00115-2},
url = {https://www.sciencedirect.com/science/article/pii/S0304407697001152},
author = {Siddhartha Chib},
}
@article{Gross-Mezard1984,
title = {The simplest spin glass},
journal = {Nuclear Physics B},
volume = {240},
number = {4},
pages = {431-452},
year = {1984},
issn = {0550-3213},
doi = {https://doi.org/10.1016/0550-3213(84)90237-2},
url = {https://www.sciencedirect.com/science/article/pii/0550321384902372},
author = {D.J. Gross and M. Mezard},
}
@article{Kapfer-Krauth2015,
  title = {Two-Dimensional Melting: From Liquid-Hexatic Coexistence to Continuous Transitions},
  author = {Kapfer, Sebastian C. and Krauth, Werner},
  journal = {Phys. Rev. Lett.},
  volume = {114},
  issue = {3},
  pages = {035702},
  numpages = {5},
  year = {2015},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.114.035702},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.114.035702}
}
@article{田邉國士2010,
  title={赤池弘次先生を悼む},
  author={田邉國士},
  journal={応用数理},
  volume={20},
  number={2},
  pages={162-164},
  year={2010},
  doi={10.11540/bjsiam.20.2_162}
}

@article{Horstmann+2018,
	abstract = {Building on the notion that people respond to media as if they were real, switching off a robot which exhibits lifelike behavior implies an interesting situation. In an experimental lab study with a 2x2 between-subjects-design (N = 85), people were given the choice to switch off a robot with which they had just interacted. The style of the interaction was either social (mimicking human behavior) or functional (displaying machinelike behavior). Additionally, the robot either voiced an objection against being switched off or it remained silent. Results show that participants rather let the robot stay switched on when the robot objected. After the functional interaction, people evaluated the robot as less likeable, which in turn led to a reduced stress experience after the switching off situation. Furthermore, individuals hesitated longest when they had experienced a functional interaction in combination with an objecting robot. This unexpected result might be due to the fact that the impression people had formed based on the task-focused behavior of the robot conflicted with the emotional nature of the objection.},
	author = {Horstmann, Aike C. AND Bock, Nikolai AND Linhuber, Eva AND Szczuka, Jessica M. AND Stra{\ss}mann, Carolin AND Kr{\"a}mer, Nicole C.},
	doi = {10.1371/journal.pone.0201581},
	journal = {PLOS ONE},
	month = {07},
	number = {7},
	pages = {1-25},
	publisher = {Public Library of Science},
	title = {Do a robot's social skills and its objection discourage interactants from switching the robot off?},
	url = {https://doi.org/10.1371/journal.pone.0201581},
	volume = {13},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1371/journal.pone.0201581}}
@article{Jakob+2019,
    author = {Jakob, Lea and Garcia-Garzon, Eduardo and Jarke, Hannes and Dablander, Fabian},
    title = {The Science Behind the Magic? The Relation of the Harry Potter “Sorting Hat Quiz” to Personality and Human Values},
    journal = {Collabra: Psychology},
    volume = {5},
    number = {1},
    pages = {31},
    year = {2019},
    month = {07},
    abstract = {The Harry Potter series describes the adventures of a boy and his peers in a fictional world at the “Hogwarts School of Witchcraft and Wizardry”. In the series, pupils get appointed to one of four groups (Houses) at the beginning of their education based on their personality traits. The author of the books has constructed an online questionnaire that allows fans to find out their House affiliation. Crysel, Cook, Schember, and Webster (2015) argued that being sorted into a particular Hogwarts House through the Sorting Hat Quiz is related to empirically established personality traits. We replicated their study while improving on sample size, methods, and analysis. Although our results are similar, effect sizes are small overall, which attenuates the claims by Crysel et al. The effect vanishes when restricting the analysis to participants who desired, but were not sorted into a particular House. On a theoretical level, we extend previous research by also analysing the relation of the Hogwarts Houses to Schwartz’s Basic Human Values but find only moderate or no relations.},
    issn = {2474-7394},
    doi = {10.1525/collabra.240},
    url = {https://doi.org/10.1525/collabra.240},
    eprint = {https://online.ucpress.edu/collabra/article-pdf/5/1/31/468739/240-3358-1-pb.pdf},
}

@article{Solari+2008,
	abstract = {We consider the standard one-way ANOVA model; it is well-known that classical statistical procedures are based on a scalar non-centrality parameter. In this paper we explore both marginal likelihood and integrated likelihood functions for this parameter and we show that they exactly lead to the same answer. On the other hand, we prove that a fully Bayesian testing procedure may provide different conclusions, depending on what is considered to be the real quantity of interest in the model or, said differently, which are the competing hypotheses. We illustrate these issues via a real data example.},
	author = {Solari, Fabrizio and Liseo, Brunero and Sun, Dongchu},
	date = {2008/09/01},
	date-added = {2024-12-08 10:15:20 +0900},
	date-modified = {2024-12-08 10:15:20 +0900},
	doi = {10.1007/s10463-007-0117-5},
	id = {Solari2008},
	isbn = {1572-9052},
	journal = {Annals of the Institute of Statistical Mathematics},
	number = {3},
	pages = {483--498},
	title = {Some remarks on Bayesian inference for one-way ANOVA models},
	url = {https://doi.org/10.1007/s10463-007-0117-5},
	volume = {60},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1007/s10463-007-0117-5}}
@article{Bertolino+1990,
 ISSN = {00390526, 14679884},
 URL = {http://www.jstor.org/stable/2349085},
 abstract = {A likelihood approach to the one-way analysis of variance is presented, based on a marginalization of the results with respect to the variance ratio statistic. The parameter space turns out to be one-dimensional and can be easily explored numerically. This type of analysis is applied to examples taken from the literature. Comparisons with the standard approach and with the use of the profile likelihood are also given, together with a sketch of what can be expected to happen by increasing the number of replicates.},
 author = {Francesco Bertolino and Ludovico Piccinato and Walter Racugno},
 journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
 number = {4},
 pages = {415--424},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {A Marginal Likelihood Approach to Analysis of Variance},
 urldate = {2024-12-07},
 volume = {39},
 year = {1990}
}

@article{永田靖2022,
	author = {永田靖},
	doi = {10.5023/jappstat.51.31},
	journal = {応用統計学},
	number = {1-2},
	pages = {31-42},
	title = {多重比較法の基本的な考え方},
	volume = {51},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.5023/jappstat.51.31}}

@article{岡田謙介2014,
	author = {岡田謙介},
	doi = {10.14947/psychono.KJ00009351488},
	journal = {基礎心理学研究},
	number = {2},
	pages = {223-231},
	title = {ベイズ統計による情報仮設の評価は分散分析にとって代わるのか?(閉じられたANOVAとその先-心理統計の現状と将来を考える,2013年度日本基礎心理学会第1回フォーラム)},
	volume = {32},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.14947/psychono.KJ00009351488}}
@article{Bradley-Terry1952,
 ISSN = {00063444, 14643510},
 URL = {http://www.jstor.org/stable/2334029},
 author = {Ralph Allan Bradley and Milton E. Terry},
 journal = {Biometrika},
 number = {3/4},
 pages = {324--345},
 publisher = {[Oxford University Press, Biometrika Trust]},
 title = {Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons},
 urldate = {2024-12-08},
 volume = {39},
 year = {1952}
}

@article{Hastie-Tibshirani1998,
	author = {Trevor Hastie and Robert Tibshirani},
	doi = {10.1214/aos/1028144844},
	journal = {The Annals of Statistics},
	keywords = {Bradley-Terry model, Pairwise},
	number = {2},
	pages = {451 -- 471},
	publisher = {Institute of Mathematical Statistics},
	title = {{Classification by pairwise coupling}},
	url = {https://doi.org/10.1214/aos/1028144844},
	volume = {26},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1214/aos/1028144844}}
@article{Rao-Kupper1967,
author = {P. V. Rao and L. L. Kupper},
title = {Ties in Paired-Comparison Experiments: A Generalization of the Bradley-Terry Model},
journal = {Journal of the American Statistical Association},
volume = {62},
number = {317},
pages = {194--204},
year = {1967},
publisher = {ASA Website},
doi = {10.1080/01621459.1967.10482901},


URL = { 
    
    
        https://www.tandfonline.com/doi/abs/10.1080/01621459.1967.10482901
    

}}
@article{Davidson-Beaver1977,
 ISSN = {0006341X, 15410420},
 URL = {http://www.jstor.org/stable/2529467},
 abstract = {The Bradley-Terry model for paired comparisons is generalized to account for the effect of the order of presentation of the objects within a pair. A multiplicative order effect is suggested as an alternative to the additive order effect proposed by Beaver and Gokhale (1975). The multiplicative order effect is then incorporated into the tie models given by Rao-Kupper and Davidson (1967, 1970). The associated estimation and testing procedures are presented with emphasis on likelihood methods as well as on the weighted least squares methods recently applied to paired comparison situations by Imrey, Johnson, and Koch (1976) and by Beaver (1977). Two numerical examples are provided, one with ties and one without.},
 author = {Roger R. Davidson and Robert J. Beaver},
 journal = {Biometrics},
 number = {4},
 pages = {693--702},
 publisher = {International Biometric Society},
 title = {On Extending the Bradley-Terry Model to Incorporate Within-Pair Order Effects},
 urldate = {2024-12-08},
 volume = {33},
 year = {1977}
}
@article{Davison-Tsai1992,
 ISSN = {03067734, 17515823},
 URL = {http://www.jstor.org/stable/1403682},
 abstract = {Various diagnostics for generalized linear models are reviewed and extended to more general models. These include some models for censored and grouped data, and regressions that are nonlinear, or where the response does not have an exponential family distribution. Among diagnostics considered are score tests, various types of residual, and approximate Cook statistics. Diagnostics for models with incidental parameters orthogonal to the regression parameters are discussed. Examples are given and the adequacy of approximations is considered. /// Nous passons en revue divers diagnoses pour modèles linéaires généralisés et nous les étendons à des modèles plus généraux. Ceux-ci comprenent quelques modèles pour données censurées et groupées, des régressions non-linéaires, ou des régressions où la réponse n'a pas une distribution faisant partie d'une famille exponentielle. Parmi les diagnoses considérées sont des tests de cote, divers types de résidus, et des statistiques de Cook approximatives. On discute des diagnoses pour des modèles avec paramétres incidentaux orthogonaux aux paramètres de régression. On donne des exemples et on considère l'exactitude des approximations.},
 author = {A. C. Davison and C.-L. Tsai},
 journal = {International Statistical Review / Revue Internationale de Statistique},
 number = {3},
 pages = {337--353},
 publisher = {[Wiley, International Statistical Institute (ISI)]},
 title = {Regression Model Diagnostics},
 urldate = {2024-12-09},
 volume = {60},
 year = {1992}
}
@book{Gelman-Hill2006,
  author = {Andrew Gelman and Jennifer Hill},
  year = {2006},
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  series = {},
  volume = {},
  edition = {},
  url = { https://doi.org/10.1017/CBO9780511790942},
  publisher = {Cambridge University Press}
}
@book{Gelman-Hill-Vehtari2020,
  author = {Andrew Gelman and Jennifer Hill and Aki Vehtari},
  year = {2020},
  title = {Regression and Other Stories},
  series = {},
  volume = {},
  edition = {},
  url = {https://avehtari.github.io/ROS-Examples/},
  publisher = {Cambridge University Press}
}

@article{Muth-Oravecz-Gabry2018,
	abstract = {This tutorial provides a pragmatic introduction to specifying, estimating and interpreting single-level and hierarchical linear regression models in the Bayesian framework. We start by summarizing why one should consider the Bayesian approach to the most common forms of regression. Next we introduce the R package rstanarm for Bayesian applied regression modeling. An overview of rstanarm fundamentals accompanies step-by-step guidance for fitting a single-level regression model with the stan_glm function, and fitting hierarchical regression models with the stan_lmer function, illustrated with data from an experience sampling study on changes in affective states. Exploration of the results is facilitated by the intuitive and user-friendly shinystan package. Data and scripts are available on the Open Science Framework page of the project. For readers unfamiliar with R, this tutorial is self-contained to enable all researchers who apply regression techniques to try these methods with their own data. Regression modeling with the functions in the rstanarm package will be a straightforward transition for researchers familiar with their frequentist counterparts, lm (or glm) and lmer.},
	author = {Muth, Chelsea AND Oravecz, Zita AND Gabry, Jonah},
	doi = {10.20982/tqmp.14.2.p099},
	journal = {The Quantitative Methods for Psychology},
	number = {2},
	pages = {99-119},
	publisher = {TQMP},
	title = {User-friendly Bayesian regression modeling: A tutorial with rstanarm and shinystan},
	url = {http://www.tqmp.org/RegularArticles/vol14-2/p099/p099.pdf},
	volume = {14},
	year = {2018},
	bdsk-url-1 = {http://www.tqmp.org/RegularArticles/vol14-2/p099/p099.pdf},
	bdsk-url-2 = {https://doi.org/10.20982/tqmp.14.2.p099}}

@article{Tversky-Kahneman1973,
	abstract = {This paper explores a judgmental heuristic in which a person evaluates the frequency of classes or the probability of events by availability, i.e., by the ease with which relevant instances come to mind. In general, availability is correlated with ecological frequency, but it is also affected by other factors. Consequently, the reliance on the availability heuristic leads to systematic biases. Such biases are demonstrated in the judged frequency of classes of words, of combinatorial outcomes, and of repeated events. The phenomenon of illusory correlation is explained as an availability bias. The effects of the availability of incidents and scenarios on subjective probability are discussed.},
	author = {Amos Tversky and Daniel Kahneman},
	doi = {https://doi.org/10.1016/0010-0285(73)90033-9},
	issn = {0010-0285},
	journal = {Cognitive Psychology},
	number = {2},
	pages = {207-232},
	title = {Availability: A heuristic for judging frequency and probability},
	url = {https://www.sciencedirect.com/science/article/pii/0010028573900339},
	volume = {5},
	year = {1973},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/0010028573900339},
	bdsk-url-2 = {https://doi.org/10.1016/0010-0285(73)90033-9}}
@article{Kahneman-Tversky1973,
  author = {Daniel Kahneman and Amos Tversky},
  year = {1973},
  title = {On the Psychology of Prediction},
  journal = {Psychological Review},
  volume = {80},
  number = {4},
  pages = {237-251},
  url = {https://doi.org/10.1037/h0034747}
}
@book{Hoff2009,
  author = {Peter D. Hoff},
  year = {2009},
  title = {A First Course in Bayesian Statistical Methods},
  series = {Springer Texts in Statistics},
  volume = {},
  edition = {},
  url = {https://doi.org/10.1007/978-0-387-92407-6},
  publisher = {Springer New York}
}
@article{Stein1956,
  author = {Charles Stein},
  year = {1956},
  title = {Inadmissibility of the Usual Estimator for the Mean of a Multivariate Normal Distribution},
  journal = {Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics},
  volume = {},
  number = {},
  pages = {197-206},
  url = {https://projecteuclid.org/ebooks/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Third-Berkeley-Symposium-on-Mathematical-Statistics-and/chapter/Inadmissibility-of-the-Usual-Estimator-for-the-Mean-of-a/bsmsp/1200501656}
}

@article{Robinson1991,
	author = {G. K. Robinson},
	doi = {10.1214/ss/1177011926},
	journal = {Statistical Science},
	keywords = {Best linear unbiased predition (BLUP), credibility theory, estimation of random effects, fixed versus random effects, foundations of statistics, Kalman filtering, likelihood, parametric empirical Bayes methods, Ranking and selection, selection index, small-area estimation},
	number = {1},
	pages = {15 -- 32},
	publisher = {Institute of Mathematical Statistics},
	title = {{That BLUP is a Good Thing: The Estimation of Random Effects}},
	url = {https://doi.org/10.1214/ss/1177011926},
	volume = {6},
	year = {1991},
	bdsk-url-1 = {https://doi.org/10.1214/ss/1177011926}}

@inbook{Solomon2005,
	abstract = {Abstract Random variation may have a complex structure, with several identifiable sources and corresponding components of variance. In a hierarchical or multilevel model, parameters may change, for example, between individuals, their variation being measured by variance components. This article reviews the history of the study of variance components and describes methods of estimation. Topics include synthesis of variance, empirical Bayes methods, nonnormal models, and model assessment and prediction.},
	author = {Solomon, P. J.},
	booktitle = {Encyclopedia of Biostatistics},
	doi = {https://doi.org/10.1002/0470011815.b2a09056},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/0470011815.b2a09056},
	isbn = {9780470011812},
	keywords = {hierarchical, multilevel, nesting, random effects, unbalanced data, estimating equations, REML, synthesis of variance, empirical Bayes, generalized linear models},
	publisher = {John Wiley & Sons, Ltd},
	title = {Variance Components},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0470011815.b2a09056},
	year = {2005},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0470011815.b2a09056},
	bdsk-url-2 = {https://doi.org/10.1002/0470011815.b2a09056}}

@book{Searle-Casella-McCulloch1992,
  author = {Shayle R. Searle and George Csella and Charles E. McCulloch},
  year = {1992},
  title = {Variance Components},
  series = {Wiley Series in Probability and Statistics},
  volume = {},
  edition = {},
  url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316856},
  publisher = {John Wiley & Sons}
}
@article{Efron-Morris1973,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2284155},
 abstract = {Stein's estimator for k normal means is known to dominate the MLE if k ≥ 3. In this article we ask if Stein's estimator is any good in its own right. Our answer is yes: the positive part version of Stein's estimator is one member of a class of "good" rules that have Bayesian properties and also dominate the MLE. Other members of this class are also useful in various situations. Our approach is by means of empirical Bayes ideas. In the later sections we discuss rules for more complicated estimation problems, and conclude with results from empirical linear Bayes rules in non-normal cases.},
 author = {Bradley Efron and Carl Morris},
 journal = {Journal of the American Statistical Association},
 number = {341},
 pages = {117--130},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Stein's Estimation Rule and Its Competitors--An Empirical Bayes Approach},
 urldate = {2024-12-11},
 volume = {68},
 year = {1973}
}
@article{Chib-Winkelmann2001,
author = {Siddhartha Chib and Rainer Winkelmann},
title = {Markov Chain Monte Carlo Analysis of Correlated Count Data},
journal = {Journal of Business \& Economic Statistics},
volume = {19},
number = {4},
pages = {428--435},
year = {2001},
publisher = {ASA Website},
doi = {10.1198/07350010152596673},


URL = { 
    
        https://doi.org/10.1198/07350010152596673
    
    

},
}
@article{Quinn2004, title={Bayesian Factor Analysis for Mixed Ordinal and Continuous Responses}, volume={12}, DOI={10.1093/pan/mph022}, number={4}, journal={Political Analysis}, author={Quinn, Kevin M.}, year={2017}, pages={338–353}}


@inbook{Mudholkar2014,
	author = {Mudholkar, Govind S.},
	booktitle = {Wiley StatsRef: Statistics Reference Online},
	doi = {https://doi.org/10.1002/9781118445112.stat00641},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118445112.stat00641},
	isbn = {9781118445112},
	publisher = {John Wiley & Sons, Ltd},
	title = {Multiple Correlation Coefficient},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat00641},
	year = {2014},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat00641},
	bdsk-url-2 = {https://doi.org/10.1002/9781118445112.stat00641}}
@article{Gelman+2019,
author = {Andrew Gelman, Ben Goodrich, Jonah Gabry and Aki Vehtari},
title = {R-squared for Bayesian Regression Models},
journal = {The American Statistician},
volume = {73},
number = {3},
pages = {307--309},
year = {2019},
publisher = {ASA Website},
doi = {10.1080/00031305.2018.1549100},


URL = { 
    
        https://doi.org/10.1080/00031305.2018.1549100
    
    

},
}

@article{Vehtari+2017,
	abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	date = {2017/09/01},
	date-added = {2024-12-12 13:04:17 +0900},
	date-modified = {2024-12-12 13:04:17 +0900},
	doi = {10.1007/s11222-016-9696-4},
	id = {Vehtari2017},
	isbn = {1573-1375},
	journal = {Statistics and Computing},
	number = {5},
	pages = {1413--1432},
	title = {Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC},
	url = {https://doi.org/10.1007/s11222-016-9696-4},
	volume = {27},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1007/s11222-016-9696-4}}
@article{Stone1974,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2984809},
 abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescription. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
 author = {M. Stone},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {2},
 pages = {111--147},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Cross-Validatory Choice and Assessment of Statistical Predictions},
 urldate = {2024-12-11},
 volume = {36},
 year = {1974}
}
@misc{Vehtari+2015,
      title={Pareto Smoothed Importance Sampling}, 
      author={Aki Vehtari and Daniel Simpson and Andrew Gelman and Yuling Yao and Jonah Gabry},
      year={2024},
      eprint={1507.02646},
      archivePrefix={arXiv},
      primaryClass={stat.CO},
      url={https://arxiv.org/abs/1507.02646}, 
}

@article{Piironen-Vehtari2017,
	author = {Juho Piironen and Aki Vehtari},
	doi = {10.1214/17-EJS1337SI},
	journal = {Electronic Journal of Statistics},
	keywords = {Bayesian inference, horseshoe prior, shrinkage priors, Sparse estimation},
	number = {2},
	pages = {5018 -- 5051},
	publisher = {Institute of Mathematical Statistics and Bernoulli Society},
	title = {{Sparsity information and regularization in the horseshoe and other shrinkage priors}},
	url = {https://doi.org/10.1214/17-EJS1337SI},
	volume = {11},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1214/17-EJS1337SI}}

@InProceedings{Piironen-Vehtari2017Choice,
  title = 	 {{On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe Prior}},
  author = 	 {Piironen, Juho and Vehtari, Aki},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {905--913},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/piironen17a/piironen17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/piironen17a.html},
  abstract = 	 {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but as shown in this paper, the results can be sensitive to the prior choice for the global shrinkage hyperparameter. We argue that the previous default choices are dubious due to their tendency to favor solutions with more unshrunk coefficients than we typically expect a priori. This can lead to bad results if this parameter is not strongly identified by data. We derive the relationship between the global parameter and the effective number of nonzeros in the coefficient vector, and show an easy and intuitive way of setting up the prior for the global parameter based on our prior beliefs about the number of nonzero coefficients in the model. The results on real world data show that one can benefit greatly – in terms of improved parameter estimates, prediction accuracy, and reduced computation time – from transforming even a crude guess for the number of nonzero coefficients into the prior for the global parameter using our framework.}
}

@article{George-McCulloch1993,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2290777},
 abstract = {A crucial problem in building a multiple regression model is the selection of predictors to include. The main thrust of this article is to propose and develop a procedure that uses probabilistic considerations for selecting promising subsets. This procedure entails embedding the regression setup in a hierarchical normal mixture model where latent variables are used to identify subset choices. In this framework the promising subsets of predictors can be identified as those with higher posterior probability. The computational burden is then alleviated by using the Gibbs sampler to indirectly sample from this multinomial posterior distribution on the set of possible subset choices. Those subsets with higher probability--the promising ones--can then be identified by their more frequent appearance in the Gibbs sample.},
 author = {Edward I. George and Robert E. McCulloch},
 journal = {Journal of the American Statistical Association},
 number = {423},
 pages = {881--889},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Variable Selection Via Gibbs Sampling},
 urldate = {2024-12-12},
 volume = {88},
 year = {1993}
}
@article{Carlin+2001,
    author = {Carlin, John B. and Wolfe, Rory and Brown, C. Hendricks and Gelman, Andrew},
    title = {A case study on the choice, interpretation and checking of multilevel models for longitudinal binary outcomes},
    journal = {Biostatistics},
    volume = {2},
    number = {4},
    pages = {397-416},
    year = {2001},
    month = {12},
    abstract = {Recent advances in statistical software have led to the rapid diffusion of new methods for modelling longitudinal data. Multilevel (also known as hierarchical or random effects) models for binary outcomes have generally been based on a logistic–normal specification, by analogy with earlier work for normally distributed data. The appropriate application and interpretation of these models remains somewhat unclear, especially when compared with the computationally more straightforward semiparametric or ‘marginal’ modelling (GEE) approaches. In this paper we pose two interrelated questions. First, what limits should be placed on the interpretation of the coefficients and inferences derived from random‐effect models involving binary outcomes? Second, what diagnostic checks are appropriate for evaluating whether such random‐effect models provide adequate fits to the data? We address these questions by means of an extended case study using data on adolescent smoking from a large cohort study. Bayesian estimation methods are used to fit a discrete‐mixture alternative to the standard logistic–normal model, and posterior predictive checking is used to assess model fit. Surprising parallels in the parameter estimates from the logistic–normal and mixture models are described and used to question the interpretability of the socalled ‘subject‐specific’ regression coefficients from the standard multilevel approach. Posterior predictive checks suggest a serious lack of fit of both multilevel models. The results do not provide final answers to the two questions posed, but we expect that lessons learned from the case study will provide general guidance for further investigation of these important issues.},
    issn = {1465-4644},
    doi = {10.1093/biostatistics/2.4.397},
    url = {https://doi.org/10.1093/biostatistics/2.4.397},
    eprint = {https://academic.oup.com/biostatistics/article-pdf/2/4/397/916134/020397.pdf},
}
@book{菅澤-久保川2023,
  author = {Shonosuke Sugasawa and Tatsuya Kubokawa},
  year = {2023},
  title = {Mixed-Effects Models and Small Area Estimation},
  series = {},
  volume = {},
  edition = {},
  url = {https://doi.org/10.1007/978-981-19-9486-9},
  publisher = {Springer Singapore}
}
@article{Henderson1950,
  author = {C. R. Henderson},
  year = {1950},
  title = {Estimation of Genetic Parameters},
  journal = {The Annals of Mathematical Statistics},
  volume = {21},
  number = {2},
  pages = {302-315},
  url = {https://www.jstor.org/stable/2236913}
}
@article{Lindley-Smith1972,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2985048},
 abstract = {The usual linear statistical model is reanalyzed using Bayesian methods and the concept of exchangeability. The general method is illustrated by applications to two-factor experimental designs and multiple regression.},
 author = {D. V. Lindley and A. F. M. Smith},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {1--41},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Bayes Estimates for the Linear Model},
 urldate = {2024-12-12},
 volume = {34},
 year = {1972}
}
@article{Fay-Herriot1979,
 ISSN = {01621459, 1537274X},
 URL = {http://www.jstor.org/stable/2286322},
 abstract = {An adaptation of the James-Stein estimator is applied to sample estimates of income for small places (i.e., population less than 1,000) from the 1970 Census of Population and Housing. The adaptation incorporates linear regression in the context of unequal variances. Evidence is presented that the resulting estimates have smaller average error than either the sample estimates or an alternate procedure of using county averages. The new estimates for these small places now form the basis for the Census Bureau's updated estimates of per capita income for the General Revenue Sharing Program.},
 author = {Robert E. Fay and Roger A. Herriot},
 journal = {Journal of the American Statistical Association},
 number = {366},
 pages = {269--277},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Estimates of Income for Small Places: An Application of James-Stein Procedures to Census Data},
 urldate = {2024-12-12},
 volume = {74},
 year = {1979}
}

@article{Gelman-Rubin1992,
	author = {Andrew Gelman and Donald B. Rubin},
	doi = {10.1214/ss/1177011136},
	journal = {Statistical Science},
	keywords = {Bayesian inference, Convergence of stochastic processes, ECM, EM, Gibbs sampler, importance sampling, Metropolis algorithm, multiple imputation, random-effects model, SIR},
	number = {4},
	pages = {457 -- 472},
	publisher = {Institute of Mathematical Statistics},
	title = {{Inference from Iterative Simulation Using Multiple Sequences}},
	url = {https://doi.org/10.1214/ss/1177011136},
	volume = {7},
	year = {1992},
	bdsk-url-1 = {https://doi.org/10.1214/ss/1177011136}}
@book{Boeck-Wilson2004,
  author = {Pual Boeck and Mark Wilson},
  year = {2004},
  title = {Explanatory Item Response Models: A Generalized Linear and Nonlinear Approach},
  series = {},
  volume = {},
  edition = {},
  url = {https://doi.org/10.1007/978-1-4757-3990-9},
  publisher = {Springer New York}
}
@article{Bates+2015,
 title={Fitting Linear Mixed-Effects Models Using lme4},
 volume={67},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v067i01},
 doi={10.18637/jss.v067.i01},
 abstract={Maximum likelihood or restricted maximum likelihood (REML) estimates of the parameters in linear mixed-effects models can be determined using the lmer function in the lme4 package for R. As for most model-fitting functions in R, the model is described in an lmer call by a formula, in this case including both fixed- and random-effects terms. The formula and data together determine a numerical representation of the model from which the profiled deviance or the profiled REML criterion can be evaluated as a function of some of the model parameters. The appropriate criterion is optimized, using one of the constrained optimization functions in R, to provide the parameter estimates. We describe the structure of the model, the steps in evaluating the profiled deviance or REML criterion, and the structure of classes or types that represents such a model. Sufficient detail is included to allow specialization of these structures by users who wish to write functions to fit specialized linear mixed models, such as models incorporating pedigrees or smoothing splines, that are not easily expressible in the formula language used by lmer.},
 number={1},
 journal={Journal of Statistical Software},
 author={Bates, Douglas and Mächler, Martin and Bolker, Ben and Walker, Steve},
 year={2015},
 pages={1–48}
}
@article{Gronau-Singmann-Wagenmakers2020,
 title={bridgesampling: An R Package for Estimating Normalizing Constants},
 volume={92},
 url={https://www.jstatsoft.org/index.php/jss/article/view/v092i10},
 doi={10.18637/jss.v092.i10},
 abstract={Statistical procedures such as Bayes factor model selection and Bayesian model averaging require the computation of normalizing constants (e.g., marginal likelihoods). These normalizing constants are notoriously difficult to obtain, as they usually involve highdimensional integrals that cannot be solved analytically. Here we introduce an R package that uses bridge sampling (Meng and Wong 1996; Meng and Schilling 2002) to estimate normalizing constants in a generic and easy-to-use fashion. For models implemented in Stan, the estimation procedure is automatic. We illustrate the functionality of the package with three examples.},
 number={10},
 journal={Journal of Statistical Software},
 author={Gronau, Quentin F. and Singmann, Henrik and Wagenmakers, Eric-Jan},
 year={2020},
 pages={1–29}
}
@article{McCullagh1980,
author = {McCullagh, Peter},
title = {Regression Models for Ordinal Data},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume = {42},
number = {2},
pages = {109-127},
keywords = {complementary log—log transform, generalized empirical logit transform, link function, location parameter, log-linear model, logit linear model, multivariate generalized linear model, ordered categories, proportional hazards, proportional odds, scale parameter, scores, survivor function},
doi = {https://doi.org/10.1111/j.2517-6161.1980.tb01109.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1980.tb01109.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1980.tb01109.x},
abstract = {Summary A general class of regression models for ordinal data is developed and discussed. These models utilize the ordinal nature of the data by describing various modes of stochastic ordering and this eliminates the need for assigning scores or otherwise assuming cardinality instead of ordinality. Two models in particular, the proportional odds and the proportional hazards models are likely to be most useful in practice because of the simplicity of their interpretation. These linear models are shown to be multivariate extensions of generalized linear models. Extensions to non-linear models are discussed and it is shown that even here the method of iteratively reweighted least squares converges to the maximum likelihood estimate, a property which greatly simplifies the necessary computation. Applications are discussed with the aid of examples.},
year = {1980}
}
@phdthesis{Vansteelandt2001,
  author = {Kristof Vansteelandt},
  school = {KU Leuven},
  title = {Formal models for contextualized personality psychology},
  year = {2001},
  url = {https://lirias.kuleuven.be/1784887&lang=en},
}

@inbook{Spielberger2010,
	abstract = {Abstract In 1872, Darwin (1965) observed that anger (rage) was a powerful emotion that motivated ``animals of all kinds, and their progenitors before them, when attacked or threatened by an enemy, to fight and protect themselves'' (p. 74). Anger is reflected in facial expressions (e.g., reddened face, clenched teeth), muscular tension, and accelerated heart rate, and it differs from rage ``only in degree, and there is no marked distinction in their characteristics'' (Darwin, 1965, p. 244).},
	author = {Spielberger, Charles D.},
	booktitle = {The Corsini Encyclopedia of Psychology},
	doi = {https://doi.org/10.1002/9780470479216.corpsy0942},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470479216.corpsy0942},
	isbn = {9780470479216},
	keywords = {anger, aggression, hostility, STAXI, emotion, personality, anger expression, anger control},
	pages = {1-1},
	publisher = {John Wiley & Sons, Ltd},
	title = {State-Trait Anger Expression Inventory},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470479216.corpsy0942},
	year = {2010},
	bdsk-url-1 = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470479216.corpsy0942},
	bdsk-url-2 = {https://doi.org/10.1002/9780470479216.corpsy0942}}
@article{熊谷龍一2012,
  title={統合的DIF検出方法の提案──“EasyDIF”の開発──},
  author={熊谷 龍一},
  journal={心理学研究},
  volume={83},
  number={1},
  pages={35-43},
  year={2012},
  doi={10.4992/jjpsy.83.35}
}

@article{Barr+2013,
	abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the `gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond.},
	author = {Dale J. Barr and Roger Levy and Christoph Scheepers and Harry J. Tily},
	doi = {https://doi.org/10.1016/j.jml.2012.11.001},
	issn = {0749-596X},
	journal = {Journal of Memory and Language},
	keywords = {Linear mixed-effects models, Generalization, Statistics, Monte Carlo simulation},
	number = {3},
	pages = {255-278},
	title = {Random effects structure for confirmatory hypothesis testing: Keep it maximal},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X12001180},
	volume = {68},
	year = {2013},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0749596X12001180},
	bdsk-url-2 = {https://doi.org/10.1016/j.jml.2012.11.001}}
@article{Wu+2024,
    doi = {10.1371/journal.pone.0298610},
    author = {Wu, Yuxiang AND Ma, Weiwei AND Cheng, Zhenda AND Zhang, Qiwei AND Li, Zhaodong AND Weng, Punan AND Li, Bushuang AND Huang, Zhiqiang AND Fu, Changlong},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Causal relationships between body mass index, low-density lipoprotein and bone mineral density: Univariable and multivariable Mendelian randomization},
    year = {2024},
    month = {06},
    volume = {19},
    url = {https://doi.org/10.1371/journal.pone.0298610},
    pages = {1-14},
    abstract = {Summary Utilizing the Mendelian randomization technique, this research clarifies the putative causal relationship between body mass index (BMI) andbone mineral density (BMD), and the mediating role of low-density lipoprotein (LDL). The implications of these findings present promising opportunities for enhancing our understanding of complex bone-related characteristics and disorders, offering potential directions for treatment and intervention.   Objective The objective of this study is to examine the correlation between BMI and BMD, while exploring the intermediary role of LDL in mediating the causal impact of BMI on BMD outcomes via Mendelian randomization.   Methods In this study, we employed genome-wide association study (GWAS) data on BMI, LDL, and BMD to conduct a comparative analysis using both univariate and multivariate Mendelian randomization.   Results Our study employed a two-sample Mendelian randomization design. Considering BMI as the exposure and BMD as the outcome, our results suggest that BMI may function as a potential protective factor for BMD (β = 0.05, 95% CI 1.01 to 1.09, P = 0.01). However, when treating LDL as the exposure and BMD as the outcome, our findings indicate LDL as a risk factor for BMD (β = -0.04, 95% CI 0.92 to 0.99, P = 0.04). In our multivariate Mendelian randomization (MVMR) model, the combined influence of BMI and LDL was used as the exposure for BMD outcomes. The analysis pointed towards a substantial protective effect of LDL on BMD (β = 0.08, 95% CI 0.85 to 0.97, P = 0.006). In the analysis of mediation effects, LDL was found to mediate the relationship between BMI and BMD, and the effect was calculated at (β = 0.05, 95% CI 1.052 to 1.048, P = 0.04).   Conclusion Our findings suggest that BMI may be considered a protective factor for BMD, while LDL may act as a risk factor. Moreover, LDL appears to play a mediatory role in the causal influence of BMI on BMD.},
    number = {6},

}

@article{Okamura+2013,
	annote = {doi: 10.1016/j.atherosclerosis.2013.04.023},
	author = {Okamura, Tomonori and Sekikawa, Akira and Sawamura, Tatsuya and Kadowaki, Takashi and Barinas-Mitchell, Emma and Mackey, Rachel H. and Kadota, Aya and Evans, Rhobert W. and Edmundowicz, Daniel and Higashiyama, Aya and Nakamura, Yasuyuki and Abbott, Robert D. and Miura, Katsuyuki and Fujiyoshi, Akira and Fujita, Yoshiko and Murakami, Yoshitaka and Miyamatsu, Naomi and Kakino, Akemi and Maegawa, Hiroshi and Murata, Kiyoshi and Horie, Minoru and Mitsunami, Kenichi and Kashiwagi, Atsunori and Kuller, Lewis H. and Ueshima, Hirotsugu},
	date = {2013/07/01},
	date-added = {2024-12-15 11:48:21 +0900},
	date-modified = {2024-12-15 11:48:21 +0900},
	doi = {10.1016/j.atherosclerosis.2013.04.023},
	isbn = {0021-9150},
	journal = {Atherosclerosis},
	journal1 = {Atherosclerosis},
	month = {2024/12/14},
	number = {1},
	pages = {240--245},
	publisher = {Elsevier},
	title = {LOX-1 ligands containing apolipoprotein B and carotid intima-media thickness in middle-aged community-dwelling US Caucasian and\&{\#}xa0;Japanese men},
	type = {doi: 10.1016/j.atherosclerosis.2013.04.023},
	url = {https://doi.org/10.1016/j.atherosclerosis.2013.04.023},
	volume = {229},
	year = {2013},
	year1 = {2013},
	bdsk-url-1 = {https://doi.org/10.1016/j.atherosclerosis.2013.04.023}}
@article{Inoue+2010,
    author = {Inoue, Nobutaka and Okamura, Tomonori and Kokubo, Yoshihiro and Fujita, Yoshiko and Sato, Yuko and Nakanishi, Mamoru and Yanagida, Kazuki and Kakino, Akemi and Iwamoto, Shin and Watanabe, Makoto and Ogura, Sayoko and Otsui, Kazunori and Matsuda, Haruo and Uchida, Kagehiro and Yoshimoto, Ryo and Sawamura, Tatsuya},
    title = {LOX Index, a Novel Predictive Biochemical Marker for Coronary Heart Disease and Stroke},
    journal = {Clinical Chemistry},
    volume = {56},
    number = {4},
    pages = {550-558},
    year = {2010},
    month = {04},
    abstract = {Background: Lectin-like oxidized LDL receptor 1 (LOX-1) is implicated in atherothrombotic diseases. Activation of LOX-1 in humans can be evaluated by use of the LOX index, obtained by multiplying the circulating concentration of LOX-1 ligands containing apolipoprotein B (LAB) times that of the soluble form of LOX-1 (sLOX-1) [LOX index = LAB × sLOX-1]. This study aimed to establish the prognostic value of the LOX index for coronary heart disease (CHD) and stroke in a community-based cohort.Methods: An 11-year cohort study of 2437 residents age 30–79 years was performed in an urban area located in Japan. Of these, we included in the analysis 1094 men and 1201 women without history of stroke and CHD. We measured LAB and sLOX-1 using ELISAs with recombinant LOX-1 and monoclonal anti–apolipoprotein B antibody and with 2 monoclonal antibodies against LOX-1, respectively.Results: During the follow-up period, there were 68 incident cases of CHD and 91 cases of stroke (with 60 ischemic strokes). Compared with the bottom quartile, the hazard ratio (HR) of the top quartile of LOX index was 1.74 (95\% CI 0.92–3.30) for stroke and 2.09 (1.00–4.35) for CHD after adjusting for sex, age, body mass index, drinking, smoking, hypertension, diabetes, non-HDL cholesterol, and use of lipid-lowering agents. Compared with the bottom quartile of LOX index, the fully adjusted HRs for ischemic stroke were consistently high from the second to the top quartile: 3.39 (95\% CI 1.34–8.53), 3.15 (1.22–8.13) and 3.23 (1.24–8.37), respectively.Conclusions: Higher LOX index values were associated with an increased risk of CHD. Low LOX index values may be protective against ischemic stroke.},
    issn = {0009-9147},
    doi = {10.1373/clinchem.2009.140707},
    url = {https://doi.org/10.1373/clinchem.2009.140707},
    eprint = {https://academic.oup.com/clinchem/article-pdf/56/4/550/32653087/clinchem0550.pdf},
}
@article{Hussai+2019, title={Correlation between Body Mass Index and Lipid Profile in patients with Type 2 Diabetes attending a tertiary care hospital in Peshawar: Correlation between BMI &amp; lipid profile in patients with T2DM}, volume={35}, url={https://www.pjms.org.pk/index.php/pjms/article/view/7}, DOI={10.12669/pjms.35.3.7}, abstractNote={&lt;p&gt;&lt;em&gt;&lt;strong&gt;Objective:&lt;/strong&gt; &lt;/em&gt;To determine the correlation between body mass index (BMI) and lipid profile in patients with Type 2 Diabetes Mellitus (T2DM) attending a tertiary care hospital in Peshawar.&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Methods:&lt;/strong&gt;&lt;/em&gt; A total of 305 patients (men, 132; women, 173) with T2DM visiting an Outpatient department in Northwest General Hospital and Research Centre, Peshawar from January 2016 to July 2016 were included in this study. The whole blood and sera were analyzed for Glycated hemoglobin (HbA1c), total cholesterol (TC), triglyceride (TGs), high density lipoprotein cholesterol (HDL-C) and low density lipoprotein cholesterol (LDL-C).The correlation of BMI with lipid ratios and individual lipid indices were analysed.&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Results:&lt;/strong&gt;&lt;/em&gt; Mean of BMI was 29.29±5.23. Dyslipidemia; increased TC, increased LDL-C, increased triglyceride and decreased HDL-C were noted in 40.7%, 54.1%, 69.5% and 41% respectively. The&amp;nbsp;mean difference of LDL-C (p=0.006) was significant between male and female. BMI, TC, TGs, and LDL-C showed no significant correlation where as a significant negative correlation between BMI and HDL-C was observed(r=-0.125, p=0.029, R2=0.016). The mean values of TC, TG, LDL-C, TC/ HDL-C and LDL-C/HDL-C ratios were greater in patients with normal BMI compared to overweight and obese; however, the differences were not significant. HDL-C differed significantly in BMI groups (p=0.040).&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/em&gt; A significant negative correlation between BMI and HDL-C was observed, while the correlation between BMI and LDL-C was observed to be insignificant. HDL-C was found significantly higher in patients with normal BMI. These results are important to indicate that there is modest impact of BMI on lipid profile. Therefore, assessment and management for altered blood lipids should not be based on a patient’s body weight or BMI.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;doi: https://doi.org/10.12669/pjms.35.3.7&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;&lt;em&gt;&lt;strong&gt;How to cite this:&lt;/strong&gt;&lt;/em&gt;&lt;br&gt;Hussain A, Ali I, Kaleem WA, Yasmeen F. Correlation between Body Mass Index and Lipid Profile in patients with Type 2 Diabetes attending a tertiary care hospital in Peshawar. Pak J Med Sci. 2019;35(3):591-597. &lt;br&gt;doi: https://doi.org/10.12669/pjms.35.3.7&lt;/p&gt; &lt;p&gt;This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/3.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.&lt;/p&#38;gt;}, number={3}, journal={Pakistan Journal of Medical Sciences}, author={Hussain, Arshad and Ali, iftikhar and Kaleem, Waqar Ahmad and Yasmeen, Fatima}, year={2019}, month={May} }

@article{Li+2021,
	abstract = {The relationship between body mass index (BMI) and low-density lipoprotein cholesterol (LDL-C) has not been clearly elucidated in middle-aged and older adults. This study aimed to evaluate the non-linear dose-response relationship between BMI and LDL-C in males and females.},
	author = {Li, Haibin and Ma, Jiahui and Zheng, Deqiang and Li, Xia and Guo, Xiuhua and Wang, Jing and Su, Pixiong},
	date = {2021/11/14},
	date-added = {2024-12-15 12:05:03 +0900},
	date-modified = {2024-12-15 12:05:03 +0900},
	doi = {10.1186/s12944-021-01591-w},
	id = {Li2021},
	isbn = {1476-511X},
	journal = {Lipids in Health and Disease},
	number = {1},
	pages = {162},
	title = {Sex differences in the non-linear association between BMI and LDL cholesterol in middle-aged and older adults: findings from two nationally representative surveys in China},
	url = {https://doi.org/10.1186/s12944-021-01591-w},
	volume = {20},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1186/s12944-021-01591-w}}

@article{Park-Casella2008,
	annote = {doi: 10.1198/016214508000000337},
	author = {Park ,Trevor and Casella ,George},
	date = {2008/06/01},
	date-added = {2024-12-17 12:36:36 +0900},
	date-modified = {2024-12-17 12:36:36 +0900},
	doi = {10.1198/016214508000000337},
	isbn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	journal1 = {Journal of the American Statistical Association},
	journal2 = {Journal of the American Statistical Association},
	month = {06},
	number = {482},
	pages = {681--686},
	publisher = {ASA Website},
	title = {The Bayesian Lasso},
	type = {doi: 10.1198/016214508000000337},
	url = {https://doi.org/10.1198/016214508000000337},
	volume = {103},
	year = {2008},
	year1 = {2008},
	bdsk-url-1 = {https://doi.org/10.1198/016214508000000337}}

@article{Hahn-Carvalho2015,
	annote = {doi: 10.1080/01621459.2014.993077},
	author = {Hahn ,P. Richard and Carvalho ,Carlos M.},
	date = {2015/01/02},
	date-added = {2024-12-17 16:26:35 +0900},
	date-modified = {2024-12-17 16:26:35 +0900},
	doi = {10.1080/01621459.2014.993077},
	isbn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	journal1 = {Journal of the American Statistical Association},
	journal2 = {Journal of the American Statistical Association},
	month = {01},
	number = {509},
	pages = {435--448},
	publisher = {ASA Website},
	title = {Decoupling Shrinkage and Selection in Bayesian Linear Models: A Posterior Summary Perspective},
	type = {doi: 10.1080/01621459.2014.993077},
	url = {https://doi.org/10.1080/01621459.2014.993077},
	volume = {110},
	year = {2015},
	year1 = {2015},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2014.993077}}

@article{Griffin-Brown2017,
	author = {Jim E. Griffin and Phil Brown},
	doi = {10.1214/15-BA990},
	journal = {Bayesian Analysis},
	keywords = {Bayesian regularization, generalized additive models, generalized beta mixture prior, interactions, normal-gamma prior, normal-gamma-gamma prior, strong and weak heredity, structured priors},
	number = {1},
	pages = {135 -- 159},
	publisher = {International Society for Bayesian Analysis},
	title = {{Hierarchical Shrinkage Priors for Regression Models}},
	url = {https://doi.org/10.1214/15-BA990},
	volume = {12},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1214/15-BA990}}
@unpublished{Chan-Tobias2020,
  author = {Joshua Chan and Justin L. Tobias},
  year = {2020},
  title = {Bayesian Econometric Methods},
  url = {https://joshuachan.org/papers/BayesMicroeconometrics.pdf}
}
@article{Walker-Duncan1967,
    author = {Strother H. Walker and David B. Duncan},
    title = {Estimation of the probability of an event as a function of several independent variables},
    journal = {Biometrika},
    volume = {54},
    number = {1-2},
    pages = {167-179},
    year = {1967},
    month = {06},
    abstract = {A method for estimating the probability of occurrence of an event from dichotomous or polychotomous data is developed, using a recursive approach. The method in the dichotomous case is applied to the data of a 10-year prospective study of coronary disease. Other areas of application are briefly indicated.},
    issn = {0006-3444},
    doi = {10.1093/biomet/54.1-2.167},
    url = {https://doi.org/10.1093/biomet/54.1-2.167},
    eprint = {https://academic.oup.com/biomet/article-pdf/54/1-2/167/947300/54-1-2-167.pdf},
}

@inbook{Liu2004-robit,
author = {Liu, Chuanhai},
publisher = {John Wiley & Sons, Ltd},
isbn = {9780470090459},
title = {Robit Regression: A Simple Robust Alternative to Logistic and Probit Regression},
booktitle = {Applied Bayesian Modeling and Causal Inference from Incomplete‐Data Perspectives},
chapter = {21},
pages = {227-238},
doi = {https://doi.org/10.1002/0470090456.ch21},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0470090456.ch21},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/0470090456.ch21},
year = {2004},
keywords = {logistic and probit regression, robit regression, EM-type algorithms, PX-EM algorithm, ECME algorithmk1, resistant fitting methods, binary response data, t-distribution},
abstract = {Summary This chapter contains sections titled: Introduction The robit model Robustness of likelihood-based inference using logistic, probit, and robit regression models Complete data for simple maximum likelihood estimation Maximum likelihood estimation using EM-type algorithms A numerical example Conclusion}
}
@book{Lord1980,
  author = {Frederic M. Lord},
  year = {1980},
  title = {Applications of Item Response Theory to Practical Testing Problems},
  series = {},
  volume = {},
  edition = {},
  url = {https://doi.org/10.4324/9780203056615},
  publisher = {Routledge}
}
@article{Griffin-Brown2021,
title = {Bayesian global-local shrinkage methods for regularisation in the high dimension linear model},
journal = {Chemometrics and Intelligent Laboratory Systems},
volume = {210},
pages = {104255},
year = {2021},
issn = {0169-7439},
doi = {https://doi.org/10.1016/j.chemolab.2021.104255},
url = {https://www.sciencedirect.com/science/article/pii/S016974392100023X},
author = {Jim E. Griffin and Philip J. Brown},
keywords = {Regularisation, Linear model, High dimensional, Fast algorithms., Drug discovery, Bayesian shrinkage priors},
abstract = {This paper reviews global-local prior distributions for Bayesian inference in high-dimensional regression problems including important properties of priors and efficient Markov chain Monte Carlo methods for inference. A chemometric example in drug discovery is used to compare the predictive performance of these methods with popular methods such as Ridge and LASSO regression.}
}
@article{Plackett1975,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2346567},
 abstract = {A probability distribution is defined over the r! permutations of r objects in such a way as to incorporate up to r! - 1 parameters. Problems of estimation and testing are considered. The results are applied to data on voting at elections and beanstores.},
 author = {R. L. Plackett},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {2},
 pages = {193--202},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {The Analysis of Permutations},
 urldate = {2024-12-25},
 volume = {24},
 year = {1975}
}
@book{Luce1959,
  author = {R. D. Luce},
  year = {1959},
  title = {Individual Choice Behavior},
  series = {},
  volume = {},
  edition = {},
  url = {},
  publisher = {John Wiley}
}
@article{Henderson2024,
author = {Daniel A. Henderson},
title = {{Modelling and Analysis of Rank Ordered Data with Ties via a Generalized Plackett-Luce Model}},
journal = {Bayesian Analysis},
publisher = {International Society for Bayesian Analysis},
pages = {1 -- 29},
keywords = {bucket order, EM algorithm, Gibbs sampler, latent variables, MCMC algorithms, ordered partitions, prediction},
year = {2024},
doi = {10.1214/24-BA1434},
URL = {https://doi.org/10.1214/24-BA1434}
}
@article{Thurstone1927,
  author = {Louis Leon Thurstone},
  year = {1927},
  title = {A Law of Comparative Judgement},
  journal = {Psychological Review},
  volume = {34},
  number = {4},
  pages = {273-286},
  url = {https://doi.org/10.1037/h0070288}
}
@book{Diaconis1988,
  author = {Persi Diaconis},
  year = {1988},
  title = {Group Representations in Probability and Statistics},
  series = {IMS Lecture Notes Monograph Series},
  volume = {11},
  edition = {},
  doi = {10.1214/lnms/1215467407},
  publisher = {Institute of Mathematical Statistics}
}
@article{Hunter2004,
author = {David R. Hunter},
title = {{MM algorithms for generalized Bradley-Terry models}},
volume = {32},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {384 -- 406},
keywords = {Bradley-Terry model, Luce's choice axiom, maximum likelihood estimation, MM algorithm, Newton-Raphson, Plackett-Luce model},
year = {2004},
doi = {10.1214/aos/1079120141},
URL = {https://doi.org/10.1214/aos/1079120141}
}
@article{Caron-Doucet2012,
author = {François Caron and Arnaud Doucet},
title = {Efficient Bayesian Inference for Generalized Bradley–Terry Models},
journal = {Journal of Computational and Graphical Statistics},
volume = {21},
number = {1},
pages = {174--196},
year = {2012},
publisher = {ASA Website},
doi = {10.1080/10618600.2012.638220},


URL = { 
    
        https://doi.org/10.1080/10618600.2012.638220
    
    

},
eprint = { 
    
        https://doi.org/10.1080/10618600.2012.638220
    
    

}

}

@article{Turner+2020,
	abstract = {This paper presents the R package PlackettLuce, which implements a generalization of the Plackett--Luce model for rankings data. The generalization accommodates both ties (of arbitrary order) and partial rankings (complete rankings of subsets of items). By default, the implementation adds a set of pseudo-comparisons with a hypothetical item, ensuring that the underlying network of wins and losses between items is always strongly connected. In this way, the worth of each item always has a finite maximum likelihood estimate, with finite standard error. The use of pseudo-comparisons also has a regularization effect, shrinking the estimated parameters towards equal item worth. In addition to standard methods for model summary, PlackettLuce provides a method to compute quasi standard errors for the item parameters. This provides the basis for comparison intervals that do not change with the choice of identifiability constraint placed on the item parameters. Finally, the package provides a method for model-based partitioning using covariates whose values vary between rankings, enabling the identification of subgroups of judges or settings with different item worths. The features of the package are demonstrated through application to classic and novel data sets.},
	author = {Turner, Heather L. and van Etten, Jacob and Firth, David and Kosmidis, Ioannis},
	date = {2020/09/01},
	date-added = {2024-12-26 11:36:31 +0900},
	date-modified = {2024-12-26 11:36:31 +0900},
	doi = {10.1007/s00180-020-00959-3},
	id = {Turner2020},
	isbn = {1613-9658},
	journal = {Computational Statistics},
	number = {3},
	pages = {1027--1057},
	title = {Modelling rankings in R: the PlackettLuce package},
	url = {https://doi.org/10.1007/s00180-020-00959-3},
	volume = {35},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1007/s00180-020-00959-3}}

@article{Koskela2022,
	annote = {doi: 10.1080/10618600.2022.2032722},
	author = {Koskela ,Jere},
	date = {2022/07/03},
	date-added = {2024-12-26 13:44:02 +0900},
	date-modified = {2024-12-26 13:44:02 +0900},
	doi = {10.1080/10618600.2022.2032722},
	isbn = {1061-8600},
	journal = {Journal of Computational and Graphical Statistics},
	journal1 = {Journal of Computational and Graphical Statistics},
	journal2 = {Journal of Computational and Graphical Statistics},
	month = {07},
	number = {3},
	pages = {684--694},
	publisher = {ASA Website},
	title = {Zig-Zag Sampling for Discrete Structures and Nonreversible Phylogenetic MCMC},
	type = {doi: 10.1080/10618600.2022.2032722},
	url = {https://doi.org/10.1080/10618600.2022.2032722},
	volume = {31},
	year = {2022},
	year1 = {2022},
	bdsk-url-1 = {https://doi.org/10.1080/10618600.2022.2032722}}

@article{Goldman+2022,
	annote = {doi: 10.1080/01621459.2021.1909600},
	author = {Goldman ,Jacob Vorstrup and Sell ,Torben and Singh ,Sumeetpal Sidhu},
	date = {2022/10/02},
	date-added = {2024-12-26 14:06:49 +0900},
	date-modified = {2024-12-26 14:06:49 +0900},
	doi = {10.1080/01621459.2021.1909600},
	isbn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	journal1 = {Journal of the American Statistical Association},
	journal2 = {Journal of the American Statistical Association},
	month = {10},
	number = {540},
	pages = {2182--2193},
	publisher = {ASA Website},
	title = {Gradient-Based Markov Chain Monte Carlo for Bayesian Inference With Non-differentiable Priors},
	type = {doi: 10.1080/01621459.2021.1909600},
	url = {https://doi.org/10.1080/01621459.2021.1909600},
	volume = {117},
	year = {2022},
	year1 = {2022},
	bdsk-url-1 = {https://doi.org/10.1080/01621459.2021.1909600}}
@article{Richardson-Green1997,
author = {Richardson, Sylvia. and Green, Peter J.},
title = {On Bayesian Analysis of Mixtures with an Unknown Number of Components (with discussion)},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
volume = {59},
number = {4},
pages = {731-792},
keywords = {Birth-and-death process, Classification, Galaxy data, Heterogeneity, Lake acidity data, Markov chain Monte Carlo method, Normal mixtures, Predictive distribution, Reversible jump algorithms, Sensitivity analysis},
doi = {https://doi.org/10.1111/1467-9868.00095},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/1467-9868.00095},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/1467-9868.00095},
abstract = {New methodology for fully Bayesian mixture analysis is developed, making use of reversible jump Markov chain Monte Carlo methods that are capable of jumping between the parameter subspaces corresponding to different numbers of components in the mixture. A sample from the full joint distribution of all unknown variables is thereby generated, and this can be used as a basis for a thorough presentation of many aspects of the posterior distribution. The methodology is applied here to the analysis of univariate normal mixtures, using a hierarchical prior model that offers an approach to dealing with weak prior information while avoiding the mathematical pitfalls of using improper priors in the mixture context.},
year = {1997}
}
@article{Denison+1998,
 ISSN = {13697412, 14679868},
 URL = {http://www.jstor.org/stable/2985943},
 abstract = {A method of estimating a variety of curves by a sequence of piecewise polynomials is proposed, motivated by a Bayesian model and an appropriate summary of the resulting posterior distribution. A joint distribution is set up over both the number and the position of the knots defining the piecewise polynomials. Throughout we use reversible jump Markov chain Monte Carlo methods to compute the posteriors. The methodology has been successful in giving good estimates for `smooth' functions (i.e. continuous and differentiable) as well as functions which are not differentiable, and perhaps not even continuous, at a finite number of points. The methodology is extended to deal with generalized additive models.},
 author = {D. G. T. Denison and B. K. Mallick and A. F. M. Smith},
 journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
 number = {2},
 pages = {333--350},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {Automatic Bayesian Curve Fitting},
 urldate = {2024-12-26},
 volume = {60},
 year = {1998}
}
@article{Peskun1973,
    author = {Peskun, P. H.},
    title = {Optimum Monte-Carlo sampling using Markov chains},
    journal = {Biometrika},
    volume = {60},
    number = {3},
    pages = {607-612},
    year = {1973},
    month = {12},
    abstract = {The sampling method proposed by Metropolis et al. (1953) requires the simulation of a Markov chain with a specified π as its stationary distribution. Hastings (1970) outlined a general procedure for constructing and simulating such a Markov chain. The matrix P of transition probabilities is constructed using a defined symmetric function sij and an arbitrary transition matrix Q. Here, for a given Q, the relative merits of the two simple choices for sij suggested by Hastings (1970) are discussed. The optimum choice for sij is shown to be one of these. For the other choice, those Q are given which are known to make the sampling method based on P asymptotically less precise than independent sampling.},
    issn = {0006-3444},
    doi = {10.1093/biomet/60.3.607},
    url = {https://doi.org/10.1093/biomet/60.3.607},
    eprint = {https://academic.oup.com/biomet/article-pdf/60/3/607/576888/60-3-607.pdf},
}
@inbook{Phillips-Smith1995,
  author = {D. B. Phillips and A. F. M. Smith},
  chapter = {Bayesian model comparison via jump diffusions},
  editor = {W. R. Gilks and S. Richardson and David Spiegelhalter},
  pages = {215-240},
  publisher = {Chapman and Hall/CRC},
  title = {Markov Chain Monte Carlo in Practice},
  year = {1995},
  url = {https://www.taylorfrancis.com/chapters/mono/10.1201/b14835-18/bayesian-model-comparison-via-jump-diffusions-gilks-richardson-david-spiegelhalter},
}
@misc{Fan+2024,
      title={Reversible jump Markov chain Monte Carlo and multi-model samplers}, 
      author={Yanan Fan and Scott A. Sisson and Laurence Davies},
      year={2024},
      eprint={1001.2055},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1001.2055}, 
}

@article{Gagnon-Doucet2020,
	annote = {doi: 10.1080/10618600.2020.1826955},
	author = {Gagnon ,Philippe and Doucet ,Arnaud},
	date = {2020/11/17},
	date-added = {2024-12-27 15:33:59 +0900},
	date-modified = {2024-12-27 15:33:59 +0900},
	doi = {10.1080/10618600.2020.1826955},
	isbn = {1061-8600},
	journal = {Journal of Computational and Graphical Statistics},
	journal1 = {Journal of Computational and Graphical Statistics},
	journal2 = {Journal of Computational and Graphical Statistics},
	month = {11},
	number = {2},
	pages = {312--323},
	publisher = {ASA Website},
	title = {Nonreversible Jump Algorithms for Bayesian Nested Model Selection},
	type = {doi: 10.1080/10618600.2020.1826955},
	url = {https://doi.org/10.1080/10618600.2020.1826955},
	volume = {30},
	year = {2020},
	year1 = {2020},
	bdsk-url-1 = {https://doi.org/10.1080/10618600.2020.1826955}}

@article{Kass-Raftery1995,
	annote = {doi: 10.1080/01621459.1995.10476572},
	author = {Kass ,Robert E. and Raftery ,Adrian E.},
	date = {1995/06/01},
	date-added = {2024-12-27 15:44:54 +0900},
	date-modified = {2024-12-27 15:44:54 +0900},
	doi = {10.1080/01621459.1995.10476572},
	isbn = {0162-1459},
	journal = {Journal of the American Statistical Association},
	journal1 = {Journal of the American Statistical Association},
	journal2 = {Journal of the American Statistical Association},
	month = {06},
	number = {430},
	pages = {773--795},
	publisher = {ASA Website},
	title = {Bayes Factors},
	type = {doi: 10.1080/01621459.1995.10476572},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572},
	volume = {90},
	year = {1995},
	year1 = {1995},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1995.10476572},
	bdsk-url-2 = {https://doi.org/10.1080/01621459.1995.10476572}}

@InProceedings{Peluchetti-Favaro2020,
  title = 	 {Infinitely deep neural networks as diffusion processes},
  author =       {Peluchetti, Stefano and Favaro, Stefano},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1126--1136},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/peluchetti20a/peluchetti20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/peluchetti20a.html},
  abstract = 	 {When the parameters are independently and identically distributed (initialized) neural networks exhibit undesirable properties that emerge as the number of layers increases, e.g. a vanishing dependency on the input and a concentration on restrictive families of functions including constant functions. We consider parameter distributions that shrink as the number of layers increases in order to recover well-behaved stochastic processes in the limit of infinite depth. This leads to set forth a link between infinitely deep residual networks and solutions to stochastic differential equations, i.e. diffusion processes. We show that these limiting processes do not suffer from the aforementioned issues and investigate their properties.}
}
@book{InPractice1996,
  author = {},
  year = {1996},
  title = {{Markov Chain Monte Carlo in Practice}},
  series = {},
  editor = {W. R. Gilks and S. Richardson and David Spiegelhalter},
  volume = {},
  edition = {},
  url = {https://doi.org/10.1201/b14835},
  publisher = {Chapman \& Hall}
}